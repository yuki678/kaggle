{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import"},{"metadata":{},"cell_type":"markdown","source":"Install HuggingFace implementation of bert (https://huggingface.co/)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (2.3.0)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.11.9)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.38)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers) (1.18.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.85)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2020.1.8)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers) (4.41.1)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.3.2)\nRequirement already satisfied: botocore<1.15.0,>=1.14.9 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (1.14.9)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.7)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.8.1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport sys\nimport time\nimport logging\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n\nfrom transformers import *","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set logger config for logging\nlogger = logging.getLogger('mylogger')\nlogger.setLevel(logging.DEBUG)\ntimestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\nfh = logging.FileHandler('log_model.txt')\nfh.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\nlogger.addHandler(fh)\nlogger.addHandler(ch)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set Random Seed\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/nlp-getting-started","execution_count":5,"outputs":[{"output_type":"stream","text":"sample_submission.csv  test.csv  train.csv\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmit_df = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train size:', train_df.shape)\nprint('Test size:', test_df.shape)","execution_count":52,"outputs":[{"output_type":"stream","text":"Train size: (7613, 5)\nTest size: (3263, 4)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":53,"outputs":[{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.value_counts()","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"0    4342\n1    3271\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text[5]","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"'#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Process Input DataFrames"},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeature(object):\n    \"\"\" A single training/test data class \"\"\"\n    def __init__(self, id, input_ids, masks, segments, label=None):\n        self.id = id\n        self.features = {\n            'input_ids': input_ids,\n            'input_mask': masks,\n            'segment_ids': segments\n        }\n        self.label = label","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Bert Pre-trained Model (Base, Uncased)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":58,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenize text, output padding masks and segment ids as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encoder(text, max_len=512):\n    \"\"\" Return embedded text vector as a list in max_len with a mask list\"\"\"\n    text_token = tokenizer.tokenize(text)\n    text_token = text_token[:max_len-2]\n    text_token = [\"[CLS]\"] + text_token + [\"[SEP]\"]\n    text_ids = tokenizer.convert_tokens_to_ids(text_token)\n    text_ids += [0] * (max_len - len(text_token))\n    pad_masks = [1] * len(text_token) + [0] * (max_len - len(text_token))\n    segment_ids = [0] * len(text_token) + [0] * (max_len - len(text_token))\n    \n    return text_ids, pad_masks, segment_ids","execution_count":67,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process train DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"# If want to change\nmax_seq_length = 512","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = []\n\nfor index, row in train_df.iterrows():\n    input_ids, masks, segments = bert_encoder(row.text, max_seq_length)\n    train_set.append(InputFeature(row.id, input_ids, masks, segments, row.target))\n\n# numpy array to split train and valid within Fold later\ntrain_valid_input_ids = np.array([data.features['input_ids'] for data in train_set])\ntrain_valid_input_masks = np.array([data.features['input_mask'] for data in train_set])\ntrain_valid_segment_ids =np.array([data.features['segment_ids'] for data in train_set])\ntrain_valid_labels = np.array([data.label for data in train_set])","execution_count":69,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process test DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = []\n\nfor index, row in test_df.iterrows():\n    input_ids, masks, segments = bert_encoder(row.text, max_seq_length)\n    train_set.append(InputFeature(row.id, input_ids, masks, segments))\n\n# torch.tensor as test set does not need to split in Fold later\ntest_input_ids = torch.tensor([data.features['input_ids'] for data in test_set], dtype=torch.long)\ntest_input_masks = torch.tensor([data.features['input_mask'] for data in test_set], dtype=torch.long)\ntest_segment_ids = torch.tensor([data.features['segment_ids'] for data in test_set], dtype=torch.long)","execution_count":71,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Very Simple Model, just adding a logistic regression on top of pretrained bert model\nclass SingleModel(nn.Module):\n    \n    def __init__(self, ):\n        \n        super(SimgpleModel, self).__init__()\n        self.base_model = BertModel.from_pretrained('bert-base-uncased')\n        self.fc1 = torch.nn.Linear(768, 1)\n        \n    def forward(self, ids, masks):\n        \n        x = self.base_model(ids, attention_mask=masks)[1]\n        x = self.fc1(x)\n        return x","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complex Model by adding 5 horizontal dropout layers (multi-sample dropout)\nclass MultiSampleDropoutModel(nn.Module):\n    def __init__(self, hidden_size=768, num_class=2):\n        super(MultiSampleDropoutModel, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n                                        output_hidden_states=True,\n                                        output_attentions=True)\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        self.weights = nn.Parameter(torch.rand(13, 1))\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.fc = nn.Linear(hidden_size, num_class)\n\n    def forward(self, input_ids, input_mask, segment_ids):\n        all_hidden_states, all_attentions = self.bert(input_ids, token_type_ids=segment_ids,\n                                                                attention_mask=input_mask)[-2:]\n        batch_size = input_ids.shape[0]\n        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n        atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n        atten = F.softmax(atten.view(-1), dim=0)\n        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                h = self.fc(dropout(feature))\n            else:\n                h += self.fc(dropout(feature))\n        h = h / len(self.dropouts)\n        return h        ","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nlearning_rate = 1e-5  \nnum_epochs = 3  \nbatch_size = 8  \npatience = 2  \nfile_name = 'model'","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define metrics\ndef metric(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='macro')\n    return acc, f1","execution_count":75,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\nUse `StratifiedKFold` to split data into `7 folds`"},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\noof_train = np.zeros((len(train_df), 2), dtype=np.float32)\noof_test = np.zeros((len(test_df), 2), dtype=np.float32)","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold, (train_indices, valid_indices) in enumerate(skf.split(train_valid_labels, train_valid_labels)):\n    \n    # Number of folds to iterrate\n    if fold == 1:\n        break\n\n    logger.info('================     fold {}        ==============='.format(fold))\n    \n    # Train Data in Tensor\n    train_input_ids = torch.tensor(train_valid_input_ids[train_indices], dtype=torch.long)\n    train_input_mask = torch.tensor(train_valid_input_masks[train_indices], dtype=torch.long)\n    train_segment_ids = torch.tensor(train_valid_segment_ids[train_indices], dtype=torch.long)\n    train_label = torch.tensor(train_valid_labels[train_indices], dtype=torch.long)\n    \n    # Validation Data in Tensor\n    valid_input_ids = torch.tensor(train_valid_input_ids[valid_indices], dtype=torch.long)\n    valid_input_mask = torch.tensor(train_valid_input_masks[valid_indices], dtype=torch.long)\n    valid_segment_ids = torch.tensor(train_valid_segment_ids[valid_indices], dtype=torch.long)\n    valid_label = torch.tensor(train_valid_labels[valid_indices], dtype=torch.long)\n\n    # Load data into TensorDataset\n    train = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label)\n    valid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label)\n    test = torch.utils.data.TensorDataset(test_input_ids, test_input_masks, test_segment_ids)\n\n    # Use DataLoader to load data from Dataset in batches\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\n    # Set Model\n    model = MultiSampleDropoutModel()\n    \n    # Move model to GUP/CPU device\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model = model.to(device)\n    \n    # Loss Function - use Cross Entropy as binary classification\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    # Optimizer - Adam with parameter groups\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-6)\n    \n    # Set Train Mode\n    model.train()\n\n    # Initialize\n    best_f1 = 0.\n    valid_best = np.zeros((valid_label.size(0), 2))\n    early_stop = 0\n    \n    for epoch in range(num_epochs):\n        train_loss = 0.\n        for i, batch in tqdm(enumerate(train_loader)):\n            # Move batch data to device\n            batch = tuple(t.to(device) for t in batch)\n            # Bert input features and labels from batch\n            x_ids, x_mask, x_sids, y_truth = batch\n            \n            # Feedforward prediction\n            y_pred = model(x_ids, x_mask, x_sids)\n            # Calculate Loss\n            loss = loss_fn(y_pred, y_truth)\n            # Reset gradient\n            optimizer.zero_grad()\n            # Backward Propagation\n            loss.backward()\n            # Update Weights\n            optimizer.step()\n            # Training Loss\n            train_loss += loss.item() / len(train_loader)\n            \n            logger.info('train batch: %d, train_loss: %8f\\n' % (i, train_loss))\n    \n        # Move to Evaluation Mode\n        model.eval()\n        \n        # Initialize\n        val_loss = 0.\n        valid_preds_fold = np.zeros((valid_label.size(0), 2))\n        \n        with torch.no_grad():\n            for i, batch in tqdm(enumerate(valid_loader)):\n                batch = tuple(t.to(device) for t in batch)\n                x_ids, x_mask, x_sids, y_truth = batch\n                y_pred = model(x_ids, x_mask, x_sids).detach()\n                val_loss += loss_fn(y_pred, y_truth).item() / len(valid_loader)\n                valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n                \n                logger.info('validation batch: %d, val_loss: %8f, val_preds_fold: %8f\\n' % (i, val_loss, val_preds_fold))\n    \n        # Calculate metrics\n        acc, f1 = metric(all_label[valid_index], np.argmax(valid_preds_fold, axis=1))\n        \n        # If improving, save the model. If not, count up for early stopping\n        if best_f1 < f1:\n            early_stop = 0\n            best_f1 = f1\n            valid_best = valid_preds_fold\n            torch.save(model.state_dict(), 'model_fold_{}.bin'.format(fold))\n        else:\n            early_stop += 1\n            \n\n        logger.info(\n            'epoch: %d, train loss: %.8f, valid loss: %.8f, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n            (epoch, train_loss, val_loss, acc, f1, best_f1))\n        \n        if device == 'cuda:0':\n            torch.cuda.empty_cache()  \n        \n        # Early stop if it reaces patience number\n        if early_stop >= patience:\n            break\n\n    # Once all epochs are done, take the best model of the fold\n    test_preds_fold = np.zeros((len(test_df), 2))\n    valid_preds_fold = np.zeros((valid_label.size(0), 2))\n    \n    # Load the best model\n    model.load_state_dict(torch.load('model_fold_{}.bin'.format(fold)))\n    # Set Evaluation Mode\n    model.eval()\n    \n    # Prediction on the validation set\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(valid_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n\n    # Prediction on the test set\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(test_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            test_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n\n    # Check the metrics for the validation set\n    valid_best = valid_preds_fold\n    oof_train[valid_index] = valid_best\n    acc, f1 = metric(all_label[valid_index], np.argmax(valid_best, axis=1))\n    logger.info('epoch: best, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n                (acc, f1, best_f1))\n    \n    #oof_test += test_preds_fold / 7 # uncomment this for 7 folds\n    oof_test += test_preds_fold / 1 # comment this line when training for 7 folds","execution_count":78,"outputs":[{"output_type":"stream","text":"[2020-02-01 09:23:35,330][INFO] ## ================     fold 0        ===============\n","name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"name 'test_input_mask' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-fca63f7586b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_segment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_segment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_segment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Use DataLoader to load data from Dataset in batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_input_mask' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger.info(f1_score(labels, np.argmax(oof_train, axis=1)))\ntrain_df['pred_target'] = np.argmax(oof_train, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['target'] = np.argmax(oof_test, axis=1)\nlogger.info(test_df['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['target'] = np.argmax(oof_test, axis=1)\nsubmit.to_csv('submission_1fold.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}