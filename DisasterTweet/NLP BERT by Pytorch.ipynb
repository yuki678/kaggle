{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import"},{"metadata":{},"cell_type":"markdown","source":"Install HuggingFace implementation of bert (https://huggingface.co/)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport sys\nimport time\nimport logging\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n\nfrom transformers import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set logger config for logging\nlogger = logging.getLogger('mylogger')\nlogger.setLevel(logging.DEBUG)\ntimestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\nfh = logging.FileHandler('log_model.txt')\nfh.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\nlogger.addHandler(fh)\nlogger.addHandler(ch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set Random Seed\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/nlp-getting-started","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmit_df = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train size:', train_df.shape)\nprint('Test size:', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['target'] = np.zeros(len(test_df), dtype=np.int64)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Process Input DataFrames"},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeature(object):\n    \"\"\" A single training/test data class \"\"\"\n    def __init__(self, id, input_ids, masks, segments, label=None):\n        self.id = id\n        self.features = {\n            'input_ids': input_ids,\n            'input_mask': masks,\n            'segment_ids': segments\n        }\n        self.label = label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Bert Pre-trained Model (Base, Uncased)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenize text, output padding masks and segment ids as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encoder(text, max_len=512):\n    \"\"\" Return embedded text vector as a list in max_len with a mask list\"\"\"\n    text_token = tokenizer.tokenize(text)\n    text_token = text_token[:max_len-2]\n    text_token = [\"[CLS]\"] + text_token + [\"[SEP]\"]\n    text_ids = tokenizer.convert_tokens_to_ids(text_token)\n    text_ids += [0] * (max_len - len(text_token))\n    pad_masks = [1] * len(text_token) + [0] * (max_len - len(text_token))\n    segment_ids = [0] * len(text_token) + [0] * (max_len - len(text_token))\n    \n    return text_ids, pad_masks, segment_ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process train DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"# If want to change\nmax_seq_length = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = []\n\nfor index, row in train_df.iterrows():\n    input_ids, masks, segments = bert_encoder(row.text, max_seq_length)\n    train_set.append(InputFeature(row.id, input_ids, masks, segments, row.target))\n\nlabels = train_df[\"target\"].astype(int).values\n# numpy array to split train and valid within Fold later\ntrain_valid_input_ids = np.array([data.features['input_ids'] for data in train_set])\ntrain_valid_input_masks = np.array([data.features['input_mask'] for data in train_set])\ntrain_valid_segment_ids =np.array([data.features['segment_ids'] for data in train_set])\ntrain_valid_labels = np.array([data.label for data in train_set])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process test DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = []\n\nfor index, row in test_df.iterrows():\n    input_ids, masks, segments = bert_encoder(row.text, max_seq_length)\n    test_set.append(InputFeature(row.id, input_ids, masks, segments))\n\n# torch.tensor as test set does not need to split in Fold later\ntest_input_ids = torch.tensor([data.features['input_ids'] for data in test_set], dtype=torch.long)\ntest_input_masks = torch.tensor([data.features['input_mask'] for data in test_set], dtype=torch.long)\ntest_segment_ids = torch.tensor([data.features['segment_ids'] for data in test_set], dtype=torch.long)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Very Simple Model, just adding a logistic regression on top of pretrained bert model\nclass SingleModel(nn.Module):\n    \n    def __init__(self, ):\n        \n        super(SimgpleModel, self).__init__()\n        self.base_model = BertModel.from_pretrained('bert-base-uncased')\n        self.fc1 = torch.nn.Linear(768, 1)\n        \n    def forward(self, ids, masks):\n        \n        x = self.base_model(ids, attention_mask=masks)[1]\n        x = self.fc1(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complex Model by adding 5 horizontal dropout layers (multi-sample dropout)\nclass MultiSampleDropoutModel(nn.Module):\n    def __init__(self, hidden_size=768, num_class=2):\n        super(MultiSampleDropoutModel, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n                                        output_hidden_states=True,\n                                        output_attentions=True)\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        self.weights = nn.Parameter(torch.rand(13, 1))\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.fc = nn.Linear(hidden_size, num_class)\n\n    def forward(self, input_ids, input_mask, segment_ids):\n        all_hidden_states, all_attentions = self.bert(input_ids, token_type_ids=segment_ids,\n                                                                attention_mask=input_mask)[-2:]\n        batch_size = input_ids.shape[0]\n        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n        atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n        atten = F.softmax(atten.view(-1), dim=0)\n        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                h = self.fc(dropout(feature))\n            else:\n                h += self.fc(dropout(feature))\n        h = h / len(self.dropouts)\n        return h        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nlearning_rate = 1e-5  \nnum_epochs = 3  \nbatch_size = 8  \npatience = 2  \nfile_name = 'model'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define metrics\ndef metric(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='macro')\n    return acc, f1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\nUse `StratifiedKFold` to split data into `7 folds`"},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\noof_train = np.zeros((len(train_df), 2), dtype=np.float32)\noof_test = np.zeros((len(test_df), 2), dtype=np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold, (train_indices, valid_indices) in enumerate(skf.split(train_valid_labels, train_valid_labels)):\n    \n    # Number of folds to iterrate\n    if fold == 7:\n        break\n\n    logger.info('================     fold {}        ==============='.format(fold))\n    \n    # Train Data in Tensor\n    train_input_ids = torch.tensor(train_valid_input_ids[train_indices], dtype=torch.long)\n    train_input_mask = torch.tensor(train_valid_input_masks[train_indices], dtype=torch.long)\n    train_segment_ids = torch.tensor(train_valid_segment_ids[train_indices], dtype=torch.long)\n    train_label = torch.tensor(train_valid_labels[train_indices], dtype=torch.long)\n    \n    # Validation Data in Tensor\n    valid_input_ids = torch.tensor(train_valid_input_ids[valid_indices], dtype=torch.long)\n    valid_input_mask = torch.tensor(train_valid_input_masks[valid_indices], dtype=torch.long)\n    valid_segment_ids = torch.tensor(train_valid_segment_ids[valid_indices], dtype=torch.long)\n    valid_label = torch.tensor(train_valid_labels[valid_indices], dtype=torch.long)\n\n    # Load data into TensorDataset\n    train = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label)\n    valid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label)\n    test = torch.utils.data.TensorDataset(test_input_ids, test_input_masks, test_segment_ids)\n\n    # Use DataLoader to load data from Dataset in batches\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\n    # Set Model\n    model = MultiSampleDropoutModel()\n    \n    # Move model to GUP/CPU device\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model = model.to(device)\n    \n    # Loss Function - use Cross Entropy as binary classification\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    # Optimizer - Adam with parameter groups\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-6)\n    \n    # Set Train Mode\n    model.train()\n\n    # Initialize\n    best_f1 = 0.\n    valid_best = np.zeros((valid_label.size(0), 2))\n    early_stop = 0\n    \n    for epoch in range(num_epochs):\n        train_loss = 0.\n        for i, batch in tqdm(enumerate(train_loader)):\n            # Move batch data to device\n            batch = tuple(t.to(device) for t in batch)\n            # Bert input features and labels from batch\n            x_ids, x_mask, x_sids, y_truth = batch\n            \n            # Feedforward prediction\n            y_pred = model(x_ids, x_mask, x_sids)\n            # Calculate Loss\n            loss = loss_fn(y_pred, y_truth)\n            # Reset gradient\n            optimizer.zero_grad()\n            # Backward Propagation\n            loss.backward()\n            # Update Weights\n            optimizer.step()\n            # Training Loss\n            train_loss += loss.item() / len(train_loader)\n            \n            #logger.debug('train batch: %d, train_loss: %8f\\n' % (i, train_loss))\n    \n        # Move to Evaluation Mode\n        model.eval()\n        \n        # Initialize\n        val_loss = 0.\n        valid_preds_fold = np.zeros((valid_label.size(0), 2))\n        \n        with torch.no_grad():\n            for i, batch in tqdm(enumerate(valid_loader)):\n                batch = tuple(t.to(device) for t in batch)\n                x_ids, x_mask, x_sids, y_truth = batch\n                y_pred = model(x_ids, x_mask, x_sids).detach()\n                val_loss += loss_fn(y_pred, y_truth).item() / len(valid_loader)\n                valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n                \n                # print('validation batch: {}, val_loss: {}, valid_preds_fold: {}'.format(i, val_loss, valid_preds_fold[i * batch_size:(i + 1) * batch_size]))\n    \n        # Calculate metrics\n        acc, f1 = metric(train_valid_labels[valid_indices], np.argmax(valid_preds_fold, axis=1))\n        \n        # If improving, save the model. If not, count up for early stopping\n        if best_f1 < f1:\n            early_stop = 0\n            best_f1 = f1\n            valid_best = valid_preds_fold\n            torch.save(model.state_dict(), 'model_fold_{}.bin'.format(fold))\n        else:\n            early_stop += 1\n            \n\n        logger.info(\n            'epoch: %d, train loss: %.8f, valid loss: %.8f, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n            (epoch, train_loss, val_loss, acc, f1, best_f1))\n        \n        if device == 'cuda:0':\n            torch.cuda.empty_cache()  \n        \n        # Early stop if it reaces patience number\n        if early_stop >= patience:\n            break\n            \n        model.train()\n\n    # Once all epochs are done, take the best model of the fold\n    test_preds_fold = np.zeros((len(test_df), 2))\n    valid_preds_fold = np.zeros((valid_label.size(0), 2))\n    \n    # Load the best model\n    model.load_state_dict(torch.load('model_fold_{}.bin'.format(fold)))\n    # Set Evaluation Mode\n    model.eval()\n    \n    # Prediction on the validation set\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(valid_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n\n    # Prediction on the test set\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(test_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            test_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n\n    # Check the metrics for the validation set\n    valid_best = valid_preds_fold\n    oof_train[valid_indices] = valid_best\n    acc, f1 = metric(train_valid_labels[valid_indices], np.argmax(valid_best, axis=1))\n    logger.info('epoch: best, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' % (acc, f1, best_f1))\n    \n    oof_test += test_preds_fold / 7 # uncomment this for 7 folds\n    #oof_test += test_preds_fold / 2 # comment this line when training for 7 folds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger.info(f1_score(labels, np.argmax(oof_train, axis=1)))\ntrain_df['pred_target'] = np.argmax(oof_train, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['target'] = np.argmax(oof_test, axis=1)\nlogger.info(test_df['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df['target'] = np.argmax(oof_test, axis=1)\nsubmit_df.to_csv('submission_1fold.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_preds_fold[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_fold[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}