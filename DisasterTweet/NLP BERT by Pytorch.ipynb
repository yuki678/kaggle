{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import"},{"metadata":{},"cell_type":"markdown","source":"Install HuggingFace implementation of bert (https://huggingface.co/)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":101,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (2.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers) (1.18.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.85)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.38)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2020.1.8)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers) (4.41.1)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.11.9)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.7)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: botocore<1.15.0,>=1.14.9 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (1.14.9)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.3.2)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.8.1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport sys\nimport time\nimport logging\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n\nfrom transformers import *","execution_count":102,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set logger config for logging\nlogger = logging.getLogger('mylogger')\nlogger.setLevel(logging.DEBUG)\ntimestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\nfh = logging.FileHandler('log_model.txt')\nfh.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\nlogger.addHandler(fh)\nlogger.addHandler(ch)","execution_count":103,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set Random Seed\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)","execution_count":104,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/nlp-getting-started","execution_count":105,"outputs":[{"output_type":"stream","text":"sample_submission.csv  test.csv  train.csv\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmit_df = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train size:', train_df.shape)\nprint('Test size:', test_df.shape)","execution_count":107,"outputs":[{"output_type":"stream","text":"Train size: (7613, 5)\nTest size: (3263, 4)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":108,"outputs":[{"output_type":"execute_result","execution_count":108,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.value_counts()","execution_count":109,"outputs":[{"output_type":"execute_result","execution_count":109,"data":{"text/plain":"0    4342\n1    3271\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text[5]","execution_count":110,"outputs":[{"output_type":"execute_result","execution_count":110,"data":{"text/plain":"'#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":111,"outputs":[{"output_type":"execute_result","execution_count":111,"data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Process Input DataFrames"},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeature(object):\n    \"\"\" A single training/test data class \"\"\"\n    def __init__(self, id, input_ids, masks, segments, label=None):\n        self.id = id\n        self.features = {\n            'input_ids': input_ids,\n            'input_mask': masks,\n            'segment_ids': segments\n        }\n        self.label = label","execution_count":112,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Bert Pre-trained Model (Base, Uncased)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":113,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenize text, output padding masks and segment ids as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encoder(text, max_len=512):\n    \"\"\" Return embedded text vector as a list in max_len with a mask list\"\"\"\n    text_token = tokenizer.tokenize(text)\n    text_token = text_token[:max_len-2]\n    text_token = [\"[CLS]\"] + text_token + [\"[SEP]\"]\n    text_ids = tokenizer.convert_tokens_to_ids(text_token)\n    text_ids += [0] * (max_len - len(text_token))\n    pad_masks = [1] * len(text_token) + [0] * (max_len - len(text_token))\n    segment_ids = [0] * len(text_token) + [0] * (max_len - len(text_token))\n    \n    return text_ids, pad_masks, segment_ids","execution_count":114,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process train DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"# If want to change\nmax_seq_length = 512","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = []\n\nfor index, row in train_df.iterrows():\n    input_ids, masks, segments = bert_encoder(row.text, max_seq_length)\n    train_set.append(InputFeature(row.id, input_ids, masks, segments, row.target))\n\n# numpy array to split train and valid within Fold later\ntrain_valid_input_ids = np.array([data.features['input_ids'] for data in train_set])\ntrain_valid_input_masks = np.array([data.features['input_mask'] for data in train_set])\ntrain_valid_segment_ids =np.array([data.features['segment_ids'] for data in train_set])\ntrain_valid_labels = np.array([data.label for data in train_set])","execution_count":116,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process test DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = []\n\nfor index, row in test_df.iterrows():\n    input_ids, masks, segments = bert_encoder(row.text, max_seq_length)\n    train_set.append(InputFeature(row.id, input_ids, masks, segments))\n\n# torch.tensor as test set does not need to split in Fold later\ntest_input_ids = torch.tensor([data.features['input_ids'] for data in test_set], dtype=torch.long)\ntest_input_masks = torch.tensor([data.features['input_mask'] for data in test_set], dtype=torch.long)\ntest_segment_ids = torch.tensor([data.features['segment_ids'] for data in test_set], dtype=torch.long)","execution_count":117,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Very Simple Model, just adding a logistic regression on top of pretrained bert model\nclass SingleModel(nn.Module):\n    \n    def __init__(self, ):\n        \n        super(SimgpleModel, self).__init__()\n        self.base_model = BertModel.from_pretrained('bert-base-uncased')\n        self.fc1 = torch.nn.Linear(768, 1)\n        \n    def forward(self, ids, masks):\n        \n        x = self.base_model(ids, attention_mask=masks)[1]\n        x = self.fc1(x)\n        return x","execution_count":118,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complex Model by adding 5 horizontal dropout layers (multi-sample dropout)\nclass MultiSampleDropoutModel(nn.Module):\n    def __init__(self, hidden_size=768, num_class=2):\n        super(MultiSampleDropoutModel, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n                                        output_hidden_states=True,\n                                        output_attentions=True)\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        self.weights = nn.Parameter(torch.rand(13, 1))\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.fc = nn.Linear(hidden_size, num_class)\n\n    def forward(self, input_ids, input_mask, segment_ids):\n        all_hidden_states, all_attentions = self.bert(input_ids, token_type_ids=segment_ids,\n                                                                attention_mask=input_mask)[-2:]\n        batch_size = input_ids.shape[0]\n        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n        atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n        atten = F.softmax(atten.view(-1), dim=0)\n        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                h = self.fc(dropout(feature))\n            else:\n                h += self.fc(dropout(feature))\n        h = h / len(self.dropouts)\n        return h        ","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nlearning_rate = 1e-5  \nnum_epochs = 3  \nbatch_size = 8  \npatience = 2  \nfile_name = 'model'","execution_count":120,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define metrics\ndef metric(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='macro')\n    return acc, f1","execution_count":121,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\nUse `StratifiedKFold` to split data into `7 folds`"},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\noof_train = np.zeros((len(train_df), 2), dtype=np.float32)\noof_test = np.zeros((len(test_df), 2), dtype=np.float32)","execution_count":122,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold, (train_indices, valid_indices) in enumerate(skf.split(train_valid_labels, train_valid_labels)):\n    \n    # Number of folds to iterrate\n    if fold == 1:\n        break\n\n    logger.info('================     fold {}        ==============='.format(fold))\n    \n    # Train Data in Tensor\n    train_input_ids = torch.tensor(train_valid_input_ids[train_indices], dtype=torch.long)\n    train_input_mask = torch.tensor(train_valid_input_masks[train_indices], dtype=torch.long)\n    train_segment_ids = torch.tensor(train_valid_segment_ids[train_indices], dtype=torch.long)\n    train_label = torch.tensor(train_valid_labels[train_indices], dtype=torch.long)\n    \n    # Validation Data in Tensor\n    valid_input_ids = torch.tensor(train_valid_input_ids[valid_indices], dtype=torch.long)\n    valid_input_mask = torch.tensor(train_valid_input_masks[valid_indices], dtype=torch.long)\n    valid_segment_ids = torch.tensor(train_valid_segment_ids[valid_indices], dtype=torch.long)\n    valid_label = torch.tensor(train_valid_labels[valid_indices], dtype=torch.long)\n\n    # Load data into TensorDataset\n    train = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label)\n    valid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label)\n    test = torch.utils.data.TensorDataset(test_input_ids, test_input_masks, test_segment_ids)\n\n    # Use DataLoader to load data from Dataset in batches\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\n    # Set Model\n    model = MultiSampleDropoutModel()\n    \n    # Move model to GUP/CPU device\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model = model.to(device)\n    \n    # Loss Function - use Cross Entropy as binary classification\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    # Optimizer - Adam with parameter groups\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-6)\n    \n    # Set Train Mode\n    model.train()\n\n    # Initialize\n    best_f1 = 0.\n    valid_best = np.zeros((valid_label.size(0), 2))\n    early_stop = 0\n    \n    for epoch in range(num_epochs):\n        train_loss = 0.\n        for i, batch in tqdm(enumerate(train_loader)):\n            # Move batch data to device\n            batch = tuple(t.to(device) for t in batch)\n            # Bert input features and labels from batch\n            x_ids, x_mask, x_sids, y_truth = batch\n            \n            # Feedforward prediction\n            y_pred = model(x_ids, x_mask, x_sids)\n            # Calculate Loss\n            loss = loss_fn(y_pred, y_truth)\n            # Reset gradient\n            optimizer.zero_grad()\n            # Backward Propagation\n            loss.backward()\n            # Update Weights\n            optimizer.step()\n            # Training Loss\n            train_loss += loss.item() / len(train_loader)\n            \n            #logger.debug('train batch: %d, train_loss: %8f\\n' % (i, train_loss))\n    \n        # Move to Evaluation Mode\n        model.eval()\n        \n        # Initialize\n        val_loss = 0.\n        valid_preds_fold = np.zeros((valid_label.size(0), 2))\n        \n        with torch.no_grad():\n            for i, batch in tqdm(enumerate(valid_loader)):\n                batch = tuple(t.to(device) for t in batch)\n                x_ids, x_mask, x_sids, y_truth = batch\n                y_pred = model(x_ids, x_mask, x_sids).detach()\n                val_loss += loss_fn(y_pred, y_truth).item() / len(valid_loader)\n                valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n                \n                print('validation batch: {}, val_loss: {}, valid_preds_fold: {}'.format(i, val_loss, valid_preds_fold[i * batch_size:(i + 1) * batch_size]))\n    \n        # Calculate metrics\n        acc, f1 = metric(train_valid_labels[valid_indices], np.argmax(valid_preds_fold, axis=1))\n        \n        # If improving, save the model. If not, count up for early stopping\n        if best_f1 < f1:\n            early_stop = 0\n            best_f1 = f1\n            valid_best = valid_preds_fold\n            torch.save(model.state_dict(), 'model_fold_{}.bin'.format(fold))\n        else:\n            early_stop += 1\n            \n\n        logger.info(\n            'epoch: %d, train loss: %.8f, valid loss: %.8f, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n            (epoch, train_loss, val_loss, acc, f1, best_f1))\n        \n        if device == 'cuda:0':\n            torch.cuda.empty_cache()  \n        \n        # Early stop if it reaces patience number\n        if early_stop >= patience:\n            break\n\n    # Once all epochs are done, take the best model of the fold\n    test_preds_fold = np.zeros((len(test_df), 2))\n    valid_preds_fold = np.zeros((valid_label.size(0), 2))\n    \n    # Load the best model\n    model.load_state_dict(torch.load('model_fold_{}.bin'.format(fold)))\n    # Set Evaluation Mode\n    model.eval()\n    \n    # Prediction on the validation set\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(valid_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n\n    # Prediction on the test set\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(test_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            test_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n\n    # Check the metrics for the validation set\n    valid_best = valid_preds_fold\n    oof_train[valid_indices] = valid_best\n    acc, f1 = metric(train_valid_labels[valid_indices], np.argmax(valid_best, axis=1))\n    logger.info('epoch: best, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' % (acc, f1, best_f1))\n    \n    #oof_test += test_preds_fold / 7 # uncomment this for 7 folds\n    oof_test += test_preds_fold / 1 # comment this line when training for 7 folds","execution_count":123,"outputs":[{"output_type":"stream","text":"[2020-02-01 13:45:43,466][INFO] ## ================     fold 0        ===============\n[2020-02-01 13:45:43,466][INFO] ## ================     fold 0        ===============\n[2020-02-01 13:45:43,466][INFO] ## ================     fold 0        ===============\n[2020-02-01 13:45:43,466][INFO] ## ================     fold 0        ===============\n[2020-02-01 13:45:43,466][INFO] ## ================     fold 0        ===============\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b2955cc12974e25bc124ae746893a27"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:89: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06fa880d03164050b5300f450e32ad9c"}},"metadata":{}},{"output_type":"stream","text":"validation batch: 0, val_loss: 0.0014007913805272458, valid_preds_fold: [[0.90873921 0.09126081]\n [0.93612921 0.06387078]\n [0.88758212 0.11241786]\n [0.81546944 0.18453053]\n [0.14845935 0.85154068]\n [0.03656323 0.96343672]\n [0.68243593 0.3175641 ]\n [0.6248275  0.3751725 ]]\nvalidation batch: 1, val_loss: 0.002769647288496477, valid_preds_fold: [[0.00663781 0.99336219]\n [0.77262014 0.22737986]\n [0.48017287 0.51982713]\n [0.78866172 0.21133833]\n [0.06533835 0.93466169]\n [0.08819859 0.91180146]\n [0.92857528 0.07142466]\n [0.89587545 0.10412456]]\nvalidation batch: 2, val_loss: 0.005222178723690284, valid_preds_fold: [[0.90573227 0.09426774]\n [0.81852728 0.1814727 ]\n [0.87089539 0.12910458]\n [0.12250561 0.87749439]\n [0.00505433 0.9949457 ]\n [0.00357952 0.9964205 ]\n [0.08021006 0.91978991]\n [0.05693388 0.94306612]]\nvalidation batch: 3, val_loss: 0.007084912627282804, valid_preds_fold: [[0.00455528 0.99544477]\n [0.67812264 0.32187739]\n [0.82313037 0.17686968]\n [0.7620306  0.23796941]\n [0.0033602  0.99663985]\n [0.00494262 0.99505734]\n [0.00534843 0.99465162]\n [0.00461579 0.99538416]]\nvalidation batch: 4, val_loss: 0.011307500574710597, valid_preds_fold: [[0.00396868 0.99603134]\n [0.87406683 0.12593317]\n [0.80968893 0.19031106]\n [0.37909201 0.62090802]\n [0.45510784 0.54489213]\n [0.54942596 0.45057404]\n [0.90983456 0.0901655 ]\n [0.8392179  0.1607821 ]]\nvalidation batch: 5, val_loss: 0.012752902333753823, valid_preds_fold: [[0.89489269 0.10510723]\n [0.84461516 0.15538487]\n [0.89675695 0.10324303]\n [0.73275465 0.26724535]\n [0.91542554 0.08457451]\n [0.6024465  0.39755353]\n [0.91106606 0.08893388]\n [0.82196504 0.17803495]]\nvalidation batch: 6, val_loss: 0.01596318196206197, valid_preds_fold: [[0.67393094 0.32606906]\n [0.86741328 0.1325867 ]\n [0.85638142 0.14361857]\n [0.90912044 0.09087955]\n [0.86726165 0.13273841]\n [0.80769181 0.19230817]\n [0.61904907 0.38095096]\n [0.89568484 0.10431515]]\nvalidation batch: 7, val_loss: 0.022846024401866605, valid_preds_fold: [[0.72164905 0.27835095]\n [0.87942296 0.12057706]\n [0.9060874  0.09391255]\n [0.10304622 0.89695382]\n [0.02016225 0.97983783]\n [0.79011184 0.2098882 ]\n [0.01034648 0.98965353]\n [0.83118278 0.16881724]]\nvalidation batch: 8, val_loss: 0.02329946934741779, valid_preds_fold: [[0.05315034 0.94684964]\n [0.91323227 0.08676771]\n [0.93062025 0.06937981]\n [0.03708559 0.96291441]\n [0.92610753 0.07389251]\n [0.9039346  0.09606542]\n [0.04981233 0.95018762]\n [0.01295732 0.98704267]]\nvalidation batch: 9, val_loss: 0.024366452093542056, valid_preds_fold: [[0.01127073 0.98872924]\n [0.86474711 0.13525295]\n [0.77935135 0.22064868]\n [0.01102135 0.98897868]\n [0.85152769 0.14847232]\n [0.89368552 0.1063145 ]\n [0.66227931 0.33772069]\n [0.93501568 0.06498434]]\nvalidation batch: 10, val_loss: 0.02710946415462633, valid_preds_fold: [[0.91481698 0.08518304]\n [0.7448324  0.25516757]\n [0.9050138  0.09498625]\n [0.85525882 0.14474116]\n [0.82045704 0.17954296]\n [0.85757005 0.1424299 ]\n [0.88472903 0.11527096]\n [0.90729785 0.09270217]]\nvalidation batch: 11, val_loss: 0.031901027599390404, valid_preds_fold: [[0.39469245 0.60530752]\n [0.30400705 0.69599295]\n [0.39347428 0.60652578]\n [0.39605692 0.60394305]\n [0.6523667  0.3476333 ]\n [0.52439719 0.47560278]\n [0.17290317 0.82709682]\n [0.4844909  0.51550913]]\nvalidation batch: 12, val_loss: 0.033730016355096856, valid_preds_fold: [[0.25567904 0.74432093]\n [0.46557513 0.5344249 ]\n [0.22577181 0.77422816]\n [0.9236775  0.0763225 ]\n [0.78657794 0.21342212]\n [0.8871634  0.11283664]\n [0.89013243 0.10986755]\n [0.87515754 0.12484246]]\nvalidation batch: 13, val_loss: 0.03481979776908011, valid_preds_fold: [[0.80647719 0.19352283]\n [0.77155811 0.22844188]\n [0.94268882 0.05731119]\n [0.87242919 0.12757082]\n [0.90346605 0.09653402]\n [0.89081568 0.10918439]\n [0.91906494 0.08093508]\n [0.80015355 0.19984642]]\nvalidation batch: 14, val_loss: 0.037748177229923054, valid_preds_fold: [[0.87413675 0.12586324]\n [0.6652413  0.33475867]\n [0.76827163 0.23172836]\n [0.92874658 0.07125338]\n [0.83327484 0.1667251 ]\n [0.62437451 0.37562543]\n [0.68036687 0.31963313]\n [0.91141993 0.08858009]]\nvalidation batch: 15, val_loss: 0.0406176960163743, valid_preds_fold: [[0.76210189 0.23789813]\n [0.66772395 0.33227608]\n [0.91757679 0.08242325]\n [0.51797754 0.48202243]\n [0.93638337 0.06361666]\n [0.89337069 0.10662933]\n [0.2383333  0.76166672]\n [0.89312798 0.10687198]]\nvalidation batch: 16, val_loss: 0.04305661279354653, valid_preds_fold: [[0.8980732  0.10192681]\n [0.822182   0.17781802]\n [0.83693469 0.1630653 ]\n [0.8604688  0.13953121]\n [0.89711457 0.10288545]\n [0.91006464 0.08993539]\n [0.88773996 0.11226004]\n [0.91944498 0.08055495]]\nvalidation batch: 17, val_loss: 0.04462574726908747, valid_preds_fold: [[0.89113694 0.10886304]\n [0.90961838 0.09038161]\n [0.89526004 0.10473994]\n [0.87678719 0.12321277]\n [0.80585957 0.19414039]\n [0.51740843 0.48259157]\n [0.81181002 0.18819   ]\n [0.89160454 0.10839545]]\nvalidation batch: 18, val_loss: 0.04526742977382493, valid_preds_fold: [[0.92085099 0.07914903]\n [0.93344146 0.06655855]\n [0.9512139  0.04878605]\n [0.93985921 0.06014083]\n [0.89312321 0.10687679]\n [0.88919872 0.11080132]\n [0.87913561 0.12086443]\n [0.92253774 0.07746228]]\nvalidation batch: 19, val_loss: 0.04732847529171157, valid_preds_fold: [[0.90698087 0.09301918]\n [0.26060992 0.73939008]\n [0.93595833 0.06404164]\n [0.91490835 0.08509166]\n [0.89826322 0.10173682]\n [0.9321245  0.06787548]\n [0.91542053 0.08457948]\n [0.67336142 0.32663858]]\nvalidation batch: 20, val_loss: 0.05160160971819049, valid_preds_fold: [[0.11414895 0.88585103]\n [0.91759455 0.08240551]\n [0.91686362 0.08313637]\n [0.87532955 0.12467042]\n [0.82999218 0.17000778]\n [0.60628211 0.39371789]\n [0.01752692 0.98247313]\n [0.01711863 0.98288131]]\nvalidation batch: 21, val_loss: 0.05208387693566997, valid_preds_fold: [[0.01623594 0.98376405]\n [0.7302404  0.26975963]\n [0.03160402 0.96839595]\n [0.03163922 0.96836072]\n [0.04481654 0.95518345]\n [0.06513945 0.93486053]\n [0.00983726 0.99016273]\n [0.01039894 0.98960108]]\nvalidation batch: 22, val_loss: 0.054735612967153534, valid_preds_fold: [[0.02914888 0.97085112]\n [0.09694782 0.90305221]\n [0.02358417 0.97641581]\n [0.15313067 0.84686929]\n [0.23975043 0.76024961]\n [0.37085137 0.6291486 ]\n [0.20255549 0.79744446]\n [0.3729834  0.6270166 ]]\nvalidation batch: 23, val_loss: 0.061455854806151705, valid_preds_fold: [[0.27587184 0.72412813]\n [0.02794988 0.97205013]\n [0.58519602 0.41480398]\n [0.4805361  0.5194639 ]\n [0.77731687 0.22268316]\n [0.03894695 0.96105301]\n [0.00852911 0.99147087]\n [0.02413539 0.97586459]]\nvalidation batch: 24, val_loss: 0.06764842760171333, valid_preds_fold: [[0.03079089 0.96920913]\n [0.04445712 0.95554292]\n [0.32380673 0.6761933 ]\n [0.8526004  0.14739966]\n [0.88028091 0.1197191 ]\n [0.86834621 0.13165385]\n [0.92095125 0.07904869]\n [0.00931731 0.99068272]]\nvalidation batch: 25, val_loss: 0.0715445730916775, valid_preds_fold: [[0.02998916 0.97001082]\n [0.85443652 0.14556345]\n [0.91708934 0.0829106 ]\n [0.89508742 0.10491256]\n [0.64383858 0.35616139]\n [0.9079423  0.09205766]\n [0.08728041 0.91271961]\n [0.31339225 0.68660772]]\nvalidation batch: 26, val_loss: 0.07429002393988797, valid_preds_fold: [[0.48319745 0.51680255]\n [0.77846587 0.22153409]\n [0.01377356 0.98622638]\n [0.36208165 0.63791829]\n [0.48298508 0.51701492]\n [0.09238435 0.90761566]\n [0.80479348 0.19520651]\n [0.02733181 0.97266823]]\nvalidation batch: 27, val_loss: 0.07648639419000514, valid_preds_fold: [[0.19123957 0.80876046]\n [0.20589343 0.79410654]\n [0.01341263 0.98658735]\n [0.06170374 0.93829626]\n [0.88084751 0.11915244]\n [0.05695606 0.94304395]\n [0.78121954 0.21878047]\n [0.01276844 0.98723155]]\nvalidation batch: 28, val_loss: 0.08018267965012223, valid_preds_fold: [[0.87610519 0.12389486]\n [0.81914121 0.18085884]\n [0.09055012 0.90944982]\n [0.06395348 0.93604648]\n [0.79384029 0.20615971]\n [0.83437371 0.16562627]\n [0.67518109 0.32481888]\n [0.06374549 0.9362545 ]]\nvalidation batch: 29, val_loss: 0.08220798631001562, valid_preds_fold: [[0.07729679 0.92270327]\n [0.67497569 0.32502434]\n [0.38358271 0.61641729]\n [0.64003515 0.35996488]\n [0.73045111 0.26954886]\n [0.77482444 0.22517559]\n [0.86827421 0.13172576]\n [0.89970613 0.10029385]]\nvalidation batch: 30, val_loss: 0.08535872437875636, valid_preds_fold: [[0.87520373 0.12479632]\n [0.85003561 0.14996442]\n [0.89022171 0.10977829]\n [0.84423459 0.1557654 ]\n [0.80850708 0.1914929 ]\n [0.85204417 0.14795586]\n [0.8179065  0.18209347]\n [0.81454492 0.18545504]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 31, val_loss: 0.08639713180978802, valid_preds_fold: [[0.91992074 0.08007926]\n [0.9235481  0.07645182]\n [0.86583126 0.13416873]\n [0.70940816 0.29059187]\n [0.90403759 0.09596247]\n [0.77913243 0.22086754]\n [0.03724131 0.96275872]\n [0.90547675 0.09452324]]\nvalidation batch: 32, val_loss: 0.08809240766032768, valid_preds_fold: [[0.85921174 0.14078821]\n [0.00989301 0.99010694]\n [0.07435723 0.92564279]\n [0.00827257 0.99172747]\n [0.04733323 0.95266676]\n [0.0167468  0.98325318]\n [0.11205936 0.88794065]\n [0.75986093 0.24013908]]\nvalidation batch: 33, val_loss: 0.09170700386710409, valid_preds_fold: [[0.00820702 0.99179298]\n [0.9098599  0.09014013]\n [0.75531715 0.24468282]\n [0.90765142 0.09234857]\n [0.79354751 0.20645255]\n [0.02911306 0.97088695]\n [0.0121697  0.98783022]\n [0.84462368 0.15537639]]\nvalidation batch: 34, val_loss: 0.0951075208556913, valid_preds_fold: [[0.67753744 0.32246259]\n [0.77637976 0.22362018]\n [0.02721869 0.97278124]\n [0.91380984 0.08619022]\n [0.93239558 0.06760444]\n [0.89707792 0.10292207]\n [0.91767716 0.08232281]\n [0.92475182 0.07524814]]\nvalidation batch: 35, val_loss: 0.09690271399534531, valid_preds_fold: [[0.83942008 0.16057988]\n [0.89142305 0.10857695]\n [0.82045811 0.17954192]\n [0.35775509 0.64224494]\n [0.85849684 0.14150316]\n [0.91106528 0.08893467]\n [0.89093119 0.10906883]\n [0.91342747 0.08657252]]\nvalidation batch: 36, val_loss: 0.1028093124066826, valid_preds_fold: [[0.87465847 0.12534155]\n [0.87027347 0.12972648]\n [0.0647857  0.93521434]\n [0.05517386 0.94482619]\n [0.89256597 0.10743404]\n [0.82228738 0.17771263]\n [0.0741882  0.92581183]\n [0.51948082 0.48051918]]\nvalidation batch: 37, val_loss: 0.10566637360484059, valid_preds_fold: [[0.00329027 0.9967097 ]\n [0.77735394 0.22264613]\n [0.72922248 0.27077746]\n [0.87550706 0.12449298]\n [0.85750872 0.14249136]\n [0.76277423 0.23722576]\n [0.91294122 0.08705875]\n [0.85219407 0.14780599]]\nvalidation batch: 38, val_loss: 0.11046082548198907, valid_preds_fold: [[0.87284636 0.12715364]\n [0.89419705 0.105803  ]\n [0.89805692 0.10194302]\n [0.91980112 0.08019885]\n [0.9233495  0.0766505 ]\n [0.85547358 0.14452642]\n [0.78919995 0.21080013]\n [0.28797263 0.71202737]]\nvalidation batch: 39, val_loss: 0.11403823491648164, valid_preds_fold: [[0.78823745 0.21176259]\n [0.74440235 0.25559768]\n [0.01879722 0.98120284]\n [0.48810339 0.51189667]\n [0.12102982 0.87897015]\n [0.15091579 0.8490842 ]\n [0.24448308 0.75551695]\n [0.86637783 0.13362217]]\nvalidation batch: 40, val_loss: 0.11810548750370957, valid_preds_fold: [[0.10058881 0.89941126]\n [0.78338832 0.21661176]\n [0.00411969 0.99588037]\n [0.00750647 0.99249351]\n [0.00747711 0.9925229 ]\n [0.91145992 0.08854006]\n [0.92593354 0.07406649]\n [0.89818656 0.10181339]]\nvalidation batch: 41, val_loss: 0.12109842640857625, valid_preds_fold: [[0.00973011 0.9902699 ]\n [0.9425776  0.05742236]\n [0.91877222 0.08122782]\n [0.7626487  0.23735131]\n [0.63686013 0.36313993]\n [0.91294891 0.08705115]\n [0.60782683 0.39217317]\n [0.81051683 0.18948315]]\nvalidation batch: 42, val_loss: 0.12221042208210393, valid_preds_fold: [[0.91733307 0.08266695]\n [0.91326666 0.08673333]\n [0.88898075 0.11101929]\n [0.71811467 0.28188533]\n [0.89025974 0.10974024]\n [0.85699016 0.14300981]\n [0.9056915  0.09430854]\n [0.79987305 0.20012696]]\nvalidation batch: 43, val_loss: 0.12730869101564377, valid_preds_fold: [[0.31068873 0.68931127]\n [0.04089044 0.95910954]\n [0.03941941 0.96058059]\n [0.63296795 0.36703208]\n [0.8505519  0.14944814]\n [0.70490921 0.29509079]\n [0.84595472 0.15404528]\n [0.00728442 0.99271554]]\nvalidation batch: 44, val_loss: 0.1289708021022107, valid_preds_fold: [[0.881073   0.11892702]\n [0.0245451  0.97545493]\n [0.66760612 0.33239388]\n [0.815781   0.18421903]\n [0.07994271 0.92005736]\n [0.87396163 0.12603836]\n [0.91010827 0.08989178]\n [0.05155735 0.94844264]]\nvalidation batch: 45, val_loss: 0.13088536093922423, valid_preds_fold: [[0.02707487 0.97292513]\n [0.01784447 0.9821555 ]\n [0.01400271 0.98599732]\n [0.07850912 0.92149091]\n [0.77634847 0.2236516 ]\n [0.76999784 0.23000216]\n [0.90340853 0.09659145]\n [0.90807813 0.0919219 ]]\nvalidation batch: 46, val_loss: 0.13487387318028146, valid_preds_fold: [[0.8556959  0.14430405]\n [0.78700989 0.21299006]\n [0.87110507 0.12889488]\n [0.86744612 0.13255386]\n [0.94214261 0.05785742]\n [0.87993431 0.12006576]\n [0.83296829 0.16703168]\n [0.78681624 0.21318379]]\nvalidation batch: 47, val_loss: 0.13621333497066562, valid_preds_fold: [[0.85349941 0.14650057]\n [0.89141059 0.10858935]\n [0.80898613 0.19101384]\n [0.70416844 0.29583156]\n [0.8215192  0.17848086]\n [0.90695196 0.09304803]\n [0.71542317 0.2845768 ]\n [0.00282778 0.9971723 ]]\nvalidation batch: 48, val_loss: 0.1407220056348473, valid_preds_fold: [[0.7102015  0.28979853]\n [0.81224358 0.18775643]\n [0.00256143 0.99743855]\n [0.29501006 0.70498997]\n [0.102074   0.89792603]\n [0.10253514 0.89746487]\n [0.93786407 0.06213595]\n [0.89919335 0.10080663]]\nvalidation batch: 49, val_loss: 0.14387952384069883, valid_preds_fold: [[0.04461154 0.95538849]\n [0.92601502 0.073985  ]\n [0.93527555 0.06472445]\n [0.8910988  0.10890119]\n [0.85220945 0.14779058]\n [0.81215054 0.18784948]\n [0.78279793 0.21720214]\n [0.71663082 0.28336921]]\nvalidation batch: 50, val_loss: 0.1486504932280874, valid_preds_fold: [[0.89602524 0.10397472]\n [0.35951698 0.64048296]\n [0.76055962 0.23944041]\n [0.63491762 0.36508241]\n [0.01137942 0.98862058]\n [0.01216147 0.98783851]\n [0.03722736 0.96277261]\n [0.0209187  0.97908127]]\nvalidation batch: 51, val_loss: 0.1541795693065998, valid_preds_fold: [[0.00471107 0.99528897]\n [0.91242033 0.08757973]\n [0.9072262  0.09277378]\n [0.01042287 0.98957717]\n [0.39414468 0.60585535]\n [0.75671178 0.24328819]\n [0.07659353 0.92340642]\n [0.01279583 0.98720425]]\nvalidation batch: 52, val_loss: 0.1570009821414077, valid_preds_fold: [[0.60186863 0.39813131]\n [0.01404505 0.98595494]\n [0.67829943 0.32170057]\n [0.92500913 0.07499091]\n [0.9159143  0.08408568]\n [0.88382727 0.11617269]\n [0.90027958 0.09972043]\n [0.83269107 0.1673089 ]]\nvalidation batch: 53, val_loss: 0.1580121695016422, valid_preds_fold: [[0.00524109 0.9947589 ]\n [0.69584358 0.30415642]\n [0.87945455 0.12054541]\n [0.00536744 0.9946326 ]\n [0.70977926 0.29022071]\n [0.834575   0.16542496]\n [0.07278021 0.92721987]\n [0.0073156  0.99268442]]\nvalidation batch: 54, val_loss: 0.15981630127142812, valid_preds_fold: [[0.00673964 0.99326032]\n [0.22691745 0.77308255]\n [0.75866807 0.24133189]\n [0.28225061 0.71774942]\n [0.00664336 0.99335665]\n [0.86746061 0.13253938]\n [0.01828436 0.98171562]\n [0.00473041 0.9952696 ]]\nvalidation batch: 55, val_loss: 0.16107692362835804, valid_preds_fold: [[0.00872882 0.99127126]\n [0.72242087 0.27757916]\n [0.12312064 0.87687933]\n [0.18797493 0.81202507]\n [0.89530027 0.10469975]\n [0.85639924 0.14360078]\n [0.8159343  0.1840657 ]\n [0.78735083 0.21264917]]\nvalidation batch: 56, val_loss: 0.16838876723590554, valid_preds_fold: [[0.0360939  0.96390611]\n [0.68976605 0.31023395]\n [0.0180039  0.98199606]\n [0.28889197 0.71110803]\n [0.03703104 0.96296889]\n [0.08485986 0.91514009]\n [0.27784774 0.72215223]\n [0.79384398 0.20615608]]\nvalidation batch: 57, val_loss: 0.17296877073763056, valid_preds_fold: [[0.03077759 0.96922237]\n [0.48267072 0.51732928]\n [0.40772977 0.59227026]\n [0.73315239 0.26684758]\n [0.19196551 0.80803454]\n [0.77817237 0.22182757]\n [0.11973909 0.88026088]\n [0.01290629 0.98709375]]\nvalidation batch: 58, val_loss: 0.17379004905258646, valid_preds_fold: [[0.8685804  0.1314196 ]\n [0.94448769 0.0555123 ]\n [0.93079168 0.0692083 ]\n [0.0463772  0.95362282]\n [0.09436596 0.90563399]\n [0.25765261 0.74234742]\n [0.12222907 0.87777096]\n [0.05396802 0.94603193]]\nvalidation batch: 59, val_loss: 0.17401724618716827, valid_preds_fold: [[0.00528326 0.9947167 ]\n [0.07016995 0.92983007]\n [0.09435338 0.90564662]\n [0.00545804 0.99454194]\n [0.0397561  0.96024388]\n [0.00593265 0.99406731]\n [0.00499956 0.99500042]\n [0.01473797 0.9852621 ]]\nvalidation batch: 60, val_loss: 0.176168670401956, valid_preds_fold: [[0.59889841 0.40110165]\n [0.0067461  0.99325389]\n [0.08502335 0.9149766 ]\n [0.00436058 0.99563944]\n [0.89391381 0.10608614]\n [0.82698059 0.17301945]\n [0.8387984  0.16120163]\n [0.71843636 0.28156361]]\nvalidation batch: 61, val_loss: 0.1770772042065641, valid_preds_fold: [[0.91408813 0.08591189]\n [0.84284866 0.15715137]\n [0.91820478 0.08179518]\n [0.8843146  0.1156854 ]\n [0.87067926 0.12932076]\n [0.93616569 0.06383436]\n [0.90488791 0.09511208]\n [0.80068219 0.19931777]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 62, val_loss: 0.18138631982524894, valid_preds_fold: [[0.82726794 0.172732  ]\n [0.85341543 0.1465845 ]\n [0.87073517 0.12926485]\n [0.7252171  0.27478293]\n [0.35356873 0.64643127]\n [0.70368314 0.29631692]\n [0.29681653 0.70318347]\n [0.07430372 0.92569631]]\nvalidation batch: 63, val_loss: 0.18614607657829335, valid_preds_fold: [[0.87349051 0.12650955]\n [0.15436259 0.84563738]\n [0.66682607 0.33317393]\n [0.14588073 0.8541193 ]\n [0.01478257 0.98521739]\n [0.00578671 0.99421328]\n [0.36905715 0.63094288]\n [0.60886753 0.39113241]]\nvalidation batch: 64, val_loss: 0.18854058746003752, valid_preds_fold: [[0.00967501 0.99032497]\n [0.17086287 0.82913709]\n [0.03250336 0.96749663]\n [0.1339     0.86610001]\n [0.0196152  0.98038483]\n [0.02919033 0.97080964]\n [0.79457229 0.20542766]\n [0.90104514 0.09895479]]\nvalidation batch: 65, val_loss: 0.18928690401524517, valid_preds_fold: [[0.71630138 0.28369862]\n [0.87199217 0.1280079 ]\n [0.04127893 0.95872104]\n [0.00952358 0.99047637]\n [0.00582559 0.99417442]\n [0.01330591 0.9866941 ]\n [0.90604609 0.09395393]\n [0.83718836 0.16281162]]\nvalidation batch: 66, val_loss: 0.19220704259010993, valid_preds_fold: [[0.72444654 0.27555344]\n [0.91417301 0.08582694]\n [0.75175154 0.2482485 ]\n [0.14907491 0.85092503]\n [0.01937625 0.98062378]\n [0.77524507 0.22475497]\n [0.8864823  0.11351778]\n [0.81456828 0.18543178]]\nvalidation batch: 67, val_loss: 0.1940749914650499, valid_preds_fold: [[0.9278394  0.07216064]\n [0.92190224 0.07809781]\n [0.89497226 0.10502775]\n [0.42875683 0.57124317]\n [0.87939161 0.12060839]\n [0.8500818  0.1499182 ]\n [0.53449589 0.46550411]\n [0.01572758 0.98427236]]\nvalidation batch: 68, val_loss: 0.19898838723880527, valid_preds_fold: [[0.40428162 0.59571838]\n [0.74909043 0.25090963]\n [0.64235193 0.35764807]\n [0.54968911 0.45031083]\n [0.00983081 0.99016917]\n [0.33888301 0.66111696]\n [0.10106232 0.8989377 ]\n [0.35195774 0.64804226]]\nvalidation batch: 69, val_loss: 0.20314895155003468, valid_preds_fold: [[0.02361263 0.97638738]\n [0.60132545 0.39867455]\n [0.88865542 0.11134458]\n [0.61798    0.38201997]\n [0.88513392 0.11486612]\n [0.89567405 0.10432593]\n [0.85668701 0.14331296]\n [0.46198797 0.53801203]]\nvalidation batch: 70, val_loss: 0.2066586348805984, valid_preds_fold: [[0.05572985 0.94427013]\n [0.77183872 0.22816128]\n [0.81888676 0.18111321]\n [0.90591341 0.0940866 ]\n [0.83341736 0.16658266]\n [0.91713357 0.08286638]\n [0.7979809  0.20201913]\n [0.86516869 0.13483128]]\nvalidation batch: 71, val_loss: 0.21428569917478696, valid_preds_fold: [[0.88964552 0.11035445]\n [0.86075842 0.13924164]\n [0.19643281 0.80356717]\n [0.29985011 0.70014989]\n [0.00958654 0.99041349]\n [0.09642711 0.90357286]\n [0.3091962  0.69080377]\n [0.17585388 0.82414615]]\nvalidation batch: 72, val_loss: 0.2146254017078963, valid_preds_fold: [[0.02380452 0.97619551]\n [0.90340793 0.09659206]\n [0.00511342 0.99488658]\n [0.00850721 0.99149281]\n [0.07189872 0.92810124]\n [0.91720074 0.0827992 ]\n [0.93390894 0.06609111]\n [0.00354447 0.99645555]]\nvalidation batch: 73, val_loss: 0.21620519312411324, valid_preds_fold: [[0.00275944 0.99724054]\n [0.41698411 0.58301592]\n [0.02170113 0.9782989 ]\n [0.01910834 0.98089164]\n [0.03457813 0.96542192]\n [0.38717061 0.61282933]\n [0.10849222 0.8915078 ]\n [0.04781233 0.95218772]]\nvalidation batch: 74, val_loss: 0.21806848805098633, valid_preds_fold: [[0.33546165 0.66453838]\n [0.03065777 0.96934223]\n [0.07425414 0.92574584]\n [0.71594638 0.28405359]\n [0.90812552 0.09187446]\n [0.88081926 0.11918075]\n [0.24383761 0.75616241]\n [0.00469496 0.99530506]]\nvalidation batch: 75, val_loss: 0.21874806442617495, valid_preds_fold: [[0.00333079 0.99666923]\n [0.06372262 0.93627739]\n [0.88460195 0.11539803]\n [0.86275083 0.13724923]\n [0.88944185 0.1105582 ]\n [0.90770143 0.09229862]\n [0.89996266 0.10003734]\n [0.91760707 0.08239287]]\nvalidation batch: 76, val_loss: 0.22307491786506048, valid_preds_fold: [[0.86483723 0.13516282]\n [0.2033743  0.79662567]\n [0.77055299 0.22944698]\n [0.88963228 0.11036768]\n [0.86766988 0.13233015]\n [0.76836562 0.23163439]\n [0.01780235 0.98219758]\n [0.10981806 0.8901819 ]]\nvalidation batch: 77, val_loss: 0.2261833231488283, valid_preds_fold: [[0.77755755 0.22244251]\n [0.12373445 0.87626553]\n [0.67672145 0.32327852]\n [0.84529537 0.15470469]\n [0.89646631 0.10353374]\n [0.62038684 0.3796131 ]\n [0.21440378 0.78559619]\n [0.19465141 0.80534863]]\nvalidation batch: 78, val_loss: 0.22883882601983352, valid_preds_fold: [[0.92832279 0.07167715]\n [0.13533998 0.86466002]\n [0.12421175 0.87578821]\n [0.62915349 0.37084651]\n [0.66001159 0.33998841]\n [0.20176642 0.79823357]\n [0.05018769 0.94981229]\n [0.41740611 0.58259386]]\nvalidation batch: 79, val_loss: 0.23238896241370774, valid_preds_fold: [[0.3993811  0.60061896]\n [0.41442305 0.58557695]\n [0.79620743 0.20379257]\n [0.02241615 0.97758389]\n [0.12419918 0.87580085]\n [0.01094408 0.98905599]\n [0.73808545 0.26191455]\n [0.03110779 0.96889222]]\nvalidation batch: 80, val_loss: 0.2345257576038367, valid_preds_fold: [[0.1085799  0.89142007]\n [0.85044426 0.1495557 ]\n [0.05274152 0.94725853]\n [0.25710055 0.74289942]\n [0.911376   0.08862403]\n [0.62006491 0.37993512]\n [0.01102921 0.98897088]\n [0.06825785 0.93174219]]\nvalidation batch: 81, val_loss: 0.23701259782061954, valid_preds_fold: [[0.00677386 0.99322611]\n [0.57029951 0.42970049]\n [0.78162688 0.2183731 ]\n [0.70276088 0.29723909]\n [0.00270638 0.99729365]\n [0.05944125 0.94055873]\n [0.0026741  0.9973259 ]\n [0.70620614 0.29379386]]\nvalidation batch: 82, val_loss: 0.2385800009335044, valid_preds_fold: [[0.00725323 0.99274677]\n [0.86698127 0.13301875]\n [0.81824738 0.18175265]\n [0.86450654 0.13549343]\n [0.76554531 0.23445475]\n [0.79445565 0.20554437]\n [0.05632004 0.94367993]\n [0.51353562 0.48646441]]\nvalidation batch: 83, val_loss: 0.24067923018749607, valid_preds_fold: [[0.62160975 0.37839025]\n [0.7120384  0.28796157]\n [0.46561229 0.53438771]\n [0.88255847 0.1174415 ]\n [0.88488442 0.11511559]\n [0.90242946 0.09757052]\n [0.87971371 0.12028623]\n [0.78408706 0.21591294]]\nvalidation batch: 84, val_loss: 0.24535361960203972, valid_preds_fold: [[0.89768898 0.10231104]\n [0.22855775 0.77144229]\n [0.80685711 0.19314291]\n [0.71815801 0.28184205]\n [0.70090026 0.29909968]\n [0.00345219 0.99654788]\n [0.76656574 0.23343423]\n [0.89134413 0.10865584]]\nvalidation batch: 85, val_loss: 0.2464133440686838, valid_preds_fold: [[0.90955967 0.09044032]\n [0.00364355 0.99635649]\n [0.23890476 0.76109523]\n [0.8922447  0.1077553 ]\n [0.62087607 0.37912399]\n [0.90268499 0.09731508]\n [0.02192619 0.97807378]\n [0.07209467 0.92790526]]\nvalidation batch: 86, val_loss: 0.2504673065604084, valid_preds_fold: [[0.46045643 0.53954357]\n [0.01358083 0.98641914]\n [0.89693177 0.10306828]\n [0.02074854 0.9792515 ]\n [0.13468587 0.86531413]\n [0.76351565 0.23648435]\n [0.6782524  0.32174757]\n [0.89085823 0.10914174]]\nvalidation batch: 87, val_loss: 0.2553818008869233, valid_preds_fold: [[0.9334898  0.06651017]\n [0.88430208 0.11569795]\n [0.13226621 0.86773378]\n [0.05672852 0.94327152]\n [0.11650678 0.88349319]\n [0.16719455 0.83280545]\n [0.23126458 0.76873535]\n [0.00764792 0.99235207]]\nvalidation batch: 88, val_loss: 0.25734404126440513, valid_preds_fold: [[0.32445684 0.67554313]\n [0.31934547 0.68065459]\n [0.83280158 0.16719845]\n [0.0458449  0.95415509]\n [0.03427851 0.96572143]\n [0.00542284 0.99457717]\n [0.90319699 0.09680299]\n [0.76468486 0.23531513]]\nvalidation batch: 89, val_loss: 0.25977643198558004, valid_preds_fold: [[0.89300096 0.10699909]\n [0.90770543 0.09229459]\n [0.90109456 0.09890544]\n [0.86790639 0.13209356]\n [0.81711346 0.1828865 ]\n [0.88392329 0.11607673]\n [0.90246993 0.09753007]\n [0.24812597 0.75187403]]\nvalidation batch: 90, val_loss: 0.2636996827965234, valid_preds_fold: [[0.90340114 0.09659888]\n [0.81684279 0.18315716]\n [0.61876971 0.38123032]\n [0.82873678 0.17126323]\n [0.01266005 0.98733991]\n [0.01637917 0.98362088]\n [0.8567996  0.1432004 ]\n [0.25785831 0.74214172]]\nvalidation batch: 91, val_loss: 0.2659548593372323, valid_preds_fold: [[0.30157995 0.69842005]\n [0.13129075 0.86870921]\n [0.10466579 0.89533418]\n [0.01491473 0.98508531]\n [0.78612554 0.21387444]\n [0.58622545 0.41377458]\n [0.86560237 0.13439761]\n [0.91612577 0.08387424]]\nvalidation batch: 92, val_loss: 0.2677820749635243, valid_preds_fold: [[0.83972174 0.16027822]\n [0.84423035 0.15576965]\n [0.83698964 0.16301039]\n [0.90314049 0.09685951]\n [0.88598865 0.11401131]\n [0.92056084 0.07943918]\n [0.49832454 0.50167543]\n [0.61559373 0.3844063 ]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 93, val_loss: 0.26977019398099306, valid_preds_fold: [[0.8165459  0.1834541 ]\n [0.78793001 0.21206994]\n [0.78877294 0.21122704]\n [0.01748394 0.98251605]\n [0.11824298 0.88175702]\n [0.01751227 0.98248774]\n [0.01901448 0.98098546]\n [0.00781711 0.99218291]]\nvalidation batch: 94, val_loss: 0.27262913729370064, valid_preds_fold: [[0.15661991 0.84338015]\n [0.00810445 0.99189562]\n [0.00673007 0.99326998]\n [0.00447251 0.99552745]\n [0.83300561 0.16699436]\n [0.91153651 0.08846352]\n [0.77338868 0.22661139]\n [0.92418969 0.07581033]]\nvalidation batch: 95, val_loss: 0.2755529058784463, valid_preds_fold: [[0.82582676 0.17417328]\n [0.79637444 0.20362556]\n [0.90894079 0.09105927]\n [0.85072613 0.14927384]\n [0.87948769 0.1205123 ]\n [0.85611671 0.14388333]\n [0.92922837 0.07077161]\n [0.45922643 0.54077357]]\nvalidation batch: 96, val_loss: 0.27682202940222106, valid_preds_fold: [[0.87486792 0.12513205]\n [0.1753386  0.82466143]\n [0.10935566 0.89064437]\n [0.34854457 0.65145546]\n [0.87528747 0.12471258]\n [0.87527227 0.12472775]\n [0.89172035 0.10827959]\n [0.87012994 0.12987007]]\nvalidation batch: 97, val_loss: 0.28339467404315055, valid_preds_fold: [[0.8635633  0.13643675]\n [0.87055874 0.12944125]\n [0.74165201 0.25834802]\n [0.76387948 0.23612054]\n [0.01640108 0.98359895]\n [0.89589912 0.10410084]\n [0.01666815 0.9833318 ]\n [0.01275782 0.98724222]]\nvalidation batch: 98, val_loss: 0.2876616909447376, valid_preds_fold: [[0.02006633 0.97993374]\n [0.16955201 0.83044803]\n [0.10378587 0.89621413]\n [0.00236237 0.99763763]\n [0.90948063 0.09051939]\n [0.27579042 0.72420961]\n [0.45617604 0.5438239 ]\n [0.03432483 0.96567518]]\nvalidation batch: 99, val_loss: 0.2903294155958793, valid_preds_fold: [[0.8720625  0.12793757]\n [0.00844953 0.99155045]\n [0.01181426 0.98818576]\n [0.00857237 0.99142766]\n [0.50293368 0.49706632]\n [0.04224677 0.95775324]\n [0.00851278 0.99148715]\n [0.09482115 0.90517884]]\nvalidation batch: 100, val_loss: 0.292941084471497, valid_preds_fold: [[0.52752668 0.47247329]\n [0.15351914 0.84648091]\n [0.34594584 0.65405416]\n [0.01318299 0.98681706]\n [0.68147272 0.31852728]\n [0.02960603 0.97039402]\n [0.62326312 0.37673688]\n [0.90929717 0.09070288]]\nvalidation batch: 101, val_loss: 0.29331811484846737, valid_preds_fold: [[0.01380998 0.98618996]\n [0.00638168 0.99361837]\n [0.92499638 0.07500365]\n [0.86766034 0.13233966]\n [0.01587965 0.98412031]\n [0.01305097 0.98694903]\n [0.01139287 0.98860711]\n [0.87599564 0.12400433]]\nvalidation batch: 102, val_loss: 0.2945940477678374, valid_preds_fold: [[0.01350035 0.98649967]\n [0.01203262 0.98796743]\n [0.461436   0.53856397]\n [0.00483908 0.99516088]\n [0.01461    0.98538995]\n [0.90728229 0.0927177 ]\n [0.88859862 0.11140138]\n [0.59517729 0.40482271]]\nvalidation batch: 103, val_loss: 0.2989107949564055, valid_preds_fold: [[0.90593278 0.09406715]\n [0.1081674  0.89183265]\n [0.46056396 0.53943604]\n [0.26618043 0.7338196 ]\n [0.71420085 0.28579915]\n [0.47890076 0.52109921]\n [0.11307149 0.8869285 ]\n [0.37397042 0.62602961]]\nvalidation batch: 104, val_loss: 0.30165959889218735, valid_preds_fold: [[0.30902946 0.69097054]\n [0.58281118 0.41718882]\n [0.32786685 0.67213321]\n [0.66388839 0.33611163]\n [0.00464413 0.99535584]\n [0.46763    0.53236997]\n [0.89703631 0.10296372]\n [0.91531867 0.08468135]]\nvalidation batch: 105, val_loss: 0.30363556856874113, valid_preds_fold: [[0.87799084 0.12200913]\n [0.89637327 0.10362671]\n [0.90863371 0.09136631]\n [0.02584388 0.97415608]\n [0.02041628 0.97958368]\n [0.0164824  0.98351753]\n [0.91730958 0.08269048]\n [0.81373602 0.18626399]]\nvalidation batch: 106, val_loss: 0.3081489697533801, valid_preds_fold: [[0.87943852 0.12056144]\n [0.84338021 0.15661977]\n [0.9059189  0.09408109]\n [0.8639074  0.13609257]\n [0.88182914 0.11817084]\n [0.79204869 0.20795134]\n [0.74299556 0.25700444]\n [0.82790434 0.17209564]]\nvalidation batch: 107, val_loss: 0.3117552698202375, valid_preds_fold: [[0.63829935 0.36170065]\n [0.83532679 0.16467319]\n [0.01531652 0.98468345]\n [0.31570709 0.68429291]\n [0.06332569 0.93667436]\n [0.01459041 0.98540962]\n [0.70035124 0.29964882]\n [0.09071635 0.90928364]]\nvalidation batch: 108, val_loss: 0.3203449848033214, valid_preds_fold: [[0.94026297 0.05973705]\n [0.83163381 0.16836618]\n [0.95004195 0.04995807]\n [0.88543665 0.11456335]\n [0.88997954 0.11002044]\n [0.35381013 0.64618987]\n [0.83163381 0.16836618]\n [0.87973285 0.12026718]]\nvalidation batch: 109, val_loss: 0.32400431451353695, valid_preds_fold: [[0.83360672 0.16639328]\n [0.68669641 0.31330353]\n [0.07496502 0.92503494]\n [0.56471342 0.43528661]\n [0.79314983 0.20685013]\n [0.89062434 0.10937563]\n [0.79608375 0.20391628]\n [0.87752718 0.12247282]]\nvalidation batch: 110, val_loss: 0.3259739874161941, valid_preds_fold: [[0.26511329 0.73488671]\n [0.87645632 0.12354366]\n [0.77842289 0.22157709]\n [0.89324349 0.10675644]\n [0.91381073 0.08618923]\n [0.93450743 0.06549252]\n [0.91174191 0.08825812]\n [0.91791743 0.08208255]]\nvalidation batch: 111, val_loss: 0.328351183148631, valid_preds_fold: [[0.77392924 0.22607076]\n [0.74188447 0.25811553]\n [0.4680022  0.5319978 ]\n [0.3501631  0.6498369 ]\n [0.92667514 0.07332482]\n [0.92072028 0.07927966]\n [0.81285143 0.18714859]\n [0.00413075 0.99586922]]\nvalidation batch: 112, val_loss: 0.32926050573587395, valid_preds_fold: [[0.02495811 0.97504187]\n [0.03044759 0.9695524 ]\n [0.85791433 0.14208572]\n [0.90157682 0.09842321]\n [0.87163478 0.12836519]\n [0.9024018  0.09759825]\n [0.00723419 0.99276578]\n [0.64647537 0.35352466]]\nvalidation batch: 113, val_loss: 0.329813072281162, valid_preds_fold: [[0.02641852 0.97358149]\n [0.35672426 0.64327574]\n [0.09450091 0.9054991 ]\n [0.00875544 0.99124449]\n [0.00803353 0.99196649]\n [0.0085026  0.99149734]\n [0.00556303 0.99443698]\n [0.00738344 0.99261647]]\nvalidation batch: 114, val_loss: 0.32990599494345846, valid_preds_fold: [[0.00920991 0.99079013]\n [0.00850811 0.99149185]\n [0.00783685 0.99216312]\n [0.00542607 0.99457401]\n [0.01385645 0.98614353]\n [0.00705075 0.99294925]\n [0.03826004 0.96174002]\n [0.01064027 0.98935974]]\nvalidation batch: 115, val_loss: 0.3336506058482356, valid_preds_fold: [[0.00544586 0.99455416]\n [0.38690451 0.61309552]\n [0.82742053 0.17257942]\n [0.8634814  0.13651854]\n [0.91239947 0.08760053]\n [0.83189279 0.16810718]\n [0.88139194 0.11860809]\n [0.91026086 0.08973911]]\nvalidation batch: 116, val_loss: 0.3351857016121382, valid_preds_fold: [[0.86291057 0.13708946]\n [0.82927269 0.1707273 ]\n [0.67233378 0.32766622]\n [0.73328841 0.26671159]\n [0.83907771 0.16092232]\n [0.00824792 0.99175209]\n [0.66770941 0.33229056]\n [0.05158333 0.94841665]]\nvalidation batch: 117, val_loss: 0.33870471981320044, valid_preds_fold: [[0.01791275 0.98208719]\n [0.34548151 0.65451849]\n [0.04836164 0.95163834]\n [0.26835558 0.73164445]\n [0.04232392 0.95767611]\n [0.13856991 0.86143011]\n [0.68431598 0.31568405]\n [0.18132536 0.81867468]]\nvalidation batch: 118, val_loss: 0.34212916828420015, valid_preds_fold: [[0.01829269 0.98170727]\n [0.84605378 0.15394627]\n [0.08913085 0.91086912]\n [0.08249944 0.9175005 ]\n [0.18031327 0.81968677]\n [0.00722267 0.99277735]\n [0.90710837 0.09289167]\n [0.44677523 0.55322474]]\nvalidation batch: 119, val_loss: 0.34513952501498846, valid_preds_fold: [[0.66945368 0.33054632]\n [0.01569401 0.98430598]\n [0.75092959 0.24907041]\n [0.4107016  0.58929843]\n [0.1289532  0.87104678]\n [0.0047936  0.99520642]\n [0.00431465 0.99568534]\n [0.00949304 0.99050689]]\nvalidation batch: 120, val_loss: 0.34985652957519453, valid_preds_fold: [[0.02209258 0.97790742]\n [0.72718447 0.27281553]\n [0.91496444 0.08503554]\n [0.03779166 0.96220833]\n [0.56434935 0.43565068]\n [0.01549551 0.98450446]\n [0.01458877 0.98541129]\n [0.73421061 0.26578936]]\nvalidation batch: 121, val_loss: 0.3536507080506232, valid_preds_fold: [[0.69724327 0.30275667]\n [0.76903725 0.23096278]\n [0.89654785 0.10345213]\n [0.86258525 0.13741471]\n [0.47353986 0.52646017]\n [0.84200013 0.15799986]\n [0.58193684 0.41806316]\n [0.24697596 0.75302404]]\nvalidation batch: 122, val_loss: 0.3555710002453656, valid_preds_fold: [[0.781569   0.21843097]\n [0.60371518 0.39628482]\n [0.80863708 0.19136286]\n [0.89729136 0.10270865]\n [0.76272452 0.23727553]\n [0.61289918 0.38710085]\n [0.90149081 0.09850926]\n [0.84481531 0.15518469]]\nvalidation batch: 123, val_loss: 0.3563565561806197, valid_preds_fold: [[0.90095568 0.09904437]\n [0.86784381 0.13215612]\n [0.90573275 0.09426719]\n [0.90151489 0.09848505]\n [0.86632013 0.13367985]\n [0.90567207 0.0943279 ]\n [0.93372929 0.06627069]\n [0.90385228 0.09614775]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 124, val_loss: 0.35708911988857, valid_preds_fold: [[0.1235928  0.87640721]\n [0.01509374 0.98490626]\n [0.19470808 0.80529189]\n [0.95088696 0.04911301]\n [0.89933282 0.10066716]\n [0.93312687 0.06687316]\n [0.88386238 0.11613764]\n [0.91386163 0.08613835]]\nvalidation batch: 125, val_loss: 0.35875150833251684, valid_preds_fold: [[0.00298762 0.99701238]\n [0.03211412 0.96788591]\n [0.00293071 0.9970693 ]\n [0.02102284 0.9789772 ]\n [0.89158303 0.10841697]\n [0.68755722 0.31244281]\n [0.77837908 0.22162087]\n [0.35978308 0.64021695]]\nvalidation batch: 126, val_loss: 0.36442848615837775, valid_preds_fold: [[0.13192073 0.8680793 ]\n [0.27352938 0.72647059]\n [0.00665736 0.9933427 ]\n [0.02514737 0.97485262]\n [0.68915766 0.31084234]\n [0.75468791 0.24531212]\n [0.40588588 0.59411412]\n [0.68823808 0.31176195]]\nvalidation batch: 127, val_loss: 0.36560933803119783, valid_preds_fold: [[0.91843098 0.081569  ]\n [0.7249741  0.2750259 ]\n [0.73640865 0.26359129]\n [0.89655662 0.10344335]\n [0.91684037 0.08315966]\n [0.85565603 0.14434399]\n [0.88843429 0.11156568]\n [0.8946358  0.1053642 ]]\nvalidation batch: 128, val_loss: 0.3720880836030861, valid_preds_fold: [[0.33644074 0.66355926]\n [0.84583694 0.15416311]\n [0.04017624 0.95982373]\n [0.92455137 0.07544865]\n [0.90759063 0.09240939]\n [0.84266216 0.15733789]\n [0.85474175 0.14525825]\n [0.02202136 0.97797865]]\nvalidation batch: 129, val_loss: 0.37599568754217033, valid_preds_fold: [[0.82271242 0.17728761]\n [0.84705204 0.15294795]\n [0.77597058 0.22402947]\n [0.81224465 0.1877553 ]\n [0.83700669 0.16299333]\n [0.0236082  0.97639185]\n [0.00615901 0.99384093]\n [0.00549253 0.99450749]]\nvalidation batch: 130, val_loss: 0.37880591101890043, valid_preds_fold: [[0.03101394 0.96898603]\n [0.0062948  0.99370521]\n [0.24318969 0.75681031]\n [0.01338318 0.98661685]\n [0.00636909 0.99363095]\n [0.14380217 0.85619789]\n [0.64195138 0.35804862]\n [0.63574904 0.36425093]]\nvalidation batch: 131, val_loss: 0.3832570423174947, valid_preds_fold: [[0.66390753 0.33609244]\n [0.51872885 0.48127118]\n [0.67612886 0.32387114]\n [0.60907501 0.39092499]\n [0.14969389 0.85030609]\n [0.55261481 0.44738516]\n [0.25682083 0.74317914]\n [0.92811418 0.07188577]]\nvalidation batch: 132, val_loss: 0.3842970163282685, valid_preds_fold: [[0.01670481 0.98329514]\n [0.01694351 0.98305649]\n [0.73875433 0.26124564]\n [0.74158722 0.25841272]\n [0.01473436 0.98526567]\n [0.00635532 0.99364471]\n [0.01012496 0.98987508]\n [0.3767007  0.62329936]]\nvalidation batch: 133, val_loss: 0.38848329939111287, valid_preds_fold: [[0.09109386 0.9089061 ]\n [0.87754226 0.12245777]\n [0.01535286 0.98464715]\n [0.81841213 0.18158787]\n [0.35220379 0.64779621]\n [0.91580981 0.08419017]\n [0.06007371 0.93992627]\n [0.52081287 0.47918713]]\nvalidation batch: 134, val_loss: 0.3893031134222544, valid_preds_fold: [[0.75978267 0.24021728]\n [0.8737278  0.12627217]\n [0.71393079 0.28606921]\n [0.92303431 0.07696572]\n [0.03209765 0.9679023 ]\n [0.00678787 0.9932121 ]\n [0.01908149 0.98091847]\n [0.01296333 0.98703671]]\nvalidation batch: 135, val_loss: 0.38994638681629257, valid_preds_fold: [[0.71203685 0.28796318]\n [0.92512971 0.07487025]\n [0.86647332 0.13352667]\n [0.91713566 0.08286428]\n [0.00663341 0.99336654]\n [0.01424723 0.98575276]\n [0.0168219  0.98317808]\n [0.0195923  0.98040766]]\nvalidation batch: 136, val_loss: 0.3909340279489536, valid_preds_fold: [[0.12655216 0.87344784]]\n\n","name":"stdout"},{"output_type":"stream","text":"[2020-02-01 13:52:31,882][INFO] ## epoch: 0, train loss: 0.45056330, valid loss: 0.39093403, acc: 0.83379247, f1: 0.83003112, best_f1: 0.83003112\n\n[2020-02-01 13:52:31,882][INFO] ## epoch: 0, train loss: 0.45056330, valid loss: 0.39093403, acc: 0.83379247, f1: 0.83003112, best_f1: 0.83003112\n\n[2020-02-01 13:52:31,882][INFO] ## epoch: 0, train loss: 0.45056330, valid loss: 0.39093403, acc: 0.83379247, f1: 0.83003112, best_f1: 0.83003112\n\n[2020-02-01 13:52:31,882][INFO] ## epoch: 0, train loss: 0.45056330, valid loss: 0.39093403, acc: 0.83379247, f1: 0.83003112, best_f1: 0.83003112\n\n[2020-02-01 13:52:31,882][INFO] ## epoch: 0, train loss: 0.45056330, valid loss: 0.39093403, acc: 0.83379247, f1: 0.83003112, best_f1: 0.83003112\n\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b998a28f42642f48d842e3e0ae6968d"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0666a2d45de14aacb91f719b1add6201"}},"metadata":{}},{"output_type":"stream","text":"validation batch: 0, val_loss: 0.0016984528433667482, valid_preds_fold: [[0.96089286 0.03910714]\n [0.97333634 0.0266636 ]\n [0.89971596 0.10028398]\n [0.93095475 0.06904528]\n [0.37031573 0.62968427]\n [0.02046168 0.97953826]\n [0.60957938 0.39042062]\n [0.527731   0.472269  ]]\nvalidation batch: 1, val_loss: 0.002550927174352381, valid_preds_fold: [[0.00212498 0.99787498]\n [0.90592271 0.09407725]\n [0.37241068 0.62758929]\n [0.86582911 0.13417085]\n [0.03260335 0.96739668]\n [0.10978251 0.89021754]\n [0.97030389 0.02969612]\n [0.95708394 0.04291613]]\nvalidation batch: 2, val_loss: 0.004432220746130838, valid_preds_fold: [[0.93694437 0.06305566]\n [0.87981498 0.12018502]\n [0.93422168 0.06577836]\n [0.18823506 0.8117649 ]\n [0.00377665 0.99622333]\n [0.00190203 0.99809796]\n [0.08407944 0.91592062]\n [0.0364242  0.96357584]]\nvalidation batch: 3, val_loss: 0.006777433583336155, valid_preds_fold: [[0.00159205 0.99840802]\n [0.63803816 0.36196187]\n [0.89703304 0.10296696]\n [0.86526012 0.13473983]\n [0.00127209 0.99872786]\n [0.00192599 0.99807394]\n [0.00190179 0.99809819]\n [0.00122547 0.99877447]]\nvalidation batch: 4, val_loss: 0.010972537263466493, valid_preds_fold: [[0.00148507 0.99851495]\n [0.77996713 0.22003286]\n [0.87960827 0.12039175]\n [0.3985616  0.6014384 ]\n [0.43497977 0.56502026]\n [0.7096945  0.29030553]\n [0.94539499 0.05460503]\n [0.87357974 0.12642026]]\nvalidation batch: 5, val_loss: 0.012187783844279546, valid_preds_fold: [[0.91372466 0.08627535]\n [0.91331458 0.08668539]\n [0.89730114 0.10269887]\n [0.75813538 0.24186464]\n [0.95230424 0.04769578]\n [0.54027569 0.45972428]\n [0.96348542 0.03651458]\n [0.93799746 0.06200255]]\nvalidation batch: 6, val_loss: 0.016413821029837114, valid_preds_fold: [[0.9245699  0.07543007]\n [0.92507267 0.07492737]\n [0.96459699 0.03540301]\n [0.96484363 0.03515641]\n [0.92440444 0.07559561]\n [0.90011382 0.09988622]\n [0.42463091 0.57536912]\n [0.94336212 0.05663787]]\nvalidation batch: 7, val_loss: 0.02376781425771922, valid_preds_fold: [[0.66595429 0.33404574]\n [0.92069995 0.07929997]\n [0.94760877 0.05239128]\n [0.10647616 0.89352381]\n [0.01219372 0.98780626]\n [0.93166971 0.06833024]\n [0.02244638 0.97755367]\n [0.89075571 0.10924429]]\nvalidation batch: 8, val_loss: 0.024052366831876934, valid_preds_fold: [[0.06284706 0.93715298]\n [0.96777344 0.03222658]\n [0.95130181 0.04869822]\n [0.04861936 0.95138067]\n [0.96218127 0.03781869]\n [0.95276034 0.0472396 ]\n [0.02201459 0.97798538]\n [0.00522013 0.99477994]]\nvalidation batch: 9, val_loss: 0.024700065000648915, valid_preds_fold: [[0.00535378 0.99464625]\n [0.89531088 0.10468911]\n [0.81594616 0.18405382]\n [0.00478442 0.99521554]\n [0.88726079 0.1127392 ]\n [0.93997997 0.06002005]\n [0.84766173 0.15233828]\n [0.96181291 0.03818715]]\nvalidation batch: 10, val_loss: 0.02792014067407942, valid_preds_fold: [[0.95751983 0.04248016]\n [0.69891012 0.30108988]\n [0.92683238 0.07316762]\n [0.89938742 0.1006126 ]\n [0.78903592 0.21096404]\n [0.92310441 0.07689555]\n [0.91008812 0.08991186]\n [0.95211214 0.04788779]]\nvalidation batch: 11, val_loss: 0.03441530058201212, valid_preds_fold: [[0.21735257 0.78264743]\n [0.09354231 0.90645772]\n [0.45547441 0.54452556]\n [0.22739525 0.77260476]\n [0.84283561 0.15716435]\n [0.5066582  0.49334177]\n [0.21727055 0.78272939]\n [0.57814878 0.42185122]]\nvalidation batch: 12, val_loss: 0.036596737040655455, valid_preds_fold: [[0.22509564 0.77490437]\n [0.55459559 0.44540438]\n [0.61984134 0.38015869]\n [0.9732213  0.02677867]\n [0.8248021  0.17519787]\n [0.8297233  0.17027667]\n [0.9349767  0.06502334]\n [0.89987087 0.10012916]]\nvalidation batch: 13, val_loss: 0.03713594251958123, valid_preds_fold: [[0.95493972 0.04506031]\n [0.89576471 0.10423527]\n [0.98121279 0.01878715]\n [0.92489511 0.07510489]\n [0.95805985 0.04194018]\n [0.91579789 0.08420212]\n [0.9445948  0.0554052 ]\n [0.86075974 0.13924024]]\nvalidation batch: 14, val_loss: 0.039958154143643204, valid_preds_fold: [[0.92065179 0.07934824]\n [0.64932573 0.35067421]\n [0.81755954 0.18244047]\n [0.97041309 0.02958686]\n [0.91301727 0.08698276]\n [0.63164449 0.36835551]\n [0.77148098 0.22851904]\n [0.96327889 0.03672109]]\nvalidation batch: 15, val_loss: 0.04108037690829187, valid_preds_fold: [[0.91805089 0.08194914]\n [0.75405592 0.24594414]\n [0.96221691 0.03778307]\n [0.65092868 0.34907132]\n [0.97568625 0.02431372]\n [0.9287442  0.07125579]\n [0.76448518 0.23551483]\n [0.9731611  0.02683889]]\nvalidation batch: 16, val_loss: 0.04310150471699499, valid_preds_fold: [[0.94901603 0.05098394]\n [0.91004372 0.08995634]\n [0.80831093 0.19168907]\n [0.82281524 0.17718478]\n [0.93809712 0.06190285]\n [0.94708818 0.05291189]\n [0.93556827 0.06443178]\n [0.96387535 0.03612462]]\nvalidation batch: 17, val_loss: 0.04414937112235675, valid_preds_fold: [[0.9365229  0.06347705]\n [0.93677348 0.06322648]\n [0.92220926 0.07779074]\n [0.8964529  0.1035471 ]\n [0.83100045 0.16899951]\n [0.38275462 0.61724538]\n [0.89535499 0.10464501]\n [0.95206201 0.04793796]]\nvalidation batch: 18, val_loss: 0.04468768807875849, valid_preds_fold: [[0.92109984 0.07890014]\n [0.94698197 0.05301804]\n [0.97144967 0.02855029]\n [0.95595646 0.04404352]\n [0.87889129 0.12110867]\n [0.93604606 0.06395392]\n [0.8851642  0.11483579]\n [0.93973434 0.06026569]]\nvalidation batch: 19, val_loss: 0.04593999428253104, valid_preds_fold: [[0.90852433 0.0914756 ]\n [0.42471212 0.57528788]\n [0.9509908  0.04900924]\n [0.96534294 0.03465711]\n [0.88581091 0.11418908]\n [0.95429575 0.0457042 ]\n [0.95576346 0.04423651]\n [0.88563603 0.11436398]]\nvalidation batch: 20, val_loss: 0.05056093268803436, valid_preds_fold: [[0.02528077 0.97471923]\n [0.94885933 0.05114066]\n [0.96896756 0.03103244]\n [0.92967796 0.07032204]\n [0.88652724 0.11347277]\n [0.90099359 0.09900637]\n [0.00696972 0.99303025]\n [0.01272834 0.98727161]]\nvalidation batch: 21, val_loss: 0.05097909978706471, valid_preds_fold: [[0.00677991 0.99322009]\n [0.70388424 0.29611579]\n [0.01193915 0.98806083]\n [0.00997229 0.99002773]\n [0.01644345 0.98355651]\n [0.05248382 0.9475162 ]\n [0.00460115 0.99539882]\n [0.00322488 0.99677509]]\nvalidation batch: 22, val_loss: 0.055915720488903295, valid_preds_fold: [[0.01103691 0.98896307]\n [0.02728841 0.97271156]\n [0.01166939 0.9883306 ]\n [0.0421654  0.9578346 ]\n [0.0708949  0.92910504]\n [0.55617332 0.44382668]\n [0.22364005 0.77635992]\n [0.79909742 0.20090266]]\nvalidation batch: 23, val_loss: 0.06255405858485368, valid_preds_fold: [[0.34171689 0.65828311]\n [0.01370737 0.9862926 ]\n [0.76322752 0.23677251]\n [0.59199011 0.40800986]\n [0.91841483 0.08158523]\n [0.05644588 0.94355416]\n [0.00233811 0.99766195]\n [0.01060196 0.989398  ]]\nvalidation batch: 24, val_loss: 0.07059648894045474, valid_preds_fold: [[0.00814814 0.99185187]\n [0.0425955  0.95740449]\n [0.06812198 0.93187797]\n [0.84100688 0.15899308]\n [0.93164384 0.0683561 ]\n [0.93044096 0.06955908]\n [0.97151303 0.02848697]\n [0.00665994 0.99334008]]\nvalidation batch: 25, val_loss: 0.07515157552531165, valid_preds_fold: [[0.01374772 0.98625225]\n [0.82110018 0.17889985]\n [0.9658125  0.03418751]\n [0.9309538  0.06904615]\n [0.78008848 0.21991149]\n [0.94132441 0.05867555]\n [0.09477068 0.90522933]\n [0.20169233 0.79830772]]\nvalidation batch: 26, val_loss: 0.07732577371771318, valid_preds_fold: [[0.55178374 0.44821623]\n [0.9182384  0.08176167]\n [0.01514256 0.98485744]\n [0.5401786  0.45982146]\n [0.59405732 0.40594268]\n [0.04424837 0.9557516 ]\n [0.88875932 0.11124068]\n [0.00715488 0.99284512]]\nvalidation batch: 27, val_loss: 0.07959872266671954, valid_preds_fold: [[0.14786604 0.85213399]\n [0.29152283 0.7084772 ]\n [0.00493644 0.9950636 ]\n [0.01382374 0.98617631]\n [0.95211798 0.04788196]\n [0.02225121 0.97774875]\n [0.87200123 0.12799881]\n [0.00763457 0.99236542]]\nvalidation batch: 28, val_loss: 0.08351384070667908, valid_preds_fold: [[0.90782255 0.09217742]\n [0.8710236  0.12897636]\n [0.07205464 0.92794544]\n [0.05441485 0.94558513]\n [0.86617458 0.13382539]\n [0.93136394 0.068636  ]\n [0.75464487 0.2453551 ]\n [0.01812108 0.98187894]]\nvalidation batch: 29, val_loss: 0.08519320483625371, valid_preds_fold: [[0.04856747 0.95143259]\n [0.78107661 0.21892338]\n [0.45061654 0.54938352]\n [0.78325993 0.2167401 ]\n [0.74356872 0.25643125]\n [0.76595283 0.23404719]\n [0.93704587 0.06295416]\n [0.93005335 0.06994666]]\nvalidation batch: 30, val_loss: 0.08883425919678961, valid_preds_fold: [[0.36310938 0.63689059]\n [0.87319493 0.12680505]\n [0.87231702 0.12768295]\n [0.76213193 0.23786807]\n [0.89206648 0.10793346]\n [0.89844257 0.10155749]\n [0.81461471 0.18538529]\n [0.91781414 0.08218584]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 31, val_loss: 0.0896611713471204, valid_preds_fold: [[0.96224517 0.03775484]\n [0.95946223 0.04053776]\n [0.89975071 0.10024926]\n [0.77022302 0.22977702]\n [0.91829264 0.0817073 ]\n [0.74570018 0.25429982]\n [0.02280798 0.97719198]\n [0.94367677 0.05632324]]\nvalidation batch: 32, val_loss: 0.09196360131902417, valid_preds_fold: [[0.92162704 0.07837296]\n [0.0027898  0.9972102 ]\n [0.06251445 0.93748558]\n [0.00212277 0.99787724]\n [0.01571223 0.98428774]\n [0.01031744 0.98968261]\n [0.09635468 0.90364528]\n [0.89405656 0.10594345]]\nvalidation batch: 33, val_loss: 0.09649354475040507, valid_preds_fold: [[0.00355709 0.99644285]\n [0.96671873 0.03328125]\n [0.92076254 0.07923749]\n [0.9667899  0.03321004]\n [0.89271772 0.10728222]\n [0.02501887 0.97498113]\n [0.0063041  0.99369591]\n [0.92169356 0.07830639]]\nvalidation batch: 34, val_loss: 0.10049365248775832, valid_preds_fold: [[0.81687129 0.18312873]\n [0.7535063  0.24649373]\n [0.00823407 0.99176592]\n [0.95666969 0.04333027]\n [0.97501457 0.02498543]\n [0.9357661  0.06423385]\n [0.95799023 0.04200973]\n [0.95361829 0.0463817 ]]\nvalidation batch: 35, val_loss: 0.10207223527840456, valid_preds_fold: [[0.90413344 0.09586658]\n [0.95195627 0.0480437 ]\n [0.91581511 0.08418486]\n [0.27524376 0.72475624]\n [0.94938338 0.05061667]\n [0.95930797 0.04069209]\n [0.93151188 0.06848819]\n [0.96305859 0.03694149]]\nvalidation batch: 36, val_loss: 0.10735997432557337, valid_preds_fold: [[0.87735605 0.12264399]\n [0.93872166 0.06127832]\n [0.14841452 0.85158545]\n [0.89830118 0.10169878]\n [0.91903001 0.08096999]\n [0.79158133 0.20841871]\n [0.65720087 0.34279913]\n [0.78027278 0.21972728]]\nvalidation batch: 37, val_loss: 0.1093881037222208, valid_preds_fold: [[0.00206529 0.9979347 ]\n [0.90684271 0.09315732]\n [0.95742232 0.04257764]\n [0.93697488 0.06302512]\n [0.91186213 0.08813787]\n [0.82776511 0.17223486]\n [0.96268535 0.03731471]\n [0.81640911 0.18359089]]\nvalidation batch: 38, val_loss: 0.11628136131232673, valid_preds_fold: [[0.87896949 0.12103054]\n [0.95147079 0.04852926]\n [0.95571309 0.04428689]\n [0.95497602 0.045024  ]\n [0.97075045 0.02924956]\n [0.8939262  0.10607383]\n [0.80510575 0.19489419]\n [0.03417408 0.96582597]]\nvalidation batch: 39, val_loss: 0.11985834263754588, valid_preds_fold: [[0.8234964  0.17650358]\n [0.75552732 0.24447264]\n [0.01017982 0.98982012]\n [0.30384684 0.69615316]\n [0.07021224 0.9297877 ]\n [0.15862699 0.84137303]\n [0.0668591  0.93314087]\n [0.83923274 0.16076721]]\nvalidation batch: 40, val_loss: 0.12433215067551956, valid_preds_fold: [[0.05006443 0.94993556]\n [0.77579296 0.22420703]\n [0.00124283 0.99875712]\n [0.00299644 0.9970035 ]\n [0.00263886 0.99736112]\n [0.96102297 0.03897705]\n [0.9617359  0.0382641 ]\n [0.95422107 0.04577896]]\nvalidation batch: 41, val_loss: 0.12839438124512237, valid_preds_fold: [[0.00346386 0.99653614]\n [0.96791178 0.03208822]\n [0.92455024 0.0754498 ]\n [0.90092653 0.09907352]\n [0.84263885 0.15736115]\n [0.97146815 0.02853188]\n [0.83128464 0.16871531]\n [0.95497161 0.04502843]]\nvalidation batch: 42, val_loss: 0.12905056034996565, valid_preds_fold: [[0.9541747  0.04582529]\n [0.96915454 0.03084547]\n [0.92019397 0.07980599]\n [0.78964925 0.21035077]\n [0.93058062 0.06941944]\n [0.91974562 0.08025438]\n [0.96217728 0.03782271]\n [0.88035142 0.11964863]]\nvalidation batch: 43, val_loss: 0.13265598266229145, valid_preds_fold: [[0.87185627 0.12814368]\n [0.10458487 0.89541513]\n [0.04294422 0.95705575]\n [0.75936955 0.24063046]\n [0.92152208 0.07847798]\n [0.91461509 0.0853849 ]\n [0.89883512 0.10116486]\n [0.00318352 0.99681646]]\nvalidation batch: 44, val_loss: 0.1338717346861415, valid_preds_fold: [[0.92358005 0.07641998]\n [0.00865612 0.99134386]\n [0.62202197 0.37797806]\n [0.92673367 0.07326633]\n [0.02599144 0.97400856]\n [0.90819728 0.09180267]\n [0.93745357 0.06254648]\n [0.00801505 0.99198496]]\nvalidation batch: 45, val_loss: 0.13478800099696558, valid_preds_fold: [[0.00742483 0.99257517]\n [0.00540366 0.99459642]\n [0.00355069 0.99644935]\n [0.00805724 0.99194276]\n [0.47000301 0.52999699]\n [0.79148394 0.20851606]\n [0.93143821 0.06856178]\n [0.9608323  0.03916766]]\nvalidation batch: 46, val_loss: 0.1392784178474524, valid_preds_fold: [[0.89624918 0.10375084]\n [0.89883929 0.10116075]\n [0.90425277 0.09574729]\n [0.81545925 0.18454073]\n [0.96933156 0.03066845]\n [0.91610336 0.08389671]\n [0.86138183 0.13861819]\n [0.86143839 0.13856158]]\nvalidation batch: 47, val_loss: 0.1400738435178778, valid_preds_fold: [[0.85752743 0.14247251]\n [0.96793181 0.03206816]\n [0.8445456  0.15545441]\n [0.83095115 0.1690488 ]\n [0.89497149 0.10502856]\n [0.9560746  0.0439254 ]\n [0.84010792 0.1598921 ]\n [0.00124129 0.99875867]]\nvalidation batch: 48, val_loss: 0.14546365312633727, valid_preds_fold: [[8.34766269e-01 1.65233672e-01]\n [9.17515039e-01 8.24849159e-02]\n [9.54962859e-04 9.99045074e-01]\n [3.81354630e-01 6.18645370e-01]\n [1.07616469e-01 8.92383575e-01]\n [1.96682572e-01 8.03317428e-01]\n [9.79747176e-01 2.02528462e-02]\n [9.09008026e-01 9.09919366e-02]]\nvalidation batch: 49, val_loss: 0.14955701950910322, valid_preds_fold: [[0.03710989 0.96289009]\n [0.95201904 0.047981  ]\n [0.97712666 0.02287339]\n [0.94263077 0.05736919]\n [0.8921513  0.10784867]\n [0.87940472 0.12059529]\n [0.90366584 0.09633417]\n [0.82352251 0.17647752]]\nvalidation batch: 50, val_loss: 0.15549616292662868, valid_preds_fold: [[0.94923693 0.0507631 ]\n [0.06000623 0.9399938 ]\n [0.90883768 0.09116234]\n [0.60633409 0.39366591]\n [0.005696   0.994304  ]\n [0.00597432 0.99402571]\n [0.01316531 0.9868347 ]\n [0.10819682 0.89180315]]\nvalidation batch: 51, val_loss: 0.15872098456551564, valid_preds_fold: [[0.001898   0.99810201]\n [0.9638235  0.03617652]\n [0.96450114 0.03549884]\n [0.07026107 0.929739  ]\n [0.93430346 0.06569658]\n [0.71963292 0.28036705]\n [0.32745492 0.67254508]\n [0.01026868 0.98973125]]\nvalidation batch: 52, val_loss: 0.16107923972563157, valid_preds_fold: [[0.86732769 0.13267231]\n [0.00754116 0.99245888]\n [0.55177045 0.44822955]\n [0.96329403 0.03670599]\n [0.95621669 0.04378325]\n [0.95925003 0.04074999]\n [0.96071887 0.03928117]\n [0.81292987 0.1870701 ]]\nvalidation batch: 53, val_loss: 0.1622742034331726, valid_preds_fold: [[0.00174417 0.99825579]\n [0.63263899 0.36736107]\n [0.94747692 0.05252306]\n [0.00175235 0.99824762]\n [0.58040518 0.41959476]\n [0.88418293 0.11581708]\n [0.11573657 0.88426346]\n [0.00424311 0.99575692]]\nvalidation batch: 54, val_loss: 0.16324434951491604, valid_preds_fold: [[0.0050206  0.99497944]\n [0.10689007 0.89310992]\n [0.92810231 0.07189775]\n [0.45190799 0.54809195]\n [0.00501424 0.99498582]\n [0.96666795 0.03333205]\n [0.0335084  0.96649164]\n [0.0032958  0.99670416]]\nvalidation batch: 55, val_loss: 0.16535024317729216, valid_preds_fold: [[0.00961028 0.99038976]\n [0.86941898 0.13058099]\n [0.38435203 0.61564797]\n [0.68120128 0.31879875]\n [0.96564275 0.03435725]\n [0.94067967 0.05932035]\n [0.81574589 0.18425408]\n [0.79419094 0.20580906]]\nvalidation batch: 56, val_loss: 0.17062770368626523, valid_preds_fold: [[0.07423107 0.92576891]\n [0.67649859 0.32350138]\n [0.00724776 0.99275219]\n [0.70398951 0.29601049]\n [0.07649285 0.92350715]\n [0.49028879 0.50971121]\n [0.78236723 0.21763279]\n [0.88945192 0.11054807]]\nvalidation batch: 57, val_loss: 0.1756588708419, valid_preds_fold: [[0.0533317  0.94666833]\n [0.64164925 0.35835075]\n [0.38086623 0.61913377]\n [0.86767685 0.13232315]\n [0.1774969  0.82250309]\n [0.7797429  0.22025716]\n [0.03924149 0.96075845]\n [0.01014643 0.98985356]]\nvalidation batch: 58, val_loss: 0.17688305540023935, valid_preds_fold: [[0.89481169 0.10518831]\n [0.96903169 0.03096835]\n [0.97607654 0.02392346]\n [0.01654435 0.9834556 ]\n [0.06974371 0.93025625]\n [0.58594692 0.41405305]\n [0.11786123 0.88213879]\n [0.07572607 0.92427385]]\nvalidation batch: 59, val_loss: 0.17714301496744161, valid_preds_fold: [[0.00160241 0.99839753]\n [0.10865138 0.8913486 ]\n [0.08817884 0.91182119]\n [0.00261449 0.99738556]\n [0.06258587 0.93741417]\n [0.00172495 0.99827504]\n [0.00194797 0.998052  ]\n [0.00504374 0.99495631]]\nvalidation batch: 60, val_loss: 0.17925930768251425, valid_preds_fold: [[0.68970728 0.31029275]\n [0.00474112 0.99525887]\n [0.60164464 0.39835536]\n [0.00219399 0.99780601]\n [0.96560949 0.03439054]\n [0.92718887 0.07281105]\n [0.92068684 0.07931316]\n [0.5628069  0.4371931 ]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 61, val_loss: 0.1800879923640377, valid_preds_fold: [[0.95849025 0.04150969]\n [0.88521397 0.11478602]\n [0.94127375 0.05872623]\n [0.86273986 0.13726011]\n [0.82568169 0.1743183 ]\n [0.95494974 0.04505024]\n [0.95443344 0.04556659]\n [0.77765399 0.22234599]]\nvalidation batch: 62, val_loss: 0.18387088467822463, valid_preds_fold: [[0.78789097 0.21210904]\n [0.93541908 0.06458098]\n [0.87781352 0.12218649]\n [0.26539704 0.73460293]\n [0.26658675 0.73341322]\n [0.90118796 0.09881203]\n [0.48402557 0.5159744 ]\n [0.0506213  0.94937873]]\nvalidation batch: 63, val_loss: 0.19019793425380754, valid_preds_fold: [[0.91379005 0.08620997]\n [0.05367344 0.94632661]\n [0.80262703 0.197373  ]\n [0.09664177 0.90335822]\n [0.0046013  0.9953987 ]\n [0.00253573 0.9974643 ]\n [0.43469879 0.56530118]\n [0.74202859 0.25797147]]\nvalidation batch: 64, val_loss: 0.19320636775589342, valid_preds_fold: [[0.00273397 0.99726605]\n [0.09065336 0.90934664]\n [0.01885887 0.98114109]\n [0.05044807 0.949552  ]\n [0.00352399 0.99647599]\n [0.0097192  0.99028075]\n [0.8729353  0.12706468]\n [0.95653671 0.04346325]]\nvalidation batch: 65, val_loss: 0.19365267906963396, valid_preds_fold: [[0.84191918 0.15808088]\n [0.92206794 0.07793203]\n [0.01982948 0.98017049]\n [0.00338964 0.99661034]\n [0.00314043 0.99685955]\n [0.01246537 0.98753464]\n [0.95974755 0.04025243]\n [0.85577196 0.14422806]]\nvalidation batch: 66, val_loss: 0.19613025685513982, valid_preds_fold: [[0.83040839 0.16959156]\n [0.9659335  0.03406644]\n [0.81018591 0.18981414]\n [0.17846467 0.82153529]\n [0.00542624 0.99457377]\n [0.74115604 0.25884393]\n [0.9407112  0.05928879]\n [0.82286179 0.17713822]]\nvalidation batch: 67, val_loss: 0.19701011413640354, valid_preds_fold: [[0.95029098 0.04970907]\n [0.96190321 0.03809678]\n [0.8898204  0.11017961]\n [0.84160459 0.15839536]\n [0.94746065 0.05253936]\n [0.89645863 0.10354141]\n [0.66060323 0.3393968 ]\n [0.007412   0.99258798]]\nvalidation batch: 68, val_loss: 0.20289685201905944, valid_preds_fold: [[0.39648351 0.60351652]\n [0.66928113 0.33071887]\n [0.91458517 0.0854148 ]\n [0.74841779 0.25158221]\n [0.00408062 0.99591941]\n [0.36795843 0.63204163]\n [0.02223683 0.97776324]\n [0.37949398 0.62050599]]\nvalidation batch: 69, val_loss: 0.20721513674642053, valid_preds_fold: [[0.0119129  0.98808718]\n [0.76276815 0.23723185]\n [0.96947193 0.03052802]\n [0.62350959 0.37649038]\n [0.9444527  0.05554726]\n [0.92750883 0.07249118]\n [0.93234468 0.06765534]\n [0.53985393 0.4601461 ]]\nvalidation batch: 70, val_loss: 0.21167864144718565, valid_preds_fold: [[0.08501307 0.91498697]\n [0.92812055 0.07187938]\n [0.88842094 0.11157911]\n [0.94450128 0.05549877]\n [0.86342007 0.13657989]\n [0.95283651 0.04716348]\n [0.82128942 0.17871056]\n [0.92512327 0.07487671]]\nvalidation batch: 71, val_loss: 0.21538267594619395, valid_preds_fold: [[0.92874336 0.07125667]\n [0.93954057 0.06045944]\n [0.41059089 0.58940911]\n [0.91190523 0.08809474]\n [0.00553074 0.99446929]\n [0.83180493 0.16819504]\n [0.90832359 0.09167637]\n [0.07029185 0.92970818]]\nvalidation batch: 72, val_loss: 0.21571374999998266, valid_preds_fold: [[0.01027019 0.98972976]\n [0.94628739 0.05371263]\n [0.00463448 0.9953655 ]\n [0.002607   0.99739301]\n [0.20005709 0.79994291]\n [0.96396613 0.03603384]\n [0.97235167 0.02764838]\n [0.00211512 0.99788481]]\nvalidation batch: 73, val_loss: 0.21747004208120993, valid_preds_fold: [[0.00104529 0.99895465]\n [0.60838896 0.39161101]\n [0.00653668 0.9934634 ]\n [0.00453981 0.99546021]\n [0.01647209 0.9835279 ]\n [0.39868873 0.60131127]\n [0.0213444  0.97865558]\n [0.01734411 0.98265588]]\nvalidation batch: 74, val_loss: 0.2186331915920669, valid_preds_fold: [[0.40414721 0.59585279]\n [0.01543042 0.98456961]\n [0.10958827 0.89041173]\n [0.89872766 0.10127236]\n [0.97307974 0.02692025]\n [0.9396199  0.06038012]\n [0.03744638 0.96255368]\n [0.00270048 0.99729949]]\nvalidation batch: 75, val_loss: 0.21909662412248396, valid_preds_fold: [[0.00163685 0.9983632 ]\n [0.06510264 0.93489736]\n [0.87823385 0.12176609]\n [0.90709078 0.09290916]\n [0.93642694 0.06357305]\n [0.94156969 0.05843032]\n [0.94290519 0.05709482]\n [0.97342932 0.02657067]]\nvalidation batch: 76, val_loss: 0.22162400746215005, valid_preds_fold: [[0.91462457 0.0853755 ]\n [0.6618492  0.33815083]\n [0.91154575 0.08845426]\n [0.80170852 0.19829154]\n [0.90576524 0.09423476]\n [0.77660733 0.22339264]\n [0.13485917 0.86514086]\n [0.05895436 0.94104564]]\nvalidation batch: 77, val_loss: 0.22664380318274469, valid_preds_fold: [[0.93463814 0.0653619 ]\n [0.25683856 0.74316144]\n [0.76154113 0.23845886]\n [0.92306602 0.07693402]\n [0.89986247 0.10013747]\n [0.68426341 0.31573665]\n [0.50594991 0.49405012]\n [0.02746758 0.97253245]]\nvalidation batch: 78, val_loss: 0.22962973397361106, valid_preds_fold: [[0.96086764 0.03913233]\n [0.240941   0.75905901]\n [0.24239139 0.75760865]\n [0.86501968 0.13498037]\n [0.8560518  0.14394818]\n [0.20314983 0.7968502 ]\n [0.02388489 0.97611505]\n [0.76331532 0.23668471]]\nvalidation batch: 79, val_loss: 0.23260980197330466, valid_preds_fold: [[0.75149417 0.24850582]\n [0.63518608 0.36481386]\n [0.87228918 0.12771076]\n [0.01336895 0.98663098]\n [0.09401599 0.90598398]\n [0.00493357 0.9950664 ]\n [0.71749198 0.28250799]\n [0.01931726 0.98068273]]\nvalidation batch: 80, val_loss: 0.23481641530338002, valid_preds_fold: [[0.14675888 0.85324115]\n [0.89355439 0.10644558]\n [0.05767584 0.9423241 ]\n [0.14303483 0.85696524]\n [0.96917611 0.03082391]\n [0.94933558 0.05066445]\n [0.01946673 0.98053324]\n [0.03935762 0.9606424 ]]\nvalidation batch: 81, val_loss: 0.23848352798797795, valid_preds_fold: [[3.25790164e-03 9.96742070e-01]\n [6.13096297e-01 3.86903733e-01]\n [7.85583615e-01 2.14416340e-01]\n [9.12903011e-01 8.70969743e-02]\n [9.39867692e-04 9.99060094e-01]\n [5.89461699e-02 9.41053808e-01]\n [9.69913672e-04 9.99030113e-01]\n [7.24998355e-01 2.75001675e-01]]\nvalidation batch: 82, val_loss: 0.2391297921647121, valid_preds_fold: [[0.00461919 0.99538088]\n [0.92926991 0.07073008]\n [0.93259478 0.06740518]\n [0.957753   0.04224704]\n [0.88460112 0.11539885]\n [0.8652423  0.13475771]\n [0.08541273 0.91458726]\n [0.85152763 0.14847238]]\nvalidation batch: 83, val_loss: 0.24036353347945394, valid_preds_fold: [[0.80064458 0.19935539]\n [0.57696068 0.42303929]\n [0.85676897 0.14323099]\n [0.93180251 0.06819747]\n [0.95161432 0.04838561]\n [0.95554173 0.0444583 ]\n [0.8778972  0.12210285]\n [0.87867349 0.12132645]]\nvalidation batch: 84, val_loss: 0.2469677907707048, valid_preds_fold: [[0.95174724 0.04825271]\n [0.42677096 0.57322901]\n [0.88773996 0.11226011]\n [0.79654342 0.20345663]\n [0.77721477 0.2227852 ]\n [0.00123384 0.99876618]\n [0.92827034 0.07172973]\n [0.92284137 0.07715862]]\nvalidation batch: 85, val_loss: 0.24810055888047192, valid_preds_fold: [[0.92211884 0.07788114]\n [0.00166009 0.99833989]\n [0.26949814 0.73050189]\n [0.83879715 0.16120285]\n [0.58217549 0.41782451]\n [0.96441966 0.03558034]\n [0.03752862 0.96247137]\n [0.05209241 0.94790763]]\nvalidation batch: 86, val_loss: 0.2521155477657806, valid_preds_fold: [[0.91520751 0.08479246]\n [0.00952599 0.99047405]\n [0.97680658 0.02319336]\n [0.02449715 0.97550279]\n [0.12084229 0.87915772]\n [0.94954377 0.05045616]\n [0.86599779 0.13400216]\n [0.92400146 0.07599852]]\nvalidation batch: 87, val_loss: 0.2572209888348615, valid_preds_fold: [[0.96715266 0.03284733]\n [0.96362257 0.03637742]\n [0.21246752 0.78753251]\n [0.04690704 0.95309299]\n [0.1880777  0.81192225]\n [0.16666411 0.83333588]\n [0.20120464 0.7987954 ]\n [0.00189317 0.99810684]]\nvalidation batch: 88, val_loss: 0.2593574455402194, valid_preds_fold: [[0.60115981 0.39884022]\n [0.77711999 0.22287995]\n [0.85893679 0.14106317]\n [0.0236189  0.97638106]\n [0.00860211 0.99139786]\n [0.00203445 0.99796557]\n [0.94310546 0.05689457]\n [0.91729641 0.08270355]]\nvalidation batch: 89, val_loss: 0.2613410685401764, valid_preds_fold: [[0.9291712  0.07082874]\n [0.96244133 0.0375587 ]\n [0.95211464 0.04788533]\n [0.94060701 0.05939297]\n [0.82503682 0.17496316]\n [0.94650704 0.053493  ]\n [0.95664626 0.04335373]\n [0.1037436  0.89625645]]\nvalidation batch: 90, val_loss: 0.26477468807766913, valid_preds_fold: [[0.91945183 0.08054812]\n [0.94147331 0.05852674]\n [0.70711809 0.29288185]\n [0.95745379 0.04254624]\n [0.00754769 0.99245232]\n [0.00924414 0.99075586]\n [0.93126416 0.06873587]\n [0.58591694 0.41408303]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 91, val_loss: 0.2668224506787141, valid_preds_fold: [[0.24819712 0.75180286]\n [0.1401276  0.8598724 ]\n [0.07596529 0.92403471]\n [0.00708326 0.9929167 ]\n [0.8921923  0.10780766]\n [0.66979015 0.33020985]\n [0.93802744 0.06197256]\n [0.96571153 0.03428844]]\nvalidation batch: 92, val_loss: 0.2687559637927662, valid_preds_fold: [[0.93186337 0.06813663]\n [0.88736469 0.11263534]\n [0.8381055  0.16189447]\n [0.93793935 0.06206058]\n [0.93915862 0.06084134]\n [0.9523741  0.04762597]\n [0.67882806 0.32117197]\n [0.64337081 0.35662916]]\nvalidation batch: 93, val_loss: 0.271443840252222, valid_preds_fold: [[0.92596859 0.07403142]\n [0.93162775 0.06837226]\n [0.89889377 0.10110621]\n [0.0072819  0.9927181 ]\n [0.05057538 0.94942462]\n [0.00756779 0.99243224]\n [0.00785251 0.99214751]\n [0.00491926 0.99508071]]\nvalidation batch: 94, val_loss: 0.27423639443233944, valid_preds_fold: [[0.03777172 0.9622283 ]\n [0.00632451 0.99367553]\n [0.002278   0.99772197]\n [0.00160773 0.99839228]\n [0.91179055 0.08820949]\n [0.93387967 0.06612025]\n [0.84085852 0.15914147]\n [0.97049898 0.02950103]]\nvalidation batch: 95, val_loss: 0.2760520421893058, valid_preds_fold: [[0.44233412 0.55766588]\n [0.87043017 0.12956978]\n [0.95209962 0.04790035]\n [0.84655184 0.15344818]\n [0.91398597 0.086014  ]\n [0.84418386 0.15581615]\n [0.96791762 0.0320824 ]\n [0.53213906 0.46786091]]\nvalidation batch: 96, val_loss: 0.2770597988019025, valid_preds_fold: [[0.9020474  0.09795256]\n [0.03648718 0.9635129 ]\n [0.17163251 0.82836747]\n [0.36879322 0.63120675]\n [0.93488187 0.06511815]\n [0.91695774 0.0830423 ]\n [0.94573158 0.05426841]\n [0.8994264  0.10057367]]\nvalidation batch: 97, val_loss: 0.28280281726896334, valid_preds_fold: [[0.91975814 0.08024184]\n [0.92465228 0.07534774]\n [0.76973993 0.23026007]\n [0.75872707 0.24127294]\n [0.00818563 0.99181432]\n [0.96203536 0.03796465]\n [0.00787376 0.99212623]\n [0.04501964 0.95498037]]\nvalidation batch: 98, val_loss: 0.2865565968473463, valid_preds_fold: [[0.16249873 0.83750129]\n [0.42587772 0.57412231]\n [0.45437694 0.54562306]\n [0.00112591 0.99887413]\n [0.97325701 0.02674301]\n [0.25584555 0.74415451]\n [0.30434555 0.69565445]\n [0.01199082 0.98800915]]\nvalidation batch: 99, val_loss: 0.28995882388014, valid_preds_fold: [[0.96057683 0.03942311]\n [0.00205972 0.99794024]\n [0.00354062 0.99645942]\n [0.00248565 0.99751437]\n [0.69723141 0.30276862]\n [0.01556419 0.9844358 ]\n [0.00214396 0.99785608]\n [0.10315248 0.89684755]]\nvalidation batch: 100, val_loss: 0.29183887119275814, valid_preds_fold: [[0.65600234 0.34399769]\n [0.18137284 0.81862712]\n [0.36328825 0.63671178]\n [0.02705561 0.97294444]\n [0.81160545 0.18839453]\n [0.03656927 0.96343076]\n [0.889539   0.110461  ]\n [0.96485281 0.03514722]]\nvalidation batch: 101, val_loss: 0.29205559672665427, valid_preds_fold: [[0.02565475 0.97434527]\n [0.00262666 0.99737334]\n [0.97507256 0.0249274 ]\n [0.91999376 0.08000626]\n [0.01033007 0.98966998]\n [0.00665382 0.99334621]\n [0.00567815 0.99432182]\n [0.925403   0.07459698]]\nvalidation batch: 102, val_loss: 0.29314137560172676, valid_preds_fold: [[0.0078506  0.99214935]\n [0.00823822 0.9917618 ]\n [0.50521541 0.49478459]\n [0.00266265 0.99733728]\n [0.00929674 0.99070334]\n [0.95970964 0.0402904 ]\n [0.93490344 0.0650966 ]\n [0.70484024 0.29515982]]\nvalidation batch: 103, val_loss: 0.2972139691131829, valid_preds_fold: [[0.94577199 0.05422805]\n [0.14629664 0.85370338]\n [0.48792359 0.51207644]\n [0.58733869 0.41266134]\n [0.90046936 0.09953062]\n [0.66394037 0.3360596 ]\n [0.07524873 0.92475122]\n [0.22362471 0.77637529]]\nvalidation batch: 104, val_loss: 0.3012662448369674, valid_preds_fold: [[0.3071447  0.6928553 ]\n [0.5803045  0.4196955 ]\n [0.18669634 0.81330371]\n [0.90300262 0.09699738]\n [0.00681041 0.99318957]\n [0.05973531 0.9402647 ]\n [0.9610672  0.03893281]\n [0.96746415 0.03253584]]\nvalidation batch: 105, val_loss: 0.303208227684028, valid_preds_fold: [[0.92355365 0.07644643]\n [0.95405328 0.04594669]\n [0.95215541 0.04784462]\n [0.00568459 0.99431545]\n [0.00533384 0.99466622]\n [0.00417368 0.9958263 ]\n [0.94148833 0.05851166]\n [0.84699732 0.15300272]]\nvalidation batch: 106, val_loss: 0.3079494948152208, valid_preds_fold: [[0.91998154 0.08001842]\n [0.84813654 0.15186349]\n [0.95540494 0.04459514]\n [0.92576116 0.07423888]\n [0.92289615 0.07710392]\n [0.68941426 0.31058577]\n [0.72076297 0.279237  ]\n [0.79888147 0.20111848]]\nvalidation batch: 107, val_loss: 0.31121332769411325, valid_preds_fold: [[0.73005885 0.26994115]\n [0.87811732 0.1218827 ]\n [0.00728228 0.99271774]\n [0.64832556 0.35167447]\n [0.12682414 0.87317592]\n [0.00307861 0.99692142]\n [0.62235606 0.37764388]\n [0.09897146 0.90102851]]\nvalidation batch: 108, val_loss: 0.32281487929995045, valid_preds_fold: [[0.96533138 0.03466859]\n [0.84792668 0.15207335]\n [0.98469019 0.01530983]\n [0.93679315 0.06320686]\n [0.91919321 0.08080675]\n [0.87757236 0.12242768]\n [0.84792668 0.15207335]\n [0.91660655 0.08339341]]\nvalidation batch: 109, val_loss: 0.32616887595096644, valid_preds_fold: [[0.85924947 0.14075045]\n [0.76688117 0.23311885]\n [0.07438277 0.92561722]\n [0.79316515 0.20683485]\n [0.88466448 0.11533557]\n [0.92635113 0.07364887]\n [0.86506873 0.1349313 ]\n [0.9287619  0.07123812]]\nvalidation batch: 110, val_loss: 0.32729466411754166, valid_preds_fold: [[0.44359463 0.55640537]\n [0.94481742 0.05518255]\n [0.89830118 0.10169878]\n [0.91071254 0.0892874 ]\n [0.96476126 0.03523871]\n [0.96996486 0.03003518]\n [0.94639063 0.0536093 ]\n [0.95886165 0.04113829]]\nvalidation batch: 111, val_loss: 0.328564500395399, valid_preds_fold: [[0.83169997 0.16830002]\n [0.94982666 0.05017333]\n [0.46108693 0.53891307]\n [0.68857098 0.31142899]\n [0.94810402 0.05189605]\n [0.95696908 0.04303096]\n [0.93689394 0.06310612]\n [0.00218054 0.99781942]]\nvalidation batch: 112, val_loss: 0.3292943718877152, valid_preds_fold: [[0.01424512 0.98575485]\n [0.06120211 0.93879789]\n [0.91593307 0.0840669 ]\n [0.90408826 0.09591182]\n [0.87082154 0.12917848]\n [0.93438464 0.06561539]\n [0.00246536 0.99753463]\n [0.72242433 0.27757567]]\nvalidation batch: 113, val_loss: 0.3306368699256521, valid_preds_fold: [[0.01035724 0.9896428 ]\n [0.75367975 0.24632026]\n [0.04337124 0.9566288 ]\n [0.00528178 0.99471825]\n [0.00268211 0.99731785]\n [0.00281622 0.99718374]\n [0.00218243 0.99781752]\n [0.00251414 0.99748588]]\nvalidation batch: 114, val_loss: 0.33068343572808007, valid_preds_fold: [[0.00291965 0.99708027]\n [0.00434163 0.9956584 ]\n [0.00310547 0.99689448]\n [0.00245867 0.99754131]\n [0.00713881 0.99286121]\n [0.00337727 0.99662268]\n [0.02208523 0.97791481]\n [0.0052948  0.99470526]]\nvalidation batch: 115, val_loss: 0.33438730751075885, valid_preds_fold: [[0.0030141  0.99698585]\n [0.59410936 0.40589064]\n [0.82006121 0.17993873]\n [0.92460722 0.07539275]\n [0.95099217 0.04900776]\n [0.87854844 0.12145155]\n [0.90571105 0.09428892]\n [0.94921738 0.05078258]]\nvalidation batch: 116, val_loss: 0.336338135556583, valid_preds_fold: [[0.882927   0.11707299]\n [0.9180001  0.08199988]\n [0.75361347 0.24638654]\n [0.49226761 0.50773245]\n [0.71709365 0.28290635]\n [0.00332768 0.99667227]\n [0.55513257 0.44486746]\n [0.01192636 0.98807365]]\nvalidation batch: 117, val_loss: 0.34028173635040754, valid_preds_fold: [[0.02823653 0.97176343]\n [0.37315792 0.62684214]\n [0.01847997 0.98152   ]\n [0.41195837 0.58804166]\n [0.05752858 0.94247139]\n [0.1367617  0.86323828]\n [0.79212248 0.20787755]\n [0.22318545 0.77681458]]\nvalidation batch: 118, val_loss: 0.3440938614798288, valid_preds_fold: [[0.00601706 0.99398297]\n [0.91860026 0.08139977]\n [0.02989318 0.97010684]\n [0.06918944 0.93081057]\n [0.06608227 0.93391776]\n [0.00385574 0.99614429]\n [0.97067696 0.029323  ]\n [0.68148005 0.31851992]]\nvalidation batch: 119, val_loss: 0.34628884429044093, valid_preds_fold: [[0.90371978 0.09628022]\n [0.00942966 0.99057031]\n [0.78859532 0.21140468]\n [0.50282621 0.49717385]\n [0.26053753 0.73946249]\n [0.00466793 0.99533206]\n [0.00203637 0.99796367]\n [0.00692305 0.99307692]]\nvalidation batch: 120, val_loss: 0.3496306017585044, valid_preds_fold: [[0.08931839 0.91068161]\n [0.88425064 0.1157494 ]\n [0.94588357 0.05411646]\n [0.01871098 0.98128903]\n [0.60648268 0.39351729]\n [0.00910706 0.99089289]\n [0.0023351  0.99766493]\n [0.58396548 0.41603449]]\nvalidation batch: 121, val_loss: 0.354205439234302, valid_preds_fold: [[0.70502436 0.2949757 ]\n [0.72402954 0.2759704 ]\n [0.94956672 0.05043335]\n [0.75765908 0.24234091]\n [0.82703292 0.17296709]\n [0.90885371 0.09114625]\n [0.90624821 0.09375175]\n [0.3264409  0.67355913]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 122, val_loss: 0.3556869134415675, valid_preds_fold: [[0.84109628 0.1589037 ]\n [0.57366717 0.42633277]\n [0.89036912 0.10963091]\n [0.9564665  0.04353348]\n [0.67908037 0.32091957]\n [0.80132467 0.1986753 ]\n [0.96583778 0.03416228]\n [0.91297424 0.08702578]]\nvalidation batch: 123, val_loss: 0.3561529255043851, valid_preds_fold: [[0.95883745 0.04116254]\n [0.92741501 0.07258498]\n [0.95684624 0.0431537 ]\n [0.92589515 0.0741049 ]\n [0.88950086 0.11049908]\n [0.94464308 0.05535696]\n [0.96358758 0.03641245]\n [0.9407106  0.05928943]]\nvalidation batch: 124, val_loss: 0.3567405914629463, valid_preds_fold: [[0.15515783 0.8448422 ]\n [0.01019637 0.98980367]\n [0.20620498 0.79379499]\n [0.97427213 0.0257278 ]\n [0.94562423 0.0543758 ]\n [0.97479045 0.02520953]\n [0.93050379 0.06949625]\n [0.94671106 0.05328892]]\nvalidation batch: 125, val_loss: 0.3589373197760025, valid_preds_fold: [[0.00161593 0.99838412]\n [0.21485105 0.78514892]\n [0.00158444 0.99841559]\n [0.0202433  0.97975665]\n [0.91648197 0.08351802]\n [0.78688425 0.21311574]\n [0.86396724 0.13603278]\n [0.18844086 0.81155914]]\nvalidation batch: 126, val_loss: 0.3634367860908056, valid_preds_fold: [[0.3968637  0.6031363 ]\n [0.12842312 0.87157685]\n [0.00313516 0.99686486]\n [0.04190181 0.95809823]\n [0.9613058  0.03869426]\n [0.89329898 0.10670102]\n [0.77258247 0.22741753]\n [0.72435802 0.27564192]]\nvalidation batch: 127, val_loss: 0.3640823164669267, valid_preds_fold: [[0.96655768 0.03344233]\n [0.93526363 0.06473631]\n [0.77694249 0.22305748]\n [0.94612497 0.05387504]\n [0.94174188 0.05825817]\n [0.88002408 0.11997594]\n [0.93113858 0.06886137]\n [0.96116084 0.03883918]]\nvalidation batch: 128, val_loss: 0.3724271468546269, valid_preds_fold: [[0.36903039 0.63096964]\n [0.91348225 0.08651777]\n [0.02057998 0.97942001]\n [0.96471155 0.03528845]\n [0.93008339 0.06991667]\n [0.89743996 0.10256006]\n [0.94299543 0.05700454]\n [0.01541924 0.9845807 ]]\nvalidation batch: 129, val_loss: 0.37579434985009424, valid_preds_fold: [[0.7337302  0.26626986]\n [0.85021347 0.14978659]\n [0.76116079 0.23883918]\n [0.9147625  0.08523755]\n [0.91254574 0.08745424]\n [0.00780126 0.99219877]\n [0.0029718  0.99702817]\n [0.00433024 0.99566972]]\nvalidation batch: 130, val_loss: 0.3796544615043341, valid_preds_fold: [[0.02488604 0.97511399]\n [0.0032533  0.99674666]\n [0.33601773 0.66398221]\n [0.0093247  0.99067527]\n [0.00285042 0.99714965]\n [0.62640053 0.37359944]\n [0.87153745 0.12846257]\n [0.86156315 0.13843687]]\nvalidation batch: 131, val_loss: 0.3840537881002809, valid_preds_fold: [[0.78996128 0.21003872]\n [0.89841175 0.10158823]\n [0.84601957 0.15398045]\n [0.89392495 0.10607504]\n [0.28821346 0.71178657]\n [0.76844019 0.23155984]\n [0.15556061 0.84443933]\n [0.96888304 0.03111689]]\nvalidation batch: 132, val_loss: 0.38505471497774124, valid_preds_fold: [[0.00733331 0.99266666]\n [0.00505834 0.99494165]\n [0.70650524 0.29349476]\n [0.81702501 0.18297504]\n [0.0058424  0.99415755]\n [0.00288957 0.99711037]\n [0.00578867 0.99421126]\n [0.40578565 0.59421432]]\nvalidation batch: 133, val_loss: 0.38918254633236976, valid_preds_fold: [[0.05618732 0.94381267]\n [0.92479986 0.07520016]\n [0.00490589 0.99509418]\n [0.89536542 0.10463458]\n [0.47479019 0.52520978]\n [0.96681666 0.03318338]\n [0.12986863 0.87013143]\n [0.58644283 0.41355723]]\nvalidation batch: 134, val_loss: 0.38965606580685525, valid_preds_fold: [[0.81133127 0.18866876]\n [0.92117941 0.07882055]\n [0.88054436 0.11945567]\n [0.95397288 0.04602708]\n [0.03499493 0.9650051 ]\n [0.00245392 0.99754602]\n [0.00675971 0.99324036]\n [0.00856474 0.99143523]]\nvalidation batch: 135, val_loss: 0.39005019673465813, valid_preds_fold: [[0.80116397 0.198836  ]\n [0.95422137 0.04577863]\n [0.91578352 0.08421649]\n [0.96514034 0.03485965]\n [0.00227001 0.99772996]\n [0.00623663 0.99376339]\n [0.02219678 0.97780323]\n [0.00894557 0.99105448]]\nvalidation batch: 136, val_loss: 0.3908786012308441, valid_preds_fold: [[0.1072881  0.89271188]]\n\n","name":"stdout"},{"output_type":"stream","text":"[2020-02-01 13:59:05,533][INFO] ## epoch: 1, train loss: 0.30751599, valid loss: 0.39087860, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n[2020-02-01 13:59:05,533][INFO] ## epoch: 1, train loss: 0.30751599, valid loss: 0.39087860, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n[2020-02-01 13:59:05,533][INFO] ## epoch: 1, train loss: 0.30751599, valid loss: 0.39087860, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n[2020-02-01 13:59:05,533][INFO] ## epoch: 1, train loss: 0.30751599, valid loss: 0.39087860, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n[2020-02-01 13:59:05,533][INFO] ## epoch: 1, train loss: 0.30751599, valid loss: 0.39087860, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1280a487da54f81964681df10a28aba"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a7710f460d84f0d8d2efcf959e5e71a"}},"metadata":{}},{"output_type":"stream","text":"validation batch: 0, val_loss: 0.002838699269468767, valid_preds_fold: [[0.9870525  0.01294747]\n [0.98577809 0.01422185]\n [0.93210018 0.06789977]\n [0.9875201  0.01247985]\n [0.81192565 0.18807434]\n [0.01520054 0.98479939]\n [0.45500582 0.54499418]\n [0.59019345 0.40980658]]\nvalidation batch: 1, val_loss: 0.0033332295987727866, valid_preds_fold: [[3.40294966e-04 9.99659657e-01]\n [9.63796258e-01 3.62037048e-02]\n [2.63867706e-01 7.36132264e-01]\n [8.90540838e-01 1.09459132e-01]\n [1.93346944e-03 9.98066485e-01]\n [6.71243072e-02 9.32875752e-01]\n [9.95725632e-01 4.27434454e-03]\n [9.93207395e-01 6.79259514e-03]]\nvalidation batch: 2, val_loss: 0.005132023598590906, valid_preds_fold: [[9.85486746e-01 1.45132402e-02]\n [9.37698841e-01 6.23011515e-02]\n [9.77831125e-01 2.21689045e-02]\n [1.57260180e-01 8.42739820e-01]\n [2.46273266e-04 9.99753773e-01]\n [1.95223882e-04 9.99804795e-01]\n [1.37489410e-02 9.86251116e-01]\n [5.95725561e-03 9.94042695e-01]]\nvalidation batch: 3, val_loss: 0.00781961965517406, valid_preds_fold: [[3.80302372e-04 9.99619722e-01]\n [6.19675517e-01 3.80324513e-01]\n [9.29801941e-01 7.01980814e-02]\n [9.08561647e-01 9.14383829e-02]\n [3.59346566e-04 9.99640584e-01]\n [5.63580601e-04 9.99436438e-01]\n [4.61854739e-04 9.99538183e-01]\n [4.06570180e-04 9.99593437e-01]]\nvalidation batch: 4, val_loss: 0.01397913319133494, valid_preds_fold: [[2.87478324e-04 9.99712527e-01]\n [9.58239317e-01 4.17606607e-02]\n [9.50162113e-01 4.98378985e-02]\n [1.43503398e-01 8.56496572e-01]\n [1.71567395e-01 8.28432560e-01]\n [8.29029083e-01 1.70970902e-01]\n [9.76200938e-01 2.37990189e-02]\n [9.35493946e-01 6.45060018e-02]]\nvalidation batch: 5, val_loss: 0.015239670600769294, valid_preds_fold: [[0.96334249 0.03665754]\n [0.96374851 0.03625143]\n [0.94480509 0.05519495]\n [0.78465927 0.21534073]\n [0.95792186 0.0420781 ]\n [0.39090416 0.60909581]\n [0.9930737  0.00692632]\n [0.98141176 0.01858829]]\nvalidation batch: 6, val_loss: 0.020085237593981474, valid_preds_fold: [[0.94642949 0.0535705 ]\n [0.98629272 0.01370731]\n [0.99192643 0.00807357]\n [0.99227154 0.00772852]\n [0.9399482  0.06005184]\n [0.91730326 0.08269677]\n [0.81101972 0.18898033]\n [0.94435602 0.05564396]]\nvalidation batch: 7, val_loss: 0.02954055907299919, valid_preds_fold: [[0.54945928 0.45054072]\n [0.77692002 0.22308002]\n [0.96546459 0.03453543]\n [0.00363835 0.99636167]\n [0.00257403 0.99742597]\n [0.96406931 0.03593073]\n [0.05641866 0.94358134]\n [0.95994765 0.04005233]]\nvalidation batch: 8, val_loss: 0.02976977923055635, valid_preds_fold: [[0.07992467 0.9200753 ]\n [0.99153417 0.00846585]\n [0.91151303 0.08848697]\n [0.01625875 0.98374128]\n [0.99051601 0.00948394]\n [0.98766553 0.0123345 ]\n [0.02355999 0.97644001]\n [0.00458955 0.9954105 ]]\nvalidation batch: 9, val_loss: 0.03035141179596421, valid_preds_fold: [[9.76126699e-04 9.99023914e-01]\n [9.08893406e-01 9.11065564e-02]\n [8.46476614e-01 1.53523326e-01]\n [6.96809846e-04 9.99303222e-01]\n [8.94947708e-01 1.05052240e-01]\n [9.83312547e-01 1.66873876e-02]\n [7.90966570e-01 2.09033385e-01]\n [9.88785386e-01 1.12146139e-02]]\nvalidation batch: 10, val_loss: 0.03441358189078143, valid_preds_fold: [[0.98923802 0.01076193]\n [0.79859459 0.20140541]\n [0.94000852 0.05999141]\n [0.94604284 0.05395723]\n [0.82927775 0.17072226]\n [0.97920907 0.02079095]\n [0.97447258 0.02552742]\n [0.98729557 0.01270444]]\nvalidation batch: 11, val_loss: 0.04094396422814279, valid_preds_fold: [[0.23673722 0.76326275]\n [0.20959595 0.79040408]\n [0.56499588 0.43500414]\n [0.44294351 0.55705649]\n [0.82772672 0.17227328]\n [0.34835562 0.65164441]\n [0.29237592 0.70762414]\n [0.52670556 0.47329447]]\nvalidation batch: 12, val_loss: 0.042797859241492556, valid_preds_fold: [[0.07362166 0.92637837]\n [0.27873042 0.72126955]\n [0.30836999 0.69163001]\n [0.97905397 0.02094605]\n [0.90842783 0.09157217]\n [0.92589778 0.07410227]\n [0.98007548 0.01992458]\n [0.90950316 0.09049682]]\nvalidation batch: 13, val_loss: 0.04314141649834431, valid_preds_fold: [[0.99220568 0.00779429]\n [0.927737   0.07226305]\n [0.99678218 0.00321781]\n [0.92683804 0.073162  ]\n [0.9779579  0.0220421 ]\n [0.93249154 0.0675085 ]\n [0.97576523 0.0242347 ]\n [0.90683901 0.09316101]]\nvalidation batch: 14, val_loss: 0.0470944317805506, valid_preds_fold: [[0.91726172 0.08273827]\n [0.37155291 0.62844712]\n [0.93935078 0.0606492 ]\n [0.98464453 0.0153555 ]\n [0.96530014 0.03469985]\n [0.68893361 0.31106639]\n [0.98129547 0.01870459]\n [0.98891526 0.01108474]]\nvalidation batch: 15, val_loss: 0.04796654024045833, valid_preds_fold: [[0.85912418 0.14087579]\n [0.86191285 0.13808721]\n [0.98147649 0.01852348]\n [0.65737605 0.34262395]\n [0.99352854 0.00647138]\n [0.97568583 0.02431419]\n [0.83444524 0.16555481]\n [0.99491656 0.00508346]]\nvalidation batch: 16, val_loss: 0.04901959812336595, valid_preds_fold: [[0.98347837 0.01652163]\n [0.97019494 0.02980507]\n [0.56747711 0.43252286]\n [0.84310585 0.15689418]\n [0.96656859 0.03343141]\n [0.96098584 0.03901419]\n [0.98815674 0.01184319]\n [0.98734272 0.01265731]]\nvalidation batch: 17, val_loss: 0.049468138085229554, valid_preds_fold: [[0.97219163 0.02780841]\n [0.8655504  0.1344496 ]\n [0.93328828 0.06671176]\n [0.92660737 0.07339258]\n [0.92577291 0.07422706]\n [0.05115594 0.94884408]\n [0.96723592 0.03276402]\n [0.98926675 0.01073329]]\nvalidation batch: 18, val_loss: 0.049709245181866805, valid_preds_fold: [[0.97267616 0.02732381]\n [0.99197352 0.00802655]\n [0.99652332 0.00347666]\n [0.9892965  0.01070353]\n [0.93532187 0.06467816]\n [0.96940529 0.03059473]\n [0.92075604 0.07924391]\n [0.9668141  0.03318594]]\nvalidation batch: 19, val_loss: 0.05023283451578043, valid_preds_fold: [[0.95254529 0.04745473]\n [0.73539859 0.26460144]\n [0.98778129 0.01221872]\n [0.98838657 0.01161337]\n [0.90780652 0.09219351]\n [0.98693991 0.01306014]\n [0.980681   0.01931905]\n [0.93749857 0.06250141]]\nvalidation batch: 20, val_loss: 0.05552909006602573, valid_preds_fold: [[1.71541621e-03 9.98284519e-01]\n [9.87354219e-01 1.26457224e-02]\n [9.90942419e-01 9.05754045e-03]\n [9.60958481e-01 3.90415750e-02]\n [9.20043111e-01 7.99569413e-02]\n [9.91359115e-01 8.64090864e-03]\n [8.41726258e-04 9.99158263e-01]\n [2.23601912e-03 9.97763991e-01]]\nvalidation batch: 21, val_loss: 0.05560724328469186, valid_preds_fold: [[1.54634740e-03 9.98453617e-01]\n [9.69385803e-01 3.06142289e-02]\n [2.16693827e-03 9.97833073e-01]\n [4.24007047e-03 9.95759904e-01]\n [2.21930095e-03 9.97780740e-01]\n [4.21753526e-02 9.57824647e-01]\n [8.00020003e-04 9.99200046e-01]\n [4.84400312e-04 9.99515653e-01]]\nvalidation batch: 22, val_loss: 0.0585104280362164, valid_preds_fold: [[0.00188334 0.99811673]\n [0.00551835 0.99448168]\n [0.0015247  0.99847525]\n [0.10555091 0.89444911]\n [0.18680944 0.81319058]\n [0.0294595  0.97054052]\n [0.07177316 0.92822689]\n [0.72178233 0.27821773]]\nvalidation batch: 23, val_loss: 0.07071771510761149, valid_preds_fold: [[0.08515257 0.91484737]\n [0.00240833 0.99759173]\n [0.35086095 0.64913899]\n [0.03785684 0.96214318]\n [0.96849525 0.03150475]\n [0.04377925 0.95622075]\n [0.00101224 0.99898773]\n [0.00491923 0.99508083]]\nvalidation batch: 24, val_loss: 0.07718132109972682, valid_preds_fold: [[9.83392703e-04 9.99016643e-01]\n [4.46146876e-02 9.55385327e-01]\n [8.60098779e-01 1.39901191e-01]\n [8.76638830e-01 1.23361163e-01]\n [9.74292874e-01 2.57071443e-02]\n [9.75248516e-01 2.47514639e-02]\n [9.96512830e-01 3.48719023e-03]\n [1.52420567e-03 9.98475850e-01]]\nvalidation batch: 25, val_loss: 0.08210447463240937, valid_preds_fold: [[0.03044825 0.96955174]\n [0.86804771 0.13195229]\n [0.98999947 0.01000053]\n [0.95045191 0.04954802]\n [0.77101928 0.22898072]\n [0.97366142 0.02633858]\n [0.00739604 0.99260396]\n [0.04325238 0.95674759]]\nvalidation batch: 26, val_loss: 0.08385148568310007, valid_preds_fold: [[0.42852375 0.57147628]\n [0.96837956 0.03162048]\n [0.00184067 0.99815935]\n [0.39064538 0.60935462]\n [0.03839701 0.96160299]\n [0.00298751 0.9970125 ]\n [0.9515692  0.04843073]\n [0.00159672 0.99840325]]\nvalidation batch: 27, val_loss: 0.08621634281899807, valid_preds_fold: [[0.38766947 0.6123305 ]\n [0.78269857 0.21730147]\n [0.00202335 0.9979766 ]\n [0.00190487 0.9980951 ]\n [0.96856111 0.03143891]\n [0.00239241 0.99760759]\n [0.92837346 0.0716266 ]\n [0.00520617 0.99479383]]\nvalidation batch: 28, val_loss: 0.09032097186920415, valid_preds_fold: [[0.9698813  0.03011874]\n [0.95308673 0.04691333]\n [0.02687207 0.97312796]\n [0.01066297 0.98933697]\n [0.8777808  0.1222192 ]\n [0.95910329 0.04089674]\n [0.52117872 0.47882125]\n [0.00151207 0.99848795]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 29, val_loss: 0.093423000543657, valid_preds_fold: [[0.05075657 0.94924337]\n [0.14910208 0.85089797]\n [0.01480016 0.98519981]\n [0.53583676 0.46416327]\n [0.69682187 0.30317813]\n [0.67676485 0.32323515]\n [0.99029142 0.00970862]\n [0.95660949 0.04339056]]\nvalidation batch: 30, val_loss: 0.09750681843635808, valid_preds_fold: [[0.64417946 0.35582057]\n [0.92455727 0.07544272]\n [0.95824414 0.04175584]\n [0.88722271 0.11277732]\n [0.83779335 0.16220665]\n [0.94627762 0.05372243]\n [0.72401464 0.27598536]\n [0.89859366 0.1014064 ]]\nvalidation batch: 31, val_loss: 0.09797145676438825, valid_preds_fold: [[0.9958424  0.00415762]\n [0.9596734  0.0403266 ]\n [0.98377615 0.01622382]\n [0.78259528 0.21740475]\n [0.89554751 0.10445253]\n [0.94285709 0.05714286]\n [0.00584088 0.9941591 ]\n [0.97297192 0.027028  ]]\nvalidation batch: 32, val_loss: 0.09993709395401668, valid_preds_fold: [[9.83460903e-01 1.65390465e-02]\n [4.38108837e-04 9.99561846e-01]\n [2.54226997e-02 9.74577308e-01]\n [9.76769486e-04 9.99023199e-01]\n [2.84668501e-03 9.97153282e-01]\n [1.37935542e-02 9.86206472e-01]\n [1.57703441e-02 9.84229624e-01]\n [8.74801874e-01 1.25198156e-01]]\nvalidation batch: 33, val_loss: 0.10527357611342937, valid_preds_fold: [[1.26458553e-03 9.98735368e-01]\n [9.89604712e-01 1.03953071e-02]\n [9.84182775e-01 1.58172492e-02]\n [9.92308855e-01 7.69109651e-03]\n [8.66200805e-01 1.33799195e-01]\n [2.00656964e-03 9.97993469e-01]\n [8.34333012e-04 9.99165654e-01]\n [9.77610052e-01 2.23898832e-02]]\nvalidation batch: 34, val_loss: 0.11053422941778697, valid_preds_fold: [[0.6458869  0.3541131 ]\n [0.47252554 0.52747446]\n [0.00112056 0.99887937]\n [0.97851151 0.02148847]\n [0.98864657 0.01135342]\n [0.97211838 0.02788154]\n [0.98619777 0.01380221]\n [0.96500176 0.03499823]]\nvalidation batch: 35, val_loss: 0.11185590563899409, valid_preds_fold: [[0.97311312 0.02688684]\n [0.97025913 0.02974084]\n [0.93162638 0.06837361]\n [0.29763302 0.70236701]\n [0.99064285 0.00935714]\n [0.94663393 0.05336604]\n [0.96573144 0.03426858]\n [0.99076372 0.00923632]]\nvalidation batch: 36, val_loss: 0.11907659503665283, valid_preds_fold: [[0.85330623 0.14669374]\n [0.97174782 0.02825226]\n [0.0994859  0.90051413]\n [0.98188365 0.01811636]\n [0.97977495 0.02022501]\n [0.8142851  0.18571489]\n [0.88265997 0.11734002]\n [0.74193615 0.25806385]]\nvalidation batch: 37, val_loss: 0.12162864969594635, valid_preds_fold: [[0.00102021 0.99897975]\n [0.94854915 0.05145086]\n [0.98634249 0.01365752]\n [0.98058861 0.01941142]\n [0.92718112 0.07281893]\n [0.82444674 0.17555322]\n [0.98964792 0.01035213]\n [0.91203362 0.08796639]]\nvalidation batch: 38, val_loss: 0.1288490545575636, valid_preds_fold: [[0.96980345 0.03019657]\n [0.98050815 0.01949188]\n [0.98200732 0.01799267]\n [0.98487467 0.01512527]\n [0.99066532 0.00933468]\n [0.83865768 0.16134232]\n [0.92498404 0.07501593]\n [0.03316832 0.96683168]]\nvalidation batch: 39, val_loss: 0.13408465520308835, valid_preds_fold: [[6.87172294e-01 3.12827706e-01]\n [7.74853826e-01 2.25146115e-01]\n [5.36189240e-04 9.99463856e-01]\n [9.37825814e-02 9.06217396e-01]\n [8.33946094e-03 9.91660535e-01]\n [4.97568399e-02 9.50243175e-01]\n [1.11980764e-02 9.88801956e-01]\n [8.52229357e-01 1.47770569e-01]]\nvalidation batch: 40, val_loss: 0.1401526368882534, valid_preds_fold: [[2.91543510e-02 9.70845640e-01]\n [8.22203875e-01 1.77796140e-01]\n [2.75274651e-04 9.99724686e-01]\n [6.34057447e-04 9.99365985e-01]\n [7.31330132e-04 9.99268711e-01]\n [9.83851135e-01 1.61488950e-02]\n [9.92329299e-01 7.67076667e-03]\n [9.94472563e-01 5.52740367e-03]]\nvalidation batch: 41, val_loss: 0.14634931674838936, valid_preds_fold: [[0.00122104 0.998779  ]\n [0.81050694 0.18949308]\n [0.91368264 0.08631738]\n [0.96191603 0.03808399]\n [0.94681662 0.05318331]\n [0.99508238 0.00491769]\n [0.95735133 0.04264868]\n [0.99235702 0.00764306]]\nvalidation batch: 42, val_loss: 0.1467701372330206, valid_preds_fold: [[0.97019774 0.02980227]\n [0.98679209 0.01320792]\n [0.9698751  0.03012485]\n [0.76048321 0.23951682]\n [0.98851043 0.01148962]\n [0.95280081 0.04719923]\n [0.99310994 0.00689005]\n [0.95460343 0.04539654]]\nvalidation batch: 43, val_loss: 0.14973143738334194, valid_preds_fold: [[9.70253706e-01 2.97463015e-02]\n [1.16144769e-01 8.83855224e-01]\n [6.03445657e-02 9.39655483e-01]\n [8.49272132e-01 1.50727928e-01]\n [9.84451532e-01 1.55484416e-02]\n [9.75484312e-01 2.45157089e-02]\n [9.23456252e-01 7.65438229e-02]\n [7.41595577e-04 9.99258339e-01]]\nvalidation batch: 44, val_loss: 0.15133776015391312, valid_preds_fold: [[9.76993084e-01 2.30069272e-02]\n [1.63534982e-03 9.98364627e-01]\n [7.95794308e-01 2.04205632e-01]\n [9.14220929e-01 8.57790038e-02]\n [1.19973710e-02 9.88002658e-01]\n [9.66963887e-01 3.30361016e-02]\n [9.89207685e-01 1.07922349e-02]\n [7.91329425e-04 9.99208748e-01]]\nvalidation batch: 45, val_loss: 0.151712508151566, valid_preds_fold: [[2.84421863e-03 9.97155786e-01]\n [1.53558468e-03 9.98464465e-01]\n [7.49896863e-04 9.99250114e-01]\n [1.58551754e-03 9.98414516e-01]\n [1.30890533e-01 8.69109452e-01]\n [7.90921688e-01 2.09078312e-01]\n [9.81439769e-01 1.85602270e-02]\n [9.89629209e-01 1.03707649e-02]]\nvalidation batch: 46, val_loss: 0.15512971618097193, valid_preds_fold: [[0.90925169 0.09074825]\n [0.91727859 0.0827214 ]\n [0.89476079 0.10523925]\n [0.9081322  0.09186781]\n [0.98930794 0.01069208]\n [0.92173064 0.07826932]\n [0.96529263 0.03470735]\n [0.60311544 0.39688456]]\nvalidation batch: 47, val_loss: 0.15633774983839396, valid_preds_fold: [[6.59048676e-01 3.40951324e-01]\n [9.97396708e-01 2.60326196e-03]\n [8.42656970e-01 1.57342985e-01]\n [8.08219671e-01 1.91780373e-01]\n [7.81157970e-01 2.18842030e-01]\n [9.86104190e-01 1.38957975e-02]\n [7.71704435e-01 2.28295550e-01]\n [1.99090282e-04 9.99800980e-01]]\nvalidation batch: 48, val_loss: 0.1625903771415244, valid_preds_fold: [[8.11486185e-01 1.88513815e-01]\n [9.84544992e-01 1.54549517e-02]\n [1.55236907e-04 9.99844790e-01]\n [1.71910062e-01 8.28089893e-01]\n [4.23343107e-02 9.57665622e-01]\n [3.16785946e-02 9.68321383e-01]\n [9.96291399e-01 3.70867481e-03]\n [8.89883220e-01 1.10116720e-01]]\nvalidation batch: 49, val_loss: 0.16711284562836598, valid_preds_fold: [[0.01407064 0.98592931]\n [0.99112481 0.00887514]\n [0.9958936  0.0041064 ]\n [0.96731067 0.03268929]\n [0.83951461 0.16048536]\n [0.85709673 0.14290328]\n [0.91931975 0.08068022]\n [0.87123835 0.12876163]]\nvalidation batch: 50, val_loss: 0.17388729680411136, valid_preds_fold: [[9.77745116e-01 2.22549643e-02]\n [1.37516242e-02 9.86248434e-01]\n [9.46731567e-01 5.32684550e-02]\n [4.39031124e-01 5.60968816e-01]\n [6.75030518e-04 9.99325037e-01]\n [6.79120480e-04 9.99320865e-01]\n [1.02731597e-03 9.98972654e-01]\n [8.86951089e-02 9.11304891e-01]]\nvalidation batch: 51, val_loss: 0.1757722175578131, valid_preds_fold: [[3.61391692e-04 9.99638557e-01]\n [9.86444056e-01 1.35559496e-02]\n [9.96546447e-01 3.45357764e-03]\n [2.98638761e-01 7.01361179e-01]\n [9.44283962e-01 5.57159819e-02]\n [5.06944656e-01 4.93055373e-01]\n [9.69603658e-02 9.03039634e-01]\n [1.21136766e-03 9.98788655e-01]]\nvalidation batch: 52, val_loss: 0.17889006821996103, valid_preds_fold: [[0.95103955 0.0489605 ]\n [0.00254348 0.99745649]\n [0.41302246 0.5869776 ]\n [0.96573144 0.03426855]\n [0.98509961 0.01490033]\n [0.98672444 0.01327562]\n [0.98400545 0.01599459]\n [0.90935409 0.09064591]]\nvalidation batch: 53, val_loss: 0.18187086452750392, valid_preds_fold: [[3.27501883e-04 9.99672532e-01]\n [4.24917758e-01 5.75082302e-01]\n [9.93406653e-01 6.59338478e-03]\n [2.97647843e-04 9.99702275e-01]\n [1.18085995e-01 8.81913960e-01]\n [9.18078601e-01 8.19213614e-02]\n [1.62448302e-01 8.37551713e-01]\n [4.75479383e-03 9.95245159e-01]]\nvalidation batch: 54, val_loss: 0.18467044966281765, valid_preds_fold: [[0.00152917 0.99847084]\n [0.01602547 0.98397452]\n [0.9598031  0.04019686]\n [0.05033816 0.94966185]\n [0.00170645 0.99829358]\n [0.99421108 0.00578893]\n [0.0119647  0.98803526]\n [0.00108978 0.99891019]]\nvalidation batch: 55, val_loss: 0.189333416304014, valid_preds_fold: [[0.00102539 0.99897468]\n [0.96762377 0.0323763 ]\n [0.97164696 0.02835309]\n [0.74071264 0.25928736]\n [0.99582654 0.00417339]\n [0.99378669 0.00621327]\n [0.93541992 0.06458008]\n [0.91698718 0.08301284]]\nvalidation batch: 56, val_loss: 0.19539197175389658, valid_preds_fold: [[0.49642652 0.50357348]\n [0.71019149 0.28980848]\n [0.00197239 0.99802762]\n [0.94224972 0.05775032]\n [0.53016758 0.46983242]\n [0.27454671 0.72545332]\n [0.96764809 0.03235191]\n [0.94451702 0.05548297]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 57, val_loss: 0.19904028548158867, valid_preds_fold: [[0.03302799 0.96697199]\n [0.71742404 0.28257599]\n [0.67582786 0.32417217]\n [0.93142217 0.06857777]\n [0.60678768 0.39321232]\n [0.9559955  0.04400446]\n [0.01393599 0.98606396]\n [0.00265865 0.99734133]]\nvalidation batch: 58, val_loss: 0.1995365152502582, valid_preds_fold: [[0.89905304 0.10094693]\n [0.81489301 0.18510699]\n [0.99630368 0.00369636]\n [0.09332258 0.90667737]\n [0.01869488 0.98130506]\n [0.05610933 0.94389063]\n [0.00938964 0.9906103 ]\n [0.04403463 0.9559654 ]]\nvalidation batch: 59, val_loss: 0.19993779079539933, valid_preds_fold: [[2.86789291e-04 9.99713242e-01]\n [1.22492376e-03 9.98775065e-01]\n [6.68123271e-03 9.93318796e-01]\n [6.66124630e-04 9.99333799e-01]\n [3.47880065e-01 6.52119935e-01]\n [4.40125179e-04 9.99559820e-01]\n [4.67004749e-04 9.99533057e-01]\n [2.47822073e-03 9.97521818e-01]]\nvalidation batch: 60, val_loss: 0.20130172866756899, valid_preds_fold: [[7.23751903e-01 2.76248097e-01]\n [8.66259099e-04 9.99133766e-01]\n [4.45813894e-01 5.54186106e-01]\n [5.03174379e-04 9.99496818e-01]\n [9.88901258e-01 1.10988105e-02]\n [9.77047801e-01 2.29522437e-02]\n [9.15676236e-01 8.43237117e-02]\n [3.67118180e-01 6.32881820e-01]]\nvalidation batch: 61, val_loss: 0.20229730004594274, valid_preds_fold: [[0.98967934 0.01032065]\n [0.94817728 0.05182275]\n [0.95015037 0.04984955]\n [0.90755057 0.09244946]\n [0.80534846 0.19465157]\n [0.98365211 0.01634783]\n [0.98861605 0.01138395]\n [0.52993298 0.47006699]]\nvalidation batch: 62, val_loss: 0.207315638280698, valid_preds_fold: [[0.73015481 0.26984513]\n [0.97483861 0.02516136]\n [0.86750853 0.13249148]\n [0.60971415 0.39028582]\n [0.39142087 0.60857916]\n [0.95652211 0.04347787]\n [0.65877628 0.34122372]\n [0.02723034 0.97276962]]\nvalidation batch: 63, val_loss: 0.2180861558248527, valid_preds_fold: [[9.67672408e-01 3.23275924e-02]\n [3.32363439e-03 9.96676326e-01]\n [8.67703080e-01 1.32296890e-01]\n [2.26117931e-02 9.77388203e-01]\n [7.04719510e-04 9.99295235e-01]\n [4.90828010e-04 9.99509215e-01]\n [4.47467476e-01 5.52532494e-01]\n [9.59804654e-01 4.01953422e-02]]\nvalidation batch: 64, val_loss: 0.22162337824158426, valid_preds_fold: [[4.42672899e-04 9.99557316e-01]\n [1.49004068e-02 9.85099614e-01]\n [4.22119861e-03 9.95778799e-01]\n [2.53047142e-02 9.74695325e-01]\n [1.67055626e-03 9.98329461e-01]\n [2.46710959e-03 9.97532845e-01]\n [8.71014833e-01 1.28985152e-01]\n [9.62609529e-01 3.73904556e-02]]\nvalidation batch: 65, val_loss: 0.22189102979907155, valid_preds_fold: [[7.94462204e-01 2.05537826e-01]\n [9.58346605e-01 4.16534543e-02]\n [2.41048331e-03 9.97589588e-01]\n [8.06671858e-04 9.99193370e-01]\n [1.34484679e-03 9.98655081e-01]\n [4.83342400e-03 9.95166540e-01]\n [9.96431708e-01 3.56822764e-03]\n [9.92305338e-01 7.69463461e-03]]\nvalidation batch: 66, val_loss: 0.22270845377097165, valid_preds_fold: [[0.9822073  0.01779271]\n [0.99404705 0.00595299]\n [0.96318132 0.0368187 ]\n [0.86593145 0.1340685 ]\n [0.00190879 0.99809116]\n [0.89391708 0.10608296]\n [0.93981445 0.0601855 ]\n [0.59786874 0.40213126]]\nvalidation batch: 67, val_loss: 0.22343724036086215, valid_preds_fold: [[0.97193152 0.0280684 ]\n [0.9958865  0.00411351]\n [0.9609198  0.03908021]\n [0.89903647 0.1009635 ]\n [0.99648333 0.00351665]\n [0.97878498 0.02121502]\n [0.55310374 0.44689626]\n [0.0026826  0.99731737]]\nvalidation batch: 68, val_loss: 0.22773005954758094, valid_preds_fold: [[0.8839367  0.11606326]\n [0.90899968 0.0910003 ]\n [0.98487639 0.01512363]\n [0.93225813 0.0677418 ]\n [0.00173656 0.99826342]\n [0.77087778 0.22912224]\n [0.0842177  0.91578227]\n [0.36312187 0.63687807]]\nvalidation batch: 69, val_loss: 0.2326410467716029, valid_preds_fold: [[0.00596404 0.99403596]\n [0.92250091 0.07749904]\n [0.99719632 0.00280363]\n [0.14022063 0.85977942]\n [0.98550481 0.01449523]\n [0.96500057 0.03499942]\n [0.98889869 0.01110132]\n [0.55378062 0.44621941]]\nvalidation batch: 70, val_loss: 0.23786474563127016, valid_preds_fold: [[0.01322511 0.98677492]\n [0.97400248 0.02599755]\n [0.807006   0.192994  ]\n [0.9921335  0.0078665 ]\n [0.89100653 0.10899352]\n [0.95641261 0.04358735]\n [0.8021971  0.19780293]\n [0.9423942  0.05760582]]\nvalidation batch: 71, val_loss: 0.2444562640612143, valid_preds_fold: [[0.97324002 0.02675995]\n [0.96784669 0.03215326]\n [0.08540048 0.91459954]\n [0.95325696 0.04674306]\n [0.00159441 0.99840564]\n [0.91837537 0.08162463]\n [0.94589275 0.05410723]\n [0.01095616 0.98904383]]\nvalidation batch: 72, val_loss: 0.24467092377208446, valid_preds_fold: [[3.77979339e-03 9.96220171e-01]\n [9.78856206e-01 2.11437456e-02]\n [1.52083789e-03 9.98479187e-01]\n [4.78408969e-04 9.99521613e-01]\n [1.69053614e-01 8.30946386e-01]\n [9.83863592e-01 1.61364228e-02]\n [9.93919969e-01 6.07999647e-03]\n [5.52342273e-04 9.99447644e-01]]\nvalidation batch: 73, val_loss: 0.24763001180695793, valid_preds_fold: [[1.86966790e-04 9.99813020e-01]\n [9.32272553e-01 6.77274391e-02]\n [1.26943458e-03 9.98730600e-01]\n [3.26018478e-03 9.96739805e-01]\n [2.83158943e-03 9.97168362e-01]\n [6.03941977e-01 3.96057993e-01]\n [2.89999917e-02 9.71000016e-01]\n [9.58557986e-03 9.90414381e-01]]\nvalidation batch: 74, val_loss: 0.24924392035625278, valid_preds_fold: [[0.22053973 0.77946025]\n [0.00194464 0.99805534]\n [0.12583286 0.8741672 ]\n [0.93943417 0.06056579]\n [0.99177998 0.00821998]\n [0.98274946 0.01725047]\n [0.03076748 0.96923256]\n [0.00135177 0.99864823]]\nvalidation batch: 75, val_loss: 0.24948660556199778, valid_preds_fold: [[4.62387776e-04 9.99537587e-01]\n [1.73711609e-02 9.82628822e-01]\n [8.77233148e-01 1.22766837e-01]\n [9.67674315e-01 3.23256478e-02]\n [9.76387322e-01 2.36127358e-02]\n [9.55534875e-01 4.44651209e-02]\n [9.90178227e-01 9.82171856e-03]\n [9.95107651e-01 4.89232270e-03]]\nvalidation batch: 76, val_loss: 0.25054195822372927, valid_preds_fold: [[0.97888559 0.02111442]\n [0.8286432  0.17135683]\n [0.97881991 0.02118007]\n [0.09532989 0.90467012]\n [0.8468188  0.15318124]\n [0.59617239 0.40382761]\n [0.12328319 0.87671679]\n [0.01064548 0.98935449]]\nvalidation batch: 77, val_loss: 0.25643746103466, valid_preds_fold: [[0.92095482 0.07904521]\n [0.06983035 0.93016958]\n [0.22333208 0.77666789]\n [0.97491723 0.02508273]\n [0.60195607 0.3980439 ]\n [0.26521802 0.73478192]\n [0.93165308 0.06834691]\n [0.76773733 0.23226267]]\nvalidation batch: 78, val_loss: 0.26003006399765505, valid_preds_fold: [[0.98022664 0.01977342]\n [0.51894897 0.48105097]\n [0.4626652  0.5373348 ]\n [0.76664913 0.23335087]\n [0.71799493 0.28200507]\n [0.26652306 0.73347694]\n [0.0039769  0.99602306]\n [0.62866509 0.37133488]]\nvalidation batch: 79, val_loss: 0.26302182494941423, valid_preds_fold: [[6.30036473e-01 3.69963497e-01]\n [6.15512788e-01 3.84487182e-01]\n [8.61697674e-01 1.38302296e-01]\n [7.47669116e-03 9.92523372e-01]\n [6.53538406e-02 9.34646130e-01]\n [8.74501828e-04 9.99125540e-01]\n [7.61822581e-01 2.38177463e-01]\n [5.35695348e-03 9.94643092e-01]]\nvalidation batch: 80, val_loss: 0.26359462422611074, valid_preds_fold: [[0.01131942 0.9886806 ]\n [0.94996327 0.05003675]\n [0.05673207 0.94326794]\n [0.64217317 0.35782686]\n [0.99550772 0.00449222]\n [0.97593021 0.02406985]\n [0.01122564 0.9887743 ]\n [0.02333766 0.9766624 ]]\nvalidation batch: 81, val_loss: 0.26687304588565, valid_preds_fold: [[8.30916339e-04 9.99169111e-01]\n [2.96422958e-01 7.03577042e-01]\n [9.28825140e-01 7.11748227e-02]\n [9.49392736e-01 5.06072603e-02]\n [2.17600420e-04 9.99782383e-01]\n [4.27512452e-03 9.95724916e-01]\n [1.96180030e-04 9.99803841e-01]\n [8.36483538e-01 1.63516536e-01]]\nvalidation batch: 82, val_loss: 0.2695212724217534, valid_preds_fold: [[8.24059243e-04 9.99175966e-01]\n [9.74502504e-01 2.54975334e-02]\n [9.72556710e-01 2.74433140e-02]\n [9.96648848e-01 3.35115916e-03]\n [8.96279216e-01 1.03720814e-01]\n [9.60447192e-01 3.95528786e-02]\n [9.13363099e-01 8.66368935e-02]\n [7.79789567e-01 2.20210463e-01]]\nvalidation batch: 83, val_loss: 0.27030875631710044, valid_preds_fold: [[0.82742608 0.17257391]\n [0.88317752 0.1168225 ]\n [0.81642175 0.18357825]\n [0.9612059  0.03879412]\n [0.95040792 0.04959213]\n [0.99514353 0.00485645]\n [0.95779103 0.04220896]\n [0.81206971 0.18793024]]\nvalidation batch: 84, val_loss: 0.27784584718246536, valid_preds_fold: [[9.87423599e-01 1.25763835e-02]\n [3.53094161e-01 6.46905839e-01]\n [9.35776949e-01 6.42230660e-02]\n [8.83299768e-01 1.16700158e-01]\n [8.37679684e-01 1.62320375e-01]\n [2.92260665e-04 9.99707758e-01]\n [9.53190446e-01 4.68095541e-02]\n [9.39118266e-01 6.08817749e-02]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 85, val_loss: 0.2785584466518277, valid_preds_fold: [[9.82899845e-01 1.71001870e-02]\n [2.92050041e-04 9.99707878e-01]\n [1.14102647e-01 8.85897338e-01]\n [8.20037007e-01 1.79963037e-01]\n [6.88997984e-01 3.11002076e-01]\n [9.89916980e-01 1.00830207e-02]\n [2.69232579e-02 9.73076820e-01]\n [3.33947316e-02 9.66605246e-01]]\nvalidation batch: 86, val_loss: 0.2827301992334589, valid_preds_fold: [[0.92091328 0.07908671]\n [0.00277358 0.99722642]\n [0.99299991 0.00700014]\n [0.00772571 0.99227422]\n [0.03189761 0.9681024 ]\n [0.66126007 0.33873993]\n [0.42425129 0.57574868]\n [0.94046855 0.05953145]]\nvalidation batch: 87, val_loss: 0.29151069780770883, valid_preds_fold: [[9.93792236e-01 6.20780466e-03]\n [9.77404892e-01 2.25950871e-02]\n [4.42842618e-02 9.55715775e-01]\n [4.15486097e-03 9.95845139e-01]\n [2.20614728e-02 9.77938533e-01]\n [1.80120673e-02 9.81987894e-01]\n [2.59342715e-02 9.74065721e-01]\n [2.75751925e-04 9.99724209e-01]]\nvalidation batch: 88, val_loss: 0.29452057857147973, valid_preds_fold: [[3.25530678e-01 6.74469352e-01]\n [8.65095615e-01 1.34904385e-01]\n [9.48679566e-01 5.13203815e-02]\n [2.39734747e-03 9.97602642e-01]\n [8.38936190e-04 9.99161124e-01]\n [5.09584497e-04 9.99490380e-01]\n [9.57734108e-01 4.22658734e-02]\n [9.28928912e-01 7.10711032e-02]]\nvalidation batch: 89, val_loss: 0.2967981355686258, valid_preds_fold: [[0.96931463 0.0306854 ]\n [0.98025572 0.01974429]\n [0.98756033 0.01243973]\n [0.94900852 0.05099151]\n [0.88716179 0.11283816]\n [0.97871095 0.02128899]\n [0.96636939 0.03363061]\n [0.13299529 0.86700475]]\nvalidation batch: 90, val_loss: 0.30193481208199136, valid_preds_fold: [[9.42686081e-01 5.73139638e-02]\n [9.75781143e-01 2.42189001e-02]\n [8.12473297e-01 1.87526703e-01]\n [9.90080178e-01 9.91982128e-03]\n [8.03587318e-04 9.99196470e-01]\n [7.87175493e-04 9.99212742e-01]\n [9.70069826e-01 2.99302116e-02]\n [1.62327632e-01 8.37672293e-01]]\nvalidation batch: 91, val_loss: 0.30491456900634906, valid_preds_fold: [[0.10267889 0.89732111]\n [0.06881231 0.93118769]\n [0.01346825 0.98653179]\n [0.00113808 0.99886191]\n [0.99291718 0.00708284]\n [0.4325904  0.56740963]\n [0.96004266 0.03995734]\n [0.98233539 0.01766459]]\nvalidation batch: 92, val_loss: 0.30596289789154585, valid_preds_fold: [[0.96763468 0.03236533]\n [0.8275727  0.17242733]\n [0.95009398 0.04990602]\n [0.98779827 0.01220178]\n [0.95863944 0.04136061]\n [0.9837833  0.01621667]\n [0.46961981 0.53038019]\n [0.84316939 0.15683058]]\nvalidation batch: 93, val_loss: 0.30760602435491385, valid_preds_fold: [[9.88359094e-01 1.16408467e-02]\n [8.21079433e-01 1.78920597e-01]\n [9.47148383e-01 5.28516397e-02]\n [8.37410626e-04 9.99162555e-01]\n [1.05224652e-02 9.89477456e-01]\n [9.30171635e-04 9.99069870e-01]\n [1.30563695e-03 9.98694360e-01]\n [3.91184643e-04 9.99608815e-01]]\nvalidation batch: 94, val_loss: 0.3104671029931438, valid_preds_fold: [[1.36116697e-02 9.86388266e-01]\n [5.71039680e-04 9.99428928e-01]\n [5.71982004e-04 9.99427974e-01]\n [2.23085997e-04 9.99776900e-01]\n [9.05544460e-01 9.44555700e-02]\n [9.43740606e-01 5.62594198e-02]\n [8.70422482e-01 1.29577488e-01]\n [9.95118976e-01 4.88108024e-03]]\nvalidation batch: 95, val_loss: 0.31337937984588377, valid_preds_fold: [[0.92107636 0.07892361]\n [0.93597203 0.06402797]\n [0.98769844 0.01230156]\n [0.7754389  0.22456115]\n [0.97904611 0.02095388]\n [0.87679774 0.12320232]\n [0.99458659 0.00541333]\n [0.14923887 0.85076118]]\nvalidation batch: 96, val_loss: 0.314025129062416, valid_preds_fold: [[0.91312778 0.08687229]\n [0.02381057 0.97618949]\n [0.06781761 0.93218243]\n [0.10453942 0.89546061]\n [0.9145757  0.08542436]\n [0.84809244 0.15190753]\n [0.9450115  0.05498851]\n [0.90348083 0.09651917]]\nvalidation batch: 97, val_loss: 0.31920249466478395, valid_preds_fold: [[0.85910481 0.14089516]\n [0.80061418 0.19938582]\n [0.80198461 0.19801544]\n [0.75583297 0.24416704]\n [0.00484102 0.99515897]\n [0.99192023 0.00807971]\n [0.001554   0.99844605]\n [0.0509375  0.94906253]]\nvalidation batch: 98, val_loss: 0.32407941892199277, valid_preds_fold: [[2.37664491e-01 7.62335479e-01]\n [6.43705964e-01 3.56294036e-01]\n [9.66039598e-01 3.39604206e-02]\n [2.52150901e-04 9.99747813e-01]\n [9.90759015e-01 9.24103800e-03]\n [8.83225575e-02 9.11677480e-01]\n [2.08591774e-01 7.91408300e-01]\n [3.51292850e-03 9.96487141e-01]]\nvalidation batch: 99, val_loss: 0.32946431310507507, valid_preds_fold: [[9.88587856e-01 1.14120748e-02]\n [5.80451509e-04 9.99419570e-01]\n [1.44522218e-03 9.98554766e-01]\n [5.56844694e-04 9.99443114e-01]\n [2.76425958e-01 7.23574042e-01]\n [3.01375124e-03 9.96986210e-01]\n [4.99995018e-04 9.99500036e-01]\n [1.27895117e-01 8.72104943e-01]]\nvalidation batch: 100, val_loss: 0.33369664271382526, valid_preds_fold: [[0.86910737 0.13089265]\n [0.01632182 0.98367822]\n [0.01807438 0.98192561]\n [0.00743144 0.99256855]\n [0.68382955 0.31617045]\n [0.01477006 0.98522991]\n [0.94606876 0.05393127]\n [0.98925531 0.01074469]]\nvalidation batch: 101, val_loss: 0.3339003678003368, valid_preds_fold: [[4.80490038e-03 9.95195091e-01]\n [4.75470268e-04 9.99524593e-01]\n [9.93978858e-01 6.02112664e-03]\n [9.31631684e-01 6.83683753e-02]\n [1.67714339e-03 9.98322785e-01]\n [4.04253893e-04 9.99595821e-01]\n [4.37445706e-04 9.99562562e-01]\n [8.70565236e-01 1.29434779e-01]]\nvalidation batch: 102, val_loss: 0.3347150868307936, valid_preds_fold: [[3.65504267e-04 9.99634504e-01]\n [6.49880269e-04 9.99350131e-01]\n [4.53795016e-01 5.46204925e-01]\n [5.65263326e-04 9.99434769e-01]\n [5.93495613e-04 9.99406457e-01]\n [9.85254228e-01 1.47457700e-02]\n [9.77502048e-01 2.24979483e-02]\n [7.80059040e-01 2.19940990e-01]]\nvalidation batch: 103, val_loss: 0.33917343420703927, valid_preds_fold: [[0.9332965  0.06670346]\n [0.01475748 0.98524255]\n [0.70758605 0.29241401]\n [0.93730205 0.06269795]\n [0.93530744 0.06469257]\n [0.94818556 0.05181441]\n [0.01048117 0.98951882]\n [0.5065158  0.49348417]]\nvalidation batch: 104, val_loss: 0.34264055046721975, valid_preds_fold: [[1.78170070e-01 8.21829915e-01]\n [9.12489593e-01 8.75104666e-02]\n [5.09239674e-01 4.90760386e-01]\n [9.71971214e-01 2.80288346e-02]\n [8.00164009e-04 9.99199808e-01]\n [6.65387213e-01 3.34612757e-01]\n [9.88293648e-01 1.17063355e-02]\n [9.92478132e-01 7.52179231e-03]]\nvalidation batch: 105, val_loss: 0.34358318291441375, valid_preds_fold: [[9.62552130e-01 3.74478363e-02]\n [9.70955372e-01 2.90445816e-02]\n [9.83401120e-01 1.65989529e-02]\n [9.44384548e-04 9.99055564e-01]\n [8.13552644e-04 9.99186456e-01]\n [7.35655893e-04 9.99264300e-01]\n [9.65720356e-01 3.42796966e-02]\n [5.98026633e-01 4.01973367e-01]]\nvalidation batch: 106, val_loss: 0.3482137646553291, valid_preds_fold: [[0.93271816 0.06728181]\n [0.86062568 0.13937429]\n [0.99384791 0.00615211]\n [0.97886866 0.0211313 ]\n [0.92566687 0.07433315]\n [0.6626265  0.33737347]\n [0.55247414 0.44752583]\n [0.6725744  0.3274256 ]]\nvalidation batch: 107, val_loss: 0.35239651755694934, valid_preds_fold: [[6.93682790e-01 3.06317240e-01]\n [9.13104892e-01 8.68950710e-02]\n [2.44346261e-03 9.97556567e-01]\n [2.84006119e-01 7.15993881e-01]\n [4.99860244e-03 9.95001376e-01]\n [6.16887759e-04 9.99383092e-01]\n [6.14805400e-01 3.85194540e-01]\n [2.19739806e-02 9.78025973e-01]]\nvalidation batch: 108, val_loss: 0.3668210051790642, valid_preds_fold: [[0.78432167 0.21567836]\n [0.86003184 0.13996816]\n [0.99825424 0.00174577]\n [0.99317706 0.00682292]\n [0.96351266 0.0364873 ]\n [0.69143772 0.30856225]\n [0.86003184 0.13996816]\n [0.98280632 0.01719363]]\nvalidation batch: 109, val_loss: 0.37088062097556407, valid_preds_fold: [[0.9421128  0.05788724]\n [0.67801881 0.32198122]\n [0.02182311 0.97817683]\n [0.75606251 0.24393749]\n [0.90667266 0.09332734]\n [0.96675628 0.03324369]\n [0.86263198 0.13736802]\n [0.9514187  0.04858135]]\nvalidation batch: 110, val_loss: 0.37215707141117466, valid_preds_fold: [[0.2918241  0.7081759 ]\n [0.98526913 0.01473087]\n [0.93985385 0.06014615]\n [0.96861768 0.03138226]\n [0.99108523 0.00891472]\n [0.99148262 0.00851736]\n [0.96810615 0.03189385]\n [0.99133098 0.00866897]]\nvalidation batch: 111, val_loss: 0.3742402641877641, valid_preds_fold: [[5.27364850e-01 4.72635120e-01]\n [9.87081766e-01 1.29182413e-02]\n [7.43269086e-01 2.56730914e-01]\n [8.18962336e-01 1.81037709e-01]\n [9.84792829e-01 1.52071537e-02]\n [9.79385376e-01 2.06146836e-02]\n [9.66455221e-01 3.35447825e-02]\n [5.95318561e-04 9.99404669e-01]]\n","name":"stdout"},{"output_type":"stream","text":"validation batch: 112, val_loss: 0.3751436617687671, valid_preds_fold: [[3.31720687e-03 9.96682823e-01]\n [4.47973579e-01 5.52026451e-01]\n [9.68113422e-01 3.18865217e-02]\n [9.20658827e-01 7.93411583e-02]\n [9.25268173e-01 7.47318566e-02]\n [9.63938236e-01 3.60618010e-02]\n [3.85657710e-04 9.99614358e-01]\n [8.49774122e-01 1.50225908e-01]]\nvalidation batch: 113, val_loss: 0.37677651404464335, valid_preds_fold: [[2.25576968e-03 9.97744203e-01]\n [8.30912292e-01 1.69087723e-01]\n [5.91939408e-03 9.94080603e-01]\n [8.66080751e-04 9.99133885e-01]\n [1.04664743e-03 9.98953342e-01]\n [1.02876208e-03 9.98971224e-01]\n [4.69873252e-04 9.99530196e-01]\n [6.60117716e-04 9.99339879e-01]]\nvalidation batch: 114, val_loss: 0.37678423480395856, valid_preds_fold: [[8.86857102e-04 9.99113142e-01]\n [1.75271870e-03 9.98247266e-01]\n [4.66661673e-04 9.99533415e-01]\n [5.04703843e-04 9.99495268e-01]\n [1.27556373e-03 9.98724401e-01]\n [5.92503580e-04 9.99407530e-01]\n [2.23513856e-03 9.97764826e-01]\n [7.41656695e-04 9.99258339e-01]]\nvalidation batch: 115, val_loss: 0.38098531113053763, valid_preds_fold: [[3.62230407e-04 9.99637723e-01]\n [5.44226468e-01 4.55773503e-01]\n [9.03832555e-01 9.61674228e-02]\n [9.64872718e-01 3.51272859e-02]\n [9.81342375e-01 1.86575521e-02]\n [8.94484043e-01 1.05515949e-01]\n [9.67266023e-01 3.27339545e-02]\n [9.75155890e-01 2.48441435e-02]]\nvalidation batch: 116, val_loss: 0.3827074799877014, valid_preds_fold: [[9.20080066e-01 7.99199864e-02]\n [9.83644187e-01 1.63558163e-02]\n [8.45074475e-01 1.54925480e-01]\n [6.89204454e-01 3.10795546e-01]\n [8.04354191e-01 1.95645884e-01]\n [3.17493366e-04 9.99682546e-01]\n [3.57658088e-01 6.42341912e-01]\n [9.51970462e-04 9.99048054e-01]]\nvalidation batch: 117, val_loss: 0.3876629083478538, valid_preds_fold: [[0.0417528  0.95824718]\n [0.2919274  0.7080726 ]\n [0.004042   0.99595803]\n [0.52625787 0.47374216]\n [0.00651241 0.9934876 ]\n [0.12016443 0.87983555]\n [0.54285508 0.45714489]\n [0.03422358 0.96577638]]\nvalidation batch: 118, val_loss: 0.3923511443129421, valid_preds_fold: [[7.92830368e-04 9.99207199e-01]\n [8.70405853e-01 1.29594117e-01]\n [2.42134114e-03 9.97578681e-01]\n [1.24244634e-02 9.87575531e-01]\n [4.45696861e-02 9.55430269e-01]\n [4.62211930e-04 9.99537826e-01]\n [9.91265595e-01 8.73438362e-03]\n [8.20987165e-01 1.79012865e-01]]\nvalidation batch: 119, val_loss: 0.394046784426174, valid_preds_fold: [[9.14559126e-01 8.54409039e-02]\n [3.18690250e-03 9.96813118e-01]\n [7.80184746e-01 2.19815284e-01]\n [4.52059269e-01 5.47940731e-01]\n [4.02740061e-01 5.97259939e-01]\n [5.29563520e-04 9.99470413e-01]\n [1.08190847e-03 9.98918056e-01]\n [5.01468405e-03 9.94985342e-01]]\nvalidation batch: 120, val_loss: 0.39690056301816534, valid_preds_fold: [[1.35249645e-01 8.64750385e-01]\n [9.76447284e-01 2.35526524e-02]\n [9.92045760e-01 7.95424450e-03]\n [4.11240896e-03 9.95887578e-01]\n [7.69133329e-01 2.30866715e-01]\n [8.79098848e-03 9.91209030e-01]\n [2.60409055e-04 9.99739587e-01]\n [4.40614134e-01 5.59385836e-01]]\nvalidation batch: 121, val_loss: 0.4018065230037174, valid_preds_fold: [[0.85058606 0.14941393]\n [0.75839442 0.24160562]\n [0.99058455 0.00941543]\n [0.66600835 0.33399162]\n [0.35890758 0.64109236]\n [0.90433162 0.09566842]\n [0.88872248 0.11127751]\n [0.05612626 0.94387376]]\nvalidation batch: 122, val_loss: 0.404123469969652, valid_preds_fold: [[0.79298681 0.20701322]\n [0.8082391  0.19176087]\n [0.88994527 0.11005469]\n [0.97248197 0.02751806]\n [0.55785358 0.44214639]\n [0.26908416 0.73091584]\n [0.99553865 0.00446128]\n [0.95202142 0.04797864]]\nvalidation batch: 123, val_loss: 0.40440110775241017, valid_preds_fold: [[0.98950386 0.01049613]\n [0.95009941 0.04990057]\n [0.99400568 0.00599433]\n [0.94675124 0.05324879]\n [0.93886167 0.06113827]\n [0.91705865 0.08294137]\n [0.99013501 0.00986504]\n [0.97801089 0.02198909]]\nvalidation batch: 124, val_loss: 0.4046039662874528, valid_preds_fold: [[0.06207298 0.93792701]\n [0.00284339 0.99715662]\n [0.07745524 0.92254472]\n [0.99466264 0.00533734]\n [0.99317223 0.00682779]\n [0.98813742 0.01186255]\n [0.96951914 0.03048085]\n [0.98050117 0.01949886]]\nvalidation batch: 125, val_loss: 0.406542996529245, valid_preds_fold: [[3.17651662e-04 9.99682307e-01]\n [3.07645589e-01 6.92354441e-01]\n [3.09406721e-04 9.99690652e-01]\n [1.10452147e-02 9.88954782e-01]\n [9.77762997e-01 2.22369991e-02]\n [9.66403306e-01 3.35966237e-02]\n [9.78611290e-01 2.13887189e-02]\n [1.88717589e-01 8.11282396e-01]]\nvalidation batch: 126, val_loss: 0.4115913772452487, valid_preds_fold: [[4.27968532e-01 5.72031438e-01]\n [8.35917220e-02 9.16408300e-01]\n [5.11216407e-04 9.99488711e-01]\n [5.08056255e-03 9.94919479e-01]\n [9.92587626e-01 7.41243549e-03]\n [9.60873485e-01 3.91265415e-02]\n [9.41080928e-01 5.89190722e-02]\n [7.22601891e-01 2.77398139e-01]]\nvalidation batch: 127, val_loss: 0.4120535367596759, valid_preds_fold: [[0.98511434 0.01488569]\n [0.96091461 0.03908536]\n [0.81128049 0.18871954]\n [0.96122116 0.03877887]\n [0.98595643 0.01404359]\n [0.8945089  0.10549104]\n [0.93066949 0.06933051]\n [0.99452233 0.00547764]]\nvalidation batch: 128, val_loss: 0.4220617657160237, valid_preds_fold: [[0.07480972 0.92519033]\n [0.95195907 0.04804099]\n [0.00152566 0.9984743 ]\n [0.98527122 0.01472873]\n [0.96406466 0.03593537]\n [0.91391188 0.08608813]\n [0.98395282 0.01604718]\n [0.00134421 0.9986558 ]]\nvalidation batch: 129, val_loss: 0.42669747650188256, valid_preds_fold: [[7.67111778e-01 2.32888192e-01]\n [9.44288313e-01 5.57116978e-02]\n [5.00600815e-01 4.99399215e-01]\n [9.72218990e-01 2.77809892e-02]\n [9.91289377e-01 8.71062744e-03]\n [4.90718707e-03 9.95092869e-01]\n [5.02206967e-04 9.99497771e-01]\n [1.68511015e-03 9.98314857e-01]]\nvalidation batch: 130, val_loss: 0.4311027391983645, valid_preds_fold: [[5.94831677e-03 9.94051635e-01]\n [5.91064105e-04 9.99408960e-01]\n [7.45364666e-01 2.54635364e-01]\n [1.26188749e-03 9.98738110e-01]\n [4.44175152e-04 9.99555767e-01]\n [9.02348995e-01 9.76510569e-02]\n [8.90723705e-01 1.09276295e-01]\n [8.75561833e-01 1.24438159e-01]]\nvalidation batch: 131, val_loss: 0.43463810147160165, valid_preds_fold: [[0.92194217 0.07805789]\n [0.96300828 0.03699169]\n [0.74327981 0.25672013]\n [0.89638305 0.10361697]\n [0.20293182 0.79706812]\n [0.93651807 0.06348194]\n [0.13744248 0.86255747]\n [0.99036306 0.00963692]]\nvalidation batch: 132, val_loss: 0.435089666532339, valid_preds_fold: [[0.00363317 0.99636686]\n [0.00105873 0.99894124]\n [0.82473505 0.17526494]\n [0.93409538 0.06590461]\n [0.00257526 0.99742472]\n [0.00143991 0.99856013]\n [0.00152954 0.99847049]\n [0.20052184 0.79947817]]\nvalidation batch: 133, val_loss: 0.44084597814039594, valid_preds_fold: [[1.31743671e-02 9.86825645e-01]\n [9.58690703e-01 4.13093381e-02]\n [8.05450196e-04 9.99194562e-01]\n [8.78007352e-01 1.21992663e-01]\n [4.50752139e-01 5.49247861e-01]\n [9.92003381e-01 7.99660664e-03]\n [2.18502041e-02 9.78149831e-01]\n [3.75542611e-01 6.24457359e-01]]\nvalidation batch: 134, val_loss: 0.4410645825258137, valid_preds_fold: [[8.99521291e-01 1.00478753e-01]\n [9.73210335e-01 2.67895963e-02]\n [9.18146193e-01 8.18538293e-02]\n [9.84671652e-01 1.53282890e-02]\n [3.91732529e-03 9.96082664e-01]\n [4.04083170e-04 9.99595940e-01]\n [5.72209654e-04 9.99427736e-01]\n [7.95090222e-04 9.99204934e-01]]\n","name":"stdout"},{"output_type":"stream","text":"[2020-02-01 14:05:38,911][INFO] ## epoch: 2, train loss: 0.17690339, valid loss: 0.44168579, acc: 0.83471074, f1: 0.82911869, best_f1: 0.83905255\n\n[2020-02-01 14:05:38,911][INFO] ## epoch: 2, train loss: 0.17690339, valid loss: 0.44168579, acc: 0.83471074, f1: 0.82911869, best_f1: 0.83905255\n\n[2020-02-01 14:05:38,911][INFO] ## epoch: 2, train loss: 0.17690339, valid loss: 0.44168579, acc: 0.83471074, f1: 0.82911869, best_f1: 0.83905255\n\n[2020-02-01 14:05:38,911][INFO] ## epoch: 2, train loss: 0.17690339, valid loss: 0.44168579, acc: 0.83471074, f1: 0.82911869, best_f1: 0.83905255\n\n[2020-02-01 14:05:38,911][INFO] ## epoch: 2, train loss: 0.17690339, valid loss: 0.44168579, acc: 0.83471074, f1: 0.82911869, best_f1: 0.83905255\n\n","name":"stderr"},{"output_type":"stream","text":"validation batch: 135, val_loss: 0.441418501854378, valid_preds_fold: [[7.61883020e-01 2.38116950e-01]\n [9.92913425e-01 7.08654383e-03]\n [9.21058834e-01 7.89411962e-02]\n [9.80051398e-01 1.99485458e-02]\n [8.79889471e-04 9.99120057e-01]\n [9.44732048e-04 9.99055326e-01]\n [3.55578819e-03 9.96444166e-01]\n [1.05204713e-03 9.98947918e-01]]\nvalidation batch: 136, val_loss: 0.44168578996493, valid_preds_fold: [[0.03595602 0.96404392]]\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:133: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"053a8e8bfcbf4c50886cf74aff0eaff4"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:141: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed32c4eb235e445ba09b67a453e48f15"}},"metadata":{}},{"output_type":"stream","text":"[2020-02-01 14:06:00,001][INFO] ## epoch: best, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n[2020-02-01 14:06:00,001][INFO] ## epoch: best, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n[2020-02-01 14:06:00,001][INFO] ## epoch: best, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n[2020-02-01 14:06:00,001][INFO] ## epoch: best, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n[2020-02-01 14:06:00,001][INFO] ## epoch: best, acc: 0.84481175, f1: 0.83905255, best_f1: 0.83905255\n\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger.info(f1_score(train_df.target, np.argmax(oof_train, axis=1)))\ntrain_df['pred_target'] = np.argmax(oof_train, axis=1)","execution_count":125,"outputs":[{"output_type":"stream","text":"[2020-02-01 14:14:05,573][INFO] ## 0.1937059142702116\n[2020-02-01 14:14:05,573][INFO] ## 0.1937059142702116\n[2020-02-01 14:14:05,573][INFO] ## 0.1937059142702116\n[2020-02-01 14:14:05,573][INFO] ## 0.1937059142702116\n[2020-02-01 14:14:05,573][INFO] ## 0.1937059142702116\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":126,"outputs":[{"output_type":"execute_result","execution_count":126,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  pred_target  \n0       1            0  \n1       1            0  \n2       1            0  \n3       1            0  \n4       1            0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>pred_target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['target'] = np.argmax(oof_test, axis=1)\nlogger.info(test_df['target'].value_counts())","execution_count":127,"outputs":[{"output_type":"stream","text":"[2020-02-01 14:15:28,572][INFO] ## 0    3263\nName: target, dtype: int64\n[2020-02-01 14:15:28,572][INFO] ## 0    3263\nName: target, dtype: int64\n[2020-02-01 14:15:28,572][INFO] ## 0    3263\nName: target, dtype: int64\n[2020-02-01 14:15:28,572][INFO] ## 0    3263\nName: target, dtype: int64\n[2020-02-01 14:15:28,572][INFO] ## 0    3263\nName: target, dtype: int64\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df['target'] = np.argmax(oof_test, axis=1)\nsubmit_df.to_csv('submission_1fold.csv', index=False)","execution_count":129,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df.head()","execution_count":130,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'submit' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-130-34fb83323dc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'submit' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}