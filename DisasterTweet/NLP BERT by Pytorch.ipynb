{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport sys\nimport time\nimport logging\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n\nfrom transformers import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/nlp-getting-started","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmit = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train size:', train.shape)\nprint('Test size:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, id, text, label=None):\n        self.id = id\n        self.text = text\n        self.label = label\n\nclass InputFeatures(object):\n    def __init__(self, example_id, choices_features, label):\n        self.example_id = example_id\n        _, input_ids, input_mask, segment_ids = choices_features[0]\n        self.choices_features = {\n            'input_ids': input_ids,\n            'input_mask': input_mask,\n            'segment_ids': segment_ids\n        }\n        self.label = label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_examples(df, is_training):\n    if not is_training:\n        df['target'] = np.zeros(len(df), dtype=np.int64)\n    examples = []\n    for val in df[['id', 'text', 'target']].values:\n        examples.append(InputExample(id=val[0], text=val[1], label=val[2]))\n    return examples, df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"example = \"Today is a good day. How are you?\"\ntext = tokenizer.tokenize(example)\ntokens = [\"[CLS]\"] + text + [\"[SEP]\"] \n\nsegment_ids = [0] * (len(text) + 2) \nprint(segment_ids)\ninput_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(input_ids)\ninput_mask = [1] * len(input_ids)\nprint(input_mask)\n\npadding_length = max_seq_length - len(input_ids)\ninput_ids += ([0] * padding_length)\ninput_mask += ([0] * padding_length)\nsegment_ids += ([0] * padding_length)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_examples_to_features(examples, tokenizer, max_seq_length,\n                                 is_training):\n    features = []\n    for example_index, example in enumerate(examples):\n\n        text = tokenizer.tokenize(example.text)\n        MAX_TEXT_LEN = max_seq_length - 2 \n        text = text[:MAX_TEXT_LEN]\n\n        choices_features = []\n\n        tokens = [\"[CLS]\"] + text + [\"[SEP]\"]  \n        segment_ids = [0] * (len(text) + 2) \n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n\n        padding_length = max_seq_length - len(input_ids)\n        input_ids += ([0] * padding_length)\n        input_mask += ([0] * padding_length)\n        segment_ids += ([0] * padding_length)\n        choices_features.append((tokens, input_ids, input_mask, segment_ids))\n\n        label = example.label\n        if example_index < 1 and is_training:\n            logger.info(\"*** Example ***\")\n            logger.info(\"idx: {}\".format(example_index))\n            logger.info(\"id: {}\".format(example.id))\n            logger.info(\"tokens: {}\".format(' '.join(tokens).replace('\\u2581', '_')))\n            logger.info(\"input_ids: {}\".format(' '.join(map(str, input_ids))))\n            logger.info(\"input_mask: {}\".format(len(input_mask)))\n            logger.info(\"segment_ids: {}\".format(len(segment_ids)))\n            logger.info(\"label: {}\".format(label))\n\n        features.append(\n            InputFeatures(\n                example_id=example.id,\n                choices_features=choices_features,\n                label=label\n            )\n        )\n    return features\n\n\ndef select_field(features, field):\n    return [\n        feature.choices_features[field] for feature in features\n    ]\n\ndef metric(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='macro')\n    return acc, f1\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nmax_seq_length = 512\nlearning_rate = 1e-5  \nnum_epochs = 3  \nbatch_size = 8  \npatience = 2  \nfile_name = 'model'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger = logging.getLogger('mylogger')\nlogger.setLevel(logging.DEBUG)\ntimestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\nfh = logging.FileHandler('log_model.txt')\nfh.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\nlogger.addHandler(fh)\nlogger.addHandler(ch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l /kaggle/input/pretrained-bert-models-for-pytorch/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_examples, train_df = read_examples(train, is_training=True)\nlabels = train_df['target'].astype(int).values\ntrain_features = convert_examples_to_features(\n    train_examples, tokenizer, max_seq_length, True)\nall_input_ids = np.array(select_field(train_features, 'input_ids'))\nall_input_mask = np.array(select_field(train_features, 'input_mask'))\nall_segment_ids = np.array(select_field(train_features, 'segment_ids'))\nall_label = np.array([f.label for f in train_features])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_examples, test_df = read_examples(test, is_training=False)\ntest_features = convert_examples_to_features(\n    test_examples, tokenizer, max_seq_length, True)\ntest_input_ids = torch.tensor(select_field(test_features, 'input_ids'), dtype=torch.long)\ntest_input_mask = torch.tensor(select_field(test_features, 'input_mask'), dtype=torch.long)\ntest_segment_ids = torch.tensor(select_field(test_features, 'segment_ids'), dtype=torch.long)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, hidden_size=768, num_class=2):\n        super(NeuralNet, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n                                        output_hidden_states=True,\n                                        output_attentions=True)\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        self.weights = nn.Parameter(torch.rand(13, 1))\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.fc = nn.Linear(hidden_size, num_class)\n\n    def forward(self, input_ids, input_mask, segment_ids):\n        all_hidden_states, all_attentions = self.bert(input_ids, token_type_ids=segment_ids,\n                                                                attention_mask=input_mask)[-2:]\n        batch_size = input_ids.shape[0]\n        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n        atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n        atten = F.softmax(atten.view(-1), dim=0)\n        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                h = self.fc(dropout(feature))\n            else:\n                h += self.fc(dropout(feature))\n        h = h / len(self.dropouts)\n        return h        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\noof_train = np.zeros((len(train_df), 2), dtype=np.float32)\noof_test = np.zeros((len(test_df), 2), dtype=np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold, (train_index, valid_index) in enumerate(skf.split(all_label, all_label)):\n    \n    # remove this line if you want to train for all 7 folds\n    if fold == 1:\n        break # due to kernel time limit\n\n    logger.info('================     fold {}        ==============='.format(fold))\n\n    train_input_ids = torch.tensor(all_input_ids[train_index], dtype=torch.long)\n    train_input_mask = torch.tensor(all_input_mask[train_index], dtype=torch.long)\n    train_segment_ids = torch.tensor(all_segment_ids[train_index], dtype=torch.long)\n    train_label = torch.tensor(all_label[train_index], dtype=torch.long)\n\n    valid_input_ids = torch.tensor(all_input_ids[valid_index], dtype=torch.long)\n    valid_input_mask = torch.tensor(all_input_mask[valid_index], dtype=torch.long)\n    valid_segment_ids = torch.tensor(all_segment_ids[valid_index], dtype=torch.long)\n    valid_label = torch.tensor(all_label[valid_index], dtype=torch.long)\n\n    train = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label)\n    valid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label)\n    test = torch.utils.data.TensorDataset(test_input_ids, test_input_mask, test_segment_ids)\n\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\n    model = NeuralNet()\n    model.cuda()\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-6)\n    model.train()\n\n    best_f1 = 0.\n    valid_best = np.zeros((valid_label.size(0), 2))\n\n    early_stop = 0\n    for epoch in range(num_epochs):\n        train_loss = 0.\n        for batch in tqdm(train_loader):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            y_pred = model(x_ids, x_mask, x_sids)\n            loss = loss_fn(y_pred, y_truth)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() / len(train_loader)\n    \n        model.eval()\n        val_loss = 0.\n        valid_preds_fold = np.zeros((valid_label.size(0), 2))\n        with torch.no_grad():\n            for i, batch in tqdm(enumerate(valid_loader)):\n                batch = tuple(t.cuda() for t in batch)\n                x_ids, x_mask, x_sids, y_truth = batch\n                y_pred = model(x_ids, x_mask, x_sids).detach()\n                val_loss += loss_fn(y_pred, y_truth).item() / len(valid_loader)\n                valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n    \n        acc, f1 = metric(all_label[valid_index], np.argmax(valid_preds_fold, axis=1))\n        if best_f1 < f1:\n            early_stop = 0\n            best_f1 = f1\n            valid_best = valid_preds_fold\n            torch.save(model.state_dict(), 'model_fold_{}.bin'.format(fold))\n        else:\n            early_stop += 1\n        logger.info(\n            'epoch: %d, train loss: %.8f, valid loss: %.8f, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n            (epoch, train_loss, val_loss, acc, f1, best_f1))\n        torch.cuda.empty_cache()  \n    \n        if early_stop >= patience:\n            break\n\n    test_preds_fold = np.zeros((len(test_df), 2))\n    valid_preds_fold = np.zeros((valid_label.size(0), 2))\n    model.load_state_dict(torch.load('model_fold_{}.bin'.format(fold)))\n    model.eval()\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(valid_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(test_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            test_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n    valid_best = valid_preds_fold\n    oof_train[valid_index] = valid_best\n    acc, f1 = metric(all_label[valid_index], np.argmax(valid_best, axis=1))\n    logger.info('epoch: best, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n                (acc, f1, best_f1))\n    \n    \n    #oof_test += test_preds_fold / 7 # uncomment this for 7 folds\n    oof_test += test_preds_fold / 1 # comment this line when training for 7 folds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger.info(f1_score(labels, np.argmax(oof_train, axis=1)))\ntrain_df['pred_target'] = np.argmax(oof_train, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['target'] = np.argmax(oof_test, axis=1)\nlogger.info(test_df['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['target'] = np.argmax(oof_test, axis=1)\nsubmit.to_csv('submission_1fold.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}