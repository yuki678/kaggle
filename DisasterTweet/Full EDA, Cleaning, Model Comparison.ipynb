{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n### Import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import Counter\n\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nprint('tweet.shape: ', tweet.shape)\nprint('test.shape: ', test.shape)\ntweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.info()\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Class Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tweet.target.value_counts()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel('samples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way to do count plot in Seaborn\nsns.set(style='darkgrid')\nsns.countplot(x='target', data=tweet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost similar number of records for both"},{"metadata":{},"cell_type":"markdown","source":"### Number of characters, words, length"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\ntweet_len = tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len, color='red')\nax1.set_title('disaster tweets')\ntweet_len = tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len, color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Around 100-150 characters per tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\ntweet_len = tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len, color='red')\nax1.set_title('disaster tweets')\ntweet_len = tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len, color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Around 10-20 words per tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\nword = tweet[tweet['target']==1]['text'].str.split().map(lambda x: [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)), ax=ax1, color='red')\nax1.set_title('disaster tweets')\nword = tweet[tweet['target']==0]['text'].str.split().map(lambda x: [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)), ax=ax2, color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Average word lengths in each tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Disaster - around 7 charactors per word\n* Not disaster - around 5 charactors per word"},{"metadata":{},"cell_type":"markdown","source":"### Common Stop Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze Not disaster tweets\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n\ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n\nx, y = zip(*top)\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze disaster tweets\ncorpus = create_corpus(1)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n\ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n\nx, y = zip(*top)\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much difference between Disaster and Not disaster"},{"metadata":{},"cell_type":"markdown","source":"### Punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze Not disaster tweets\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor word in corpus:\n    if word in special:\n        dic[word] += 1\n\nx, y = zip(*dic.items())\nplt.figure(figsize=(10, 5))\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze disaster tweets\ncorpus = create_corpus(1)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor word in corpus:\n    if word in special:\n        dic[word] += 1\n\nx, y = zip(*dic.items())\nplt.figure(figsize=(10, 5))\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common words not in stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = Counter(corpus)\nmost = counter.most_common()\nx, y = [], []\nfor word, count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y, y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ngram analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_tweet_bigrams = get_top_tweet_bigrams(tweet['text'])[:10]\ntop_tweet_bigrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nx, y = map(list, zip(*top_tweet_bigrams))\nsns.barplot(x=y, y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hashtag Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_hashtags(text):\n    return ','.join(match.group(0)[1:].lower() for match in re.finditer(r'#\\w+', text)) or None\n\ndef create_hashtag_corpus(target):\n    corpus_ht = []\n    hashtags = tweet[tweet['target']==target]['text'].apply(lambda x: find_hashtags(x))\n    print('The number of tweets having hashtag(s): ', len([x for x in hashtags if isinstance(x, str)]))\n    hashtags.fillna(value='no', inplace=True)\n    \n    for x in hashtags.str.split(','):\n        for i in x:\n            if i != 'no':\n                corpus_ht.append(i)\n    return corpus_ht","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_ht = create_hashtag_corpus(0)\ndic = defaultdict(int)\nfor word in corpus_ht:\n    dic[word] += 1\n\ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20]\nplt.figure(figsize=(10,6))\nx, y = map(list, zip(*top))\nsns.barplot(x=y, y=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_ht = create_hashtag_corpus(1)\ndic = defaultdict(int)\nfor word in corpus_ht:\n    dic[word] += 1\n\ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20]\nplt.figure(figsize=(10,6))\nx, y = map(list, zip(*top))\nsns.barplot(x=y, y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keyword"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove the encoded space character\ntweet['keyword'] = tweet['keyword'].map(lambda s: s.replace('%20', ' ') if isinstance(s, str) else s)\n\nkw_unique  = {kw for kw in tweet['keyword'].values if isinstance(kw, str)}\nkw_total = len(tweet) - len(tweet[tweet[\"keyword\"].isna()])\n\nprint(\"Unique Keyword / Total: {} / {}\".format(len(kw_unique), kw_total))\nprint(\"Tweets with no keyword: {}\".format(len(tweet[tweet[\"keyword\"].isna()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kw_disaster = [kw for kw in tweet.loc[tweet.target == 1].keyword]\nkw_not_disaster = [kw for kw in tweet.loc[tweet.target == 0].keyword]\n\nkw_disaster_cn = dict(pd.DataFrame(data={'x': kw_disaster}).x.value_counts())\nkw_not_disaster_cn = dict(pd.DataFrame(data={'x': kw_not_disaster}).x.value_counts())\nkw_all_cn =  dict(pd.DataFrame(data={'x': tweet.keyword.values}).x.value_counts())\n\nfor keyword, _ in sorted(kw_all_cn.items(), key=lambda x: x[1], reverse=True)[:10]:\n    print(\"> Keyword: {}\".format(keyword))\n    print(\"-- # in disaster tweets:     {}\".format(kw_disaster_cn.get(keyword, 0)))\n    print(\"-- # in not disaster tweets: {}\".format(kw_not_disaster_cn.get(keyword, 0)))\n    print('--------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some keywords are common, some are only for not disaster\n### Location"},{"metadata":{"trusted":true},"cell_type":"code","source":"loc_unique  = {loc for loc in tweet['location'].values if isinstance(loc, str)}\nloc_total = len(tweet) - len(tweet[tweet[\"location\"].isna()])\n\nprint(\"Unique Location / Total: {} / {}\".format(len(loc_unique), loc_total))\nprint(\"Tweets with no Location: {}\".format(len(tweet[tweet[\"location\"].isna()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loc_disaster = [loc for loc in tweet.loc[tweet.target == 1].location]\nloc_not_disaster = [loc for loc in tweet.loc[tweet.target == 0].location]\n\nloc_disaster_cn = dict(pd.DataFrame(data={'x': loc_disaster}).x.value_counts())\nloc_not_disaster_cn = dict(pd.DataFrame(data={'x': loc_not_disaster}).x.value_counts())\nloc_all_cn =  dict(pd.DataFrame(data={'x': tweet.location.values}).x.value_counts())\n\nfor location, _ in sorted(loc_all_cn.items(), key=lambda x: x[1], reverse=True)[:10]:\n    print(\"> Location: {}\".format(location))\n    print(\"-- # in disaster tweets:     {}\".format(loc_disaster_cn.get(location, 0)))\n    print(\"-- # in not disaster tweets: {}\".format(loc_not_disaster_cn.get(location, 0)))\n    print('--------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We may use this but the data is too sparce"},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\n* Remove URLs\n* Remove HTML Tags\n* Remove Emoji\n* Remove Punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([tweet, test])\nprint(df.shape)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing URLs"},{"metadata":{"trusted":true},"cell_type":"code","source":"example=\"This is a test message :https://www.abc.com/test/sample-project\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\nremove_URL(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_URL(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing HTML tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\nprint(remove_html(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Emojis"},{"metadata":{"trusted":true},"cell_type":"code","source":"example = \"Omg another Earthquake 😔😔\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"example=\"I am a #king\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nprint(remove_punct(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model, Train and Predict\n## Utility"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_save(model, test_df, name_pre='test'):\n    sample_sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n    y_pre = model.predict(test_df.text)\n    y_pre = np.round(y_pre).astype(int).reshape(3263)\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n    sub_name = name_pre + \"_submission.csv\"\n    sub.to_csv(sub_name, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A) tfidf + Logistic Regression as a baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\ndef build_tfidf_logreg(df, validation=False):\n    x_train = df['text']\n    y_train = df['target']\n    clf = Pipeline([\n        ('tfidf', TfidfVectorizer(analyzer='word')),\n        ('clf', LogisticRegression())\n        ])\n\n    if validation:\n        x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train.values, y_train.values, test_size=0.2, random_state=7)\n        print(x_train, y_train)\n        clf.fit(x_train, y_train)\n        predicted_train = clf.predict(x_train)\n        predicted_valid = clf.predict(x_valid)\n        print(\"Training Accuracy: \", np.mean(predicted_train == y_train))\n        print(\"Validation Accuracy: \", np.mean(predicted_valid == y_valid))\n    \n    else:\n        clf.fit(x_train, y_train)\n    \n    return clf\n\ntrain_df = df[:tweet.shape[0]]\ntest_df = df[tweet.shape[0]:]\n\nmodel = build_tfidf_logreg(train_df, validation=False)\n\ntest_save(model, test_df, \"a_tfidf\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B) LSTM\n### GloVe Word Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(tweet) if ((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/glove-global-vectors-for-word-representation/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load a Lookup table\nembedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize corpus\nMAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\nprint(sequences)\ntweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\nprint(tweet_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer_obj.word_index\nprint('Number of unique words:', len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embed words\nnum_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words, 200))\n\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    embed_vec = embedding_dict.get(word)\n    if embed_vec is not None:\n        embedding_matrix[i] = embed_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nembedding = Embedding(num_words, 200, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LEN, trainable=False)\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = tweet_pad[:tweet.shape[0]]\ntest = tweet_pad[tweet.shape[0]:]\n\nX_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_save(model, test, \"b_LSTM\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}